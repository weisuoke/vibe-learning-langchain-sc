# BaseChatModel å®ç°

> åŸå­åŒ–çŸ¥è¯†ç‚¹ | LangChain æºç  | èŠå¤©æ¨¡å‹åŸºç±»å®ç°

---

## 1. ã€30å­—æ ¸å¿ƒã€‘

**BaseChatModel æ˜¯ LangChain èŠå¤©æ¨¡å‹çš„æŠ½è±¡åŸºç±»ï¼Œå®šä¹‰äº†æ¶ˆæ¯è¾“å…¥è¾“å‡ºå’Œ _generate æ ¸å¿ƒæ–¹æ³•ï¼Œæ˜¯æ‰€æœ‰ LLM çš„ç»Ÿä¸€æ¥å£ã€‚**

---

## 2. ã€ç¬¬ä¸€æ€§åŸç†ã€‘

### ä»€ä¹ˆæ˜¯ç¬¬ä¸€æ€§åŸç†ï¼Ÿ

**ç¬¬ä¸€æ€§åŸç†**ï¼šå›åˆ°äº‹ç‰©æœ€åŸºæœ¬çš„çœŸç†ï¼Œä»æºå¤´æ€è€ƒé—®é¢˜

### BaseChatModel çš„ç¬¬ä¸€æ€§åŸç† ğŸ¯

#### 1. æœ€åŸºç¡€çš„å®šä¹‰

**BaseChatModel = æ¶ˆæ¯åˆ—è¡¨ â†’ LLM â†’ AIæ¶ˆæ¯**

ä»…æ­¤è€Œå·²ï¼æ²¡æœ‰æ›´åŸºç¡€çš„äº†ã€‚

- **è¾“å…¥**ï¼šæ¶ˆæ¯åˆ—è¡¨ `List[BaseMessage]`
- **å¤„ç†**ï¼šè°ƒç”¨åº•å±‚ LLM API
- **è¾“å‡º**ï¼šAI æ¶ˆæ¯ `AIMessage`

```python
# BaseChatModel çš„æœ¬è´¨
def chat_model(messages: List[BaseMessage]) -> AIMessage:
    return call_llm_api(messages)
```

#### 2. ä¸ºä»€ä¹ˆéœ€è¦ BaseChatModelï¼Ÿ

**æ ¸å¿ƒé—®é¢˜ï¼šå¦‚ä½•ç»Ÿä¸€ä¸åŒ LLM æä¾›å•†çš„æ¥å£ï¼Ÿ**

```python
# æ²¡æœ‰ç»Ÿä¸€åŸºç±»çš„å›°å¢ƒ
# OpenAI
from openai import OpenAI
client = OpenAI()
response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello"}]
)
result = response.choices[0].message.content

# Anthropic
from anthropic import Anthropic
client = Anthropic()
response = client.messages.create(
    model="claude-3-opus",
    messages=[{"role": "user", "content": "Hello"}]
)
result = response.content[0].text

# é—®é¢˜ï¼š
# 1. æ¯ä¸ª API è°ƒç”¨æ–¹å¼ä¸åŒ
# 2. å“åº”æ ¼å¼ä¸åŒ
# 3. æ¶ˆæ¯æ ¼å¼ä¸åŒ
# 4. éš¾ä»¥åˆ‡æ¢æ¨¡å‹
```

```python
# æœ‰äº† BaseChatModel
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

# ç»Ÿä¸€çš„è°ƒç”¨æ–¹å¼
openai_llm = ChatOpenAI(model="gpt-4")
anthropic_llm = ChatAnthropic(model="claude-3-opus")

# ç›¸åŒçš„ invoke æ¥å£
result1 = openai_llm.invoke("Hello")
result2 = anthropic_llm.invoke("Hello")

# ä¼˜åŠ¿ï¼š
# 1. ç»Ÿä¸€çš„è°ƒç”¨æ–¹å¼
# 2. ç»Ÿä¸€çš„æ¶ˆæ¯æ ¼å¼
# 3. å¯ä»¥æ— ç¼åˆ‡æ¢æ¨¡å‹
# 4. è‡ªåŠ¨è·å¾— stream/batch/ainvoke
```

#### 3. BaseChatModel çš„ä¸‰å±‚ä»·å€¼

##### ä»·å€¼1ï¼šç»Ÿä¸€æ¥å£ - å±è”½ API å·®å¼‚

```python
# ä¸ç®¡æ˜¯ä»€ä¹ˆæ¨¡å‹ï¼Œéƒ½ç”¨ç›¸åŒæ–¹å¼è°ƒç”¨
def process_with_llm(llm: BaseChatModel, query: str) -> str:
    return llm.invoke(query).content

# å¯ä»¥ä¼ å…¥ä»»ä½•å®ç°
process_with_llm(ChatOpenAI(), "Hello")
process_with_llm(ChatAnthropic(), "Hello")
process_with_llm(ChatOllama(), "Hello")
```

##### ä»·å€¼2ï¼šæ¶ˆæ¯æ ‡å‡†åŒ– - ç»Ÿä¸€æ¶ˆæ¯æ ¼å¼

```python
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# ç»Ÿä¸€çš„æ¶ˆæ¯ç±»å‹
messages = [
    SystemMessage(content="You are helpful"),
    HumanMessage(content="Hello"),
    AIMessage(content="Hi there!"),
]

# æ‰€æœ‰æ¨¡å‹éƒ½æ¥å—è¿™ç§æ ¼å¼
result = llm.invoke(messages)
```

##### ä»·å€¼3ï¼šRunnable é›†æˆ - å‚ä¸ LCEL ç®¡é“

```python
# BaseChatModel å®ç°äº† Runnable åè®®
chain = prompt | llm | parser

# è‡ªåŠ¨è·å¾—è¿™äº›èƒ½åŠ›
chain.invoke(input)
chain.stream(input)
chain.batch(inputs)
await chain.ainvoke(input)
```

#### 4. ä»ç¬¬ä¸€æ€§åŸç†æ¨å¯¼ BaseChatModel æ¶æ„

**æ¨ç†é“¾ï¼š**

```
1. LLM åº”ç”¨éœ€è¦è°ƒç”¨å„ç§è¯­è¨€æ¨¡å‹
   â†“
2. ä¸åŒæ¨¡å‹æœ‰ä¸åŒçš„ API å’Œæ¶ˆæ¯æ ¼å¼
   â†“
3. éœ€è¦ä¸€ä¸ªç»Ÿä¸€çš„æŠ½è±¡å±‚
   â†“
4. å®šä¹‰ BaseChatModel æŠ½è±¡åŸºç±»
   â†“
5. æŠ½è±¡æ–¹æ³• _generateï¼šå­ç±»å®ç°å…·ä½“ API è°ƒç”¨
   â†“
6. å…¬å¼€æ–¹æ³• invokeï¼šå¤„ç†è¾“å…¥è½¬æ¢å’Œç»“æœåŒ…è£…
   â†“
7. ç»§æ‰¿ Runnableï¼šè‡ªåŠ¨è·å¾—ç»„åˆèƒ½åŠ›
   â†“
8. å„æ¨¡å‹æä¾›å•†å®ç°å…·ä½“å­ç±»
```

#### 5. ä¸€å¥è¯æ€»ç»“ç¬¬ä¸€æ€§åŸç†

**BaseChatModel æ˜¯"æ¶ˆæ¯â†’AIå“åº”"çš„ç»Ÿä¸€æŠ½è±¡ï¼Œé€šè¿‡æ¨¡æ¿æ–¹æ³•æ¨¡å¼è®©ä¸åŒ LLM æä¾›ç»Ÿä¸€æ¥å£ï¼ŒåŒæ—¶é›†æˆ Runnable åè®®å‚ä¸ LCEL ç®¡é“ã€‚**

---

## 3. ã€æ ¸å¿ƒæ¦‚å¿µï¼ˆå…¨é¢è¦†ç›–ï¼‰ã€‘

### æ ¸å¿ƒæ¦‚å¿µ1ï¼šBaseChatModel ç±»å±‚æ¬¡ ğŸ—ï¸

**BaseChatModel ç»§æ‰¿è‡ª BaseLanguageModel å’Œ Runnable**

```python
from abc import ABC, abstractmethod
from typing import List, Optional, Any, Iterator, AsyncIterator
from langchain_core.messages import BaseMessage, AIMessage
from langchain_core.runnables import Runnable

class BaseChatModel(BaseLanguageModel, Runnable[LanguageModelInput, BaseMessage]):
    """èŠå¤©æ¨¡å‹æŠ½è±¡åŸºç±»

    ç»§æ‰¿å…³ç³»ï¼š
    - BaseLanguageModelï¼šè¯­è¨€æ¨¡å‹çš„åŸºç¡€èƒ½åŠ›
    - Runnableï¼šLCEL ç»„åˆèƒ½åŠ›

    å­ç±»éœ€è¦å®ç°ï¼š
    - _generateï¼šæ ¸å¿ƒç”Ÿæˆæ–¹æ³•
    - _llm_typeï¼šæ¨¡å‹ç±»å‹æ ‡è¯†
    """

    # ===== æŠ½è±¡æ–¹æ³•ï¼šå­ç±»å¿…é¡»å®ç° =====

    @abstractmethod
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManager] = None,
        **kwargs
    ) -> ChatResult:
        """æ ¸å¿ƒç”Ÿæˆæ–¹æ³•ï¼šå­ç±»å®ç°å…·ä½“çš„ API è°ƒç”¨"""
        pass

    @property
    @abstractmethod
    def _llm_type(self) -> str:
        """æ¨¡å‹ç±»å‹æ ‡è¯†"""
        pass

    # ===== å…¬å¼€æ¥å£æ–¹æ³• =====

    def invoke(
        self,
        input: LanguageModelInput,
        config: Optional[RunnableConfig] = None,
        **kwargs
    ) -> BaseMessage:
        """åŒæ­¥è°ƒç”¨ï¼ˆRunnable æ¥å£ï¼‰"""
        # 1. è½¬æ¢è¾“å…¥ä¸ºæ¶ˆæ¯åˆ—è¡¨
        messages = self._convert_input(input)

        # 2. è°ƒç”¨ _generate
        result = self._generate(messages, **kwargs)

        # 3. è¿”å›ç¬¬ä¸€ä¸ªç”Ÿæˆç»“æœ
        return result.generations[0].message

    def stream(
        self,
        input: LanguageModelInput,
        config: Optional[RunnableConfig] = None,
        **kwargs
    ) -> Iterator[BaseMessageChunk]:
        """æµå¼è¾“å‡º"""
        messages = self._convert_input(input)

        # è°ƒç”¨ _stream æ–¹æ³•
        for chunk in self._stream(messages, **kwargs):
            yield chunk
```

**ç±»å±‚æ¬¡å›¾ï¼š**

```
Runnable[Input, Output]
    â†‘
BaseLanguageModel
    â†‘
BaseChatModel
    â†‘
â”œâ”€â”€ ChatOpenAI
â”œâ”€â”€ ChatAnthropic
â”œâ”€â”€ ChatOllama
â”œâ”€â”€ ChatGoogleGenerativeAI
â””â”€â”€ ...æ›´å¤šå®ç°
```

---

### æ ¸å¿ƒæ¦‚å¿µ2ï¼š_generate æ ¸å¿ƒæ–¹æ³• ğŸ“

**_generate æ˜¯æ¨¡æ¿æ–¹æ³•æ¨¡å¼çš„æ ¸å¿ƒï¼Œå­ç±»å®ç°å…·ä½“ API è°ƒç”¨**

```python
from langchain_core.outputs import ChatResult, ChatGeneration

class ChatResult:
    """èŠå¤©ç»“æœ"""
    generations: List[ChatGeneration]  # ç”Ÿæˆçš„æ¶ˆæ¯åˆ—è¡¨
    llm_output: Optional[dict] = None  # LLM é¢å¤–è¾“å‡ºï¼ˆtoken ä½¿ç”¨é‡ç­‰ï¼‰

class ChatGeneration:
    """å•ä¸ªç”Ÿæˆç»“æœ"""
    message: BaseMessage  # ç”Ÿæˆçš„æ¶ˆæ¯
    generation_info: Optional[dict] = None  # ç”Ÿæˆä¿¡æ¯

# ChatOpenAI çš„ _generate å®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰
class ChatOpenAI(BaseChatModel):
    model: str = "gpt-4"
    temperature: float = 0.7
    client: Any = None  # OpenAI client

    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        **kwargs
    ) -> ChatResult:
        # 1. è½¬æ¢æ¶ˆæ¯æ ¼å¼
        openai_messages = self._convert_messages(messages)

        # 2. è°ƒç”¨ OpenAI API
        response = self.client.chat.completions.create(
            model=self.model,
            messages=openai_messages,
            temperature=self.temperature,
            stop=stop,
            **kwargs
        )

        # 3. è½¬æ¢å“åº”ä¸º ChatResult
        return self._create_chat_result(response)

    def _convert_messages(self, messages: List[BaseMessage]) -> List[dict]:
        """å°† LangChain æ¶ˆæ¯è½¬æ¢ä¸º OpenAI æ ¼å¼"""
        result = []
        for msg in messages:
            if isinstance(msg, HumanMessage):
                result.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                result.append({"role": "assistant", "content": msg.content})
            elif isinstance(msg, SystemMessage):
                result.append({"role": "system", "content": msg.content})
        return result

    def _create_chat_result(self, response) -> ChatResult:
        """å°† OpenAI å“åº”è½¬æ¢ä¸º ChatResult"""
        generations = []
        for choice in response.choices:
            message = AIMessage(content=choice.message.content)
            generations.append(ChatGeneration(message=message))

        return ChatResult(
            generations=generations,
            llm_output={
                "token_usage": response.usage.model_dump(),
                "model": response.model,
            }
        )

    @property
    def _llm_type(self) -> str:
        return "openai-chat"
```

---

### æ ¸å¿ƒæ¦‚å¿µ3ï¼šæ¶ˆæ¯ç±»å‹ç³»ç»Ÿ ğŸ’¬

**LangChain å®šä¹‰äº†ç»Ÿä¸€çš„æ¶ˆæ¯ç±»å‹**

```python
from langchain_core.messages import (
    BaseMessage,
    HumanMessage,
    AIMessage,
    SystemMessage,
    ToolMessage,
    FunctionMessage,
)

# BaseMessage åŸºç±»
class BaseMessage:
    """æ¶ˆæ¯åŸºç±»"""
    content: str                          # æ¶ˆæ¯å†…å®¹
    type: str                             # æ¶ˆæ¯ç±»å‹
    additional_kwargs: dict = {}          # é¢å¤–å‚æ•°
    response_metadata: dict = {}          # å“åº”å…ƒæ•°æ®

# å…·ä½“æ¶ˆæ¯ç±»å‹
class HumanMessage(BaseMessage):
    """ç”¨æˆ·æ¶ˆæ¯"""
    type: str = "human"

class AIMessage(BaseMessage):
    """AI æ¶ˆæ¯"""
    type: str = "ai"
    tool_calls: List[ToolCall] = []       # å·¥å…·è°ƒç”¨

class SystemMessage(BaseMessage):
    """ç³»ç»Ÿæ¶ˆæ¯"""
    type: str = "system"

class ToolMessage(BaseMessage):
    """å·¥å…·è¿”å›æ¶ˆæ¯"""
    type: str = "tool"
    tool_call_id: str                     # å¯¹åº”çš„å·¥å…·è°ƒç”¨ ID

# ä½¿ç”¨ç¤ºä¾‹
messages = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content="What is 2+2?"),
    AIMessage(content="2+2 equals 4."),
    HumanMessage(content="Thanks!"),
]

result = llm.invoke(messages)
# result æ˜¯ AIMessage
```

**æ¶ˆæ¯ç±»å‹å¯¹ç…§è¡¨ï¼š**

| LangChain ç±»å‹ | OpenAI role | Anthropic role |
|---------------|-------------|----------------|
| SystemMessage | system | system |
| HumanMessage | user | user |
| AIMessage | assistant | assistant |
| ToolMessage | tool | tool_result |

---

### æ ¸å¿ƒæ¦‚å¿µ4ï¼šæµå¼è¾“å‡º _stream ğŸŒŠ

**æµå¼è¾“å‡ºé€å—è¿”å› LLM å“åº”**

```python
from langchain_core.messages import AIMessageChunk

class AIMessageChunk(BaseMessageChunk):
    """AI æ¶ˆæ¯å—ï¼šæµå¼è¾“å‡ºçš„å•ä¸ªç‰‡æ®µ"""
    type: str = "AIMessageChunk"

class BaseChatModel:
    def _stream(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        **kwargs
    ) -> Iterator[ChatGenerationChunk]:
        """æµå¼ç”Ÿæˆï¼ˆå­ç±»å¯é‡å†™ï¼‰

        é»˜è®¤å®ç°ï¼šè°ƒç”¨ _generate ç„¶åä¸€æ¬¡æ€§è¿”å›
        ä¼˜åŒ–å®ç°ï¼šçœŸæ­£çš„æµå¼ API è°ƒç”¨
        """
        # é»˜è®¤å®ç°ï¼ˆéæµå¼ï¼‰
        result = self._generate(messages, stop=stop, **kwargs)
        yield ChatGenerationChunk(
            message=AIMessageChunk(content=result.generations[0].message.content)
        )

# ChatOpenAI çš„æµå¼å®ç°
class ChatOpenAI(BaseChatModel):
    def _stream(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        **kwargs
    ) -> Iterator[ChatGenerationChunk]:
        # è°ƒç”¨ OpenAI æµå¼ API
        response = self.client.chat.completions.create(
            model=self.model,
            messages=self._convert_messages(messages),
            stream=True,  # å¯ç”¨æµå¼
            **kwargs
        )

        # é€å—è¿”å›
        for chunk in response:
            if chunk.choices[0].delta.content:
                yield ChatGenerationChunk(
                    message=AIMessageChunk(
                        content=chunk.choices[0].delta.content
                    )
                )

# ä½¿ç”¨æµå¼è¾“å‡º
for chunk in llm.stream("Tell me a story"):
    print(chunk.content, end="", flush=True)
```

---

### æ ¸å¿ƒæ¦‚å¿µ5ï¼šCallback å›è°ƒé›†æˆ ğŸ“

**BaseChatModel é›†æˆå›è°ƒç³»ç»Ÿè¿½è¸ªæ‰§è¡Œè¿‡ç¨‹**

```python
from langchain_core.callbacks import CallbackManager, BaseCallbackHandler

class BaseChatModel:
    callbacks: Optional[List[BaseCallbackHandler]] = None

    def invoke(self, input, config=None, **kwargs):
        # è·å–å›è°ƒç®¡ç†å™¨
        callback_manager = CallbackManager.configure(
            inheritable_callbacks=self.callbacks,
            local_callbacks=config.get("callbacks") if config else None,
        )

        # å¼€å§‹è¿è¡Œå›è°ƒ
        run_manager = callback_manager.on_chat_model_start(
            serialized=self._serialized,
            messages=messages,
        )

        try:
            # æ‰§è¡Œç”Ÿæˆ
            result = self._generate(messages, run_manager=run_manager, **kwargs)

            # æˆåŠŸå›è°ƒ
            run_manager.on_llm_end(result)
            return result.generations[0].message
        except Exception as e:
            # é”™è¯¯å›è°ƒ
            run_manager.on_llm_error(e)
            raise

# è‡ªå®šä¹‰å›è°ƒå¤„ç†å™¨
class MyCallbackHandler(BaseCallbackHandler):
    def on_chat_model_start(self, serialized, messages, **kwargs):
        print(f"Starting LLM call with {len(messages)} messages")

    def on_llm_end(self, response, **kwargs):
        print(f"LLM finished, generated {len(response.generations)} responses")

    def on_llm_error(self, error, **kwargs):
        print(f"LLM error: {error}")

# ä½¿ç”¨å›è°ƒ
llm = ChatOpenAI(callbacks=[MyCallbackHandler()])
result = llm.invoke("Hello")
```

---

### æ ¸å¿ƒæ¦‚å¿µ6ï¼šbind_tools å·¥å…·ç»‘å®š ğŸ”§

**bind_tools è®©æ¨¡å‹å¯ä»¥è°ƒç”¨å·¥å…·**

```python
from langchain_core.tools import tool

@tool
def get_weather(city: str) -> str:
    """è·å–åŸå¸‚å¤©æ°”"""
    return f"{city}: æ™´å¤©ï¼Œ25Â°C"

@tool
def search(query: str) -> str:
    """æœç´¢ä¿¡æ¯"""
    return f"æœç´¢ç»“æœï¼š{query}"

# ç»‘å®šå·¥å…·åˆ°æ¨¡å‹
llm = ChatOpenAI()
llm_with_tools = llm.bind_tools([get_weather, search])

# è°ƒç”¨æ—¶æ¨¡å‹å¯èƒ½è¿”å›å·¥å…·è°ƒç”¨
result = llm_with_tools.invoke("åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")

# æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
if result.tool_calls:
    for tool_call in result.tool_calls:
        print(f"Tool: {tool_call['name']}")
        print(f"Args: {tool_call['args']}")

# bind_tools çš„å®ç°
class BaseChatModel:
    def bind_tools(
        self,
        tools: List[BaseTool],
        **kwargs
    ) -> "BaseChatModel":
        """ç»‘å®šå·¥å…·åˆ°æ¨¡å‹"""
        # å°†å·¥å…·è½¬æ¢ä¸ºæ¨¡å‹éœ€è¦çš„æ ¼å¼
        formatted_tools = self._convert_tools(tools)
        # è¿”å›ç»‘å®šäº†å·¥å…·çš„æ–°æ¨¡å‹
        return self.bind(tools=formatted_tools, **kwargs)
```

---

### æ‰©å±•æ¦‚å¿µ7ï¼šwith_structured_output ç»“æ„åŒ–è¾“å‡º ğŸ“‹

```python
from pydantic import BaseModel, Field

class Person(BaseModel):
    """äººå‘˜ä¿¡æ¯"""
    name: str = Field(description="å§“å")
    age: int = Field(description="å¹´é¾„")
    hobbies: List[str] = Field(description="çˆ±å¥½åˆ—è¡¨")

# è·å–ç»“æ„åŒ–è¾“å‡º
llm = ChatOpenAI()
structured_llm = llm.with_structured_output(Person)

result = structured_llm.invoke("ä»‹ç»ä¸€ä¸‹å¼ ä¸‰ï¼Œä»–ä»Šå¹´25å²ï¼Œå–œæ¬¢ç¼–ç¨‹å’Œè¯»ä¹¦")
# result æ˜¯ Person å¯¹è±¡
print(result.name)     # "å¼ ä¸‰"
print(result.age)      # 25
print(result.hobbies)  # ["ç¼–ç¨‹", "è¯»ä¹¦"]
```

---

## 4. ã€æœ€å°å¯ç”¨ã€‘

æŒæ¡ä»¥ä¸‹å†…å®¹ï¼Œå°±èƒ½ä½¿ç”¨å’Œç†è§£ BaseChatModelï¼š

### 4.1 åŸºæœ¬è°ƒç”¨

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4", temperature=0.7)

# å­—ç¬¦ä¸²è¾“å…¥
result = llm.invoke("Hello")
print(result.content)

# æ¶ˆæ¯åˆ—è¡¨è¾“å…¥
from langchain_core.messages import HumanMessage, SystemMessage

messages = [
    SystemMessage(content="You are helpful"),
    HumanMessage(content="Hello"),
]
result = llm.invoke(messages)
```

### 4.2 æµå¼è¾“å‡º

```python
for chunk in llm.stream("Tell me a story"):
    print(chunk.content, end="", flush=True)
```

### 4.3 ç»‘å®šå·¥å…·

```python
from langchain_core.tools import tool

@tool
def search(query: str) -> str:
    """æœç´¢ä¿¡æ¯"""
    return f"Result for: {query}"

llm_with_tools = llm.bind_tools([search])
result = llm_with_tools.invoke("Search for Python tutorials")
```

### 4.4 ç»“æ„åŒ–è¾“å‡º

```python
from pydantic import BaseModel

class Answer(BaseModel):
    answer: str
    confidence: float

structured_llm = llm.with_structured_output(Answer)
result = structured_llm.invoke("What is 2+2?")
```

**è¿™äº›çŸ¥è¯†è¶³ä»¥ï¼š**
- ä½¿ç”¨ä»»ä½• LangChain æ”¯æŒçš„èŠå¤©æ¨¡å‹
- å®ç°æµå¼è¾“å‡ºæå‡ç”¨æˆ·ä½“éªŒ
- è®©æ¨¡å‹è°ƒç”¨å·¥å…·ï¼ˆFunction Callingï¼‰
- è·å–ç»“æ„åŒ–çš„ JSON è¾“å‡º

---

## 5. ã€1ä¸ªç±»æ¯”ã€‘ï¼ˆåŒè½¨åˆ¶ï¼‰

### ç±»æ¯”1ï¼šBaseChatModel æŠ½è±¡åŸºç±»

#### ğŸ¨ å‰ç«¯è§†è§’ï¼šReact ç»„ä»¶åŸºç±»

```typescript
// Reactï¼šæ‰€æœ‰ç»„ä»¶ç»§æ‰¿ Component
abstract class Component<Props, State> {
  abstract render(): ReactNode;  // å­ç±»å¿…é¡»å®ç°
  setState(state: State): void;  // åŸºç±»æä¾›
}

class MyComponent extends Component {
  render() {  // å…·ä½“å®ç°
    return <div>Hello</div>;
  }
}
```

```python
# LangChainï¼šæ‰€æœ‰èŠå¤©æ¨¡å‹ç»§æ‰¿ BaseChatModel
class BaseChatModel(ABC):
    @abstractmethod
    def _generate(self, messages): pass  # å­ç±»å¿…é¡»å®ç°

    def invoke(self, input):  # åŸºç±»æä¾›
        messages = self._convert_input(input)
        return self._generate(messages)

class ChatOpenAI(BaseChatModel):
    def _generate(self, messages):  # å…·ä½“å®ç°
        return self.client.chat.completions.create(...)
```

#### ğŸ§’ å°æœ‹å‹è§†è§’ï¼šé¤å…çš„æ ‡å‡†èœè°±

```
BaseChatModel å°±åƒé¤å…çš„æ ‡å‡†èœè°±æ¨¡æ¿ï¼š

æ ‡å‡†èœè°±ï¼ˆBaseChatModelï¼‰è¯´ï¼š
1. æ¥æ”¶é¡¾å®¢ç‚¹çš„èœï¼ˆmessagesï¼‰
2. æŒ‰ç…§æŸç§æ–¹å¼åšèœï¼ˆ_generateï¼‰
3. æŠŠèœç«¯ç»™é¡¾å®¢ï¼ˆè¿”å›ç»“æœï¼‰

ä¸åŒé¤å…ï¼ˆå­ç±»ï¼‰çš„åšæ³•ä¸åŒï¼š
- ä¸­é¤å…ï¼ˆChatOpenAIï¼‰ï¼šç”¨ç‚’é”…åš
- è¥¿é¤å…ï¼ˆChatAnthropicï¼‰ï¼šç”¨çƒ¤ç®±åš
- æ—¥æœ¬æ–™ç†ï¼ˆChatOllamaï¼‰ï¼šç”¨å¯¿å¸æ‰‹æ³•

ä½†å¯¹é¡¾å®¢æ¥è¯´ï¼š
"æˆ‘è¦ä¸€ä»½å®«ä¿é¸¡ä¸"ï¼ˆinvokeï¼‰
ä¸ç®¡å“ªä¸ªé¤å…ï¼Œéƒ½æ˜¯åŒæ ·çš„ç‚¹èœæ–¹å¼ï¼
```

---

### ç±»æ¯”2ï¼šæ¶ˆæ¯ç±»å‹ç³»ç»Ÿ

#### ğŸ¨ å‰ç«¯è§†è§’ï¼šTypeScript Union Types

```typescript
// TypeScriptï¼šæ¶ˆæ¯ç±»å‹è”åˆ
type Message =
  | { type: "user"; content: string }
  | { type: "assistant"; content: string }
  | { type: "system"; content: string };

function processMessage(msg: Message) {
  switch (msg.type) {
    case "user": ...
    case "assistant": ...
    case "system": ...
  }
}
```

#### ğŸ§’ å°æœ‹å‹è§†è§’ï¼šå¯¹è¯ä¸­çš„è§’è‰²

```
æ¶ˆæ¯ç±»å‹å°±åƒå¯¹è¯ä¸­çš„ä¸åŒè§’è‰²ï¼š

SystemMessage = è€å¸ˆï¼ˆè®¾å®šè§„åˆ™ï¼‰
"ä½ è¦è®¤çœŸå¬è®²ï¼Œå›ç­”é—®é¢˜è¦å®Œæ•´"

HumanMessage = å­¦ç”Ÿï¼ˆæé—®ï¼‰
"è€å¸ˆï¼Œ1+1 ç­‰äºå‡ ï¼Ÿ"

AIMessage = AI åŠ©æ‰‹ï¼ˆå›ç­”ï¼‰
"1+1 ç­‰äº 2"

ToolMessage = å°åŠ©æ‰‹ï¼ˆæŸ¥èµ„æ–™åæŠ¥å‘Šï¼‰
"æˆ‘æŸ¥äº†ä¸€ä¸‹ï¼Œç­”æ¡ˆæ˜¯ 2"
```

---

### ç±»æ¯”æ€»ç»“è¡¨

| BaseChatModel æ¦‚å¿µ | å‰ç«¯ç±»æ¯” | å°æœ‹å‹ç±»æ¯” |
|-------------------|---------|-----------|
| BaseChatModel | Component åŸºç±» | æ ‡å‡†èœè°±æ¨¡æ¿ |
| _generate | abstract render() | åšèœçš„å…·ä½“æ–¹æ³• |
| invoke | è°ƒç”¨ç»„ä»¶ | ç‚¹èœ |
| ChatOpenAI | å…·ä½“ç»„ä»¶å®ç° | ä¸­é¤å… |
| messages | props | é¡¾å®¢çš„è¦æ±‚ |
| AIMessage | è¿”å›çš„ ReactNode | ç«¯ä¸Šæ¥çš„èœ |
| stream | Progressive Rendering | ä¸€é“é“ä¸Šèœ |
| bind_tools | æ·»åŠ äº‹ä»¶å¤„ç†å™¨ | åŠ é…èœé€‰é¡¹ |

---

## 6. ã€åç›´è§‰ç‚¹ã€‘

### è¯¯åŒº1ï¼šBaseChatModel åªæ˜¯ API åŒ…è£… âŒ

**ä¸ºä»€ä¹ˆé”™ï¼Ÿ**
- BaseChatModel æä¾›äº†å®Œæ•´çš„æ‰§è¡Œæ¡†æ¶
- é›†æˆäº†å›è°ƒç³»ç»Ÿã€é…ç½®ä¼ é€’ã€é”™è¯¯å¤„ç†
- å®ç°äº† Runnable åè®®ï¼Œå¯å‚ä¸ LCEL ç®¡é“

**æ­£ç¡®ç†è§£ï¼š**
```python
# ä¸åªæ˜¯åŒ…è£… API
llm = ChatOpenAI()

# è‡ªåŠ¨è·å¾—è¿™äº›èƒ½åŠ›
llm.invoke(input)           # åŒæ­¥
llm.stream(input)           # æµå¼
llm.batch(inputs)           # æ‰¹é‡
await llm.ainvoke(input)    # å¼‚æ­¥
llm.with_config(...)        # é…ç½®
llm.bind_tools(tools)       # å·¥å…·ç»‘å®š

# å‚ä¸ LCEL ç®¡é“
chain = prompt | llm | parser
```

---

### è¯¯åŒº2ï¼šæ‰€æœ‰æ¨¡å‹çš„æ¶ˆæ¯æ ¼å¼ç›¸åŒ âŒ

**ä¸ºä»€ä¹ˆé”™ï¼Ÿ**
- ä¸åŒ LLM æä¾›å•†çš„æ¶ˆæ¯æ ¼å¼ä¸åŒ
- BaseChatModel è´Ÿè´£æ ¼å¼è½¬æ¢
- LangChain æ¶ˆæ¯æ˜¯ä¸­é—´å±‚æŠ½è±¡

**æ­£ç¡®ç†è§£ï¼š**
```python
# LangChain ç»Ÿä¸€æ ¼å¼
messages = [HumanMessage(content="Hello")]

# OpenAI æ ¼å¼
[{"role": "user", "content": "Hello"}]

# Anthropic æ ¼å¼
[{"role": "user", "content": [{"type": "text", "text": "Hello"}]}]

# BaseChatModel è‡ªåŠ¨è½¬æ¢
result = llm.invoke(messages)  # å†…éƒ¨ä¼šè½¬æ¢æ ¼å¼
```

---

### è¯¯åŒº3ï¼šstream å’Œ invoke å®Œå…¨ä¸åŒ âŒ

**ä¸ºä»€ä¹ˆé”™ï¼Ÿ**
- stream æœ€ç»ˆç»“æœå’Œ invoke ç›¸åŒ
- stream åªæ˜¯åˆ†å—è¿”å›
- å¯ä»¥åˆå¹¶ stream ç»“æœ

**æ­£ç¡®ç†è§£ï¼š**
```python
# invoke ä¸€æ¬¡æ€§è¿”å›
result = llm.invoke("Hello")
print(result.content)

# stream åˆ†å—è¿”å›ï¼Œä½†æœ€ç»ˆå†…å®¹ç›¸åŒ
chunks = list(llm.stream("Hello"))
full_content = "".join(chunk.content for chunk in chunks)
# full_content == result.content
```

---

## 7. ã€å®æˆ˜ä»£ç ã€‘

```python
"""
ç¤ºä¾‹ï¼šå®ç°ç®€åŒ–ç‰ˆ BaseChatModel
æ¼”ç¤ºèŠå¤©æ¨¡å‹çš„æ ¸å¿ƒæ¶æ„
"""

from abc import ABC, abstractmethod
from typing import List, Optional, Iterator, Any, Dict
from dataclasses import dataclass

# ===== 1. æ¶ˆæ¯ç±»å‹ =====
print("=== 1. æ¶ˆæ¯ç±»å‹ç³»ç»Ÿ ===")

@dataclass
class BaseMessage:
    content: str
    type: str = "base"

@dataclass
class HumanMessage(BaseMessage):
    type: str = "human"

@dataclass
class AIMessage(BaseMessage):
    type: str = "ai"

@dataclass
class SystemMessage(BaseMessage):
    type: str = "system"

@dataclass
class AIMessageChunk(BaseMessage):
    type: str = "ai_chunk"

# ===== 2. ChatResult =====
@dataclass
class ChatGeneration:
    message: BaseMessage

@dataclass
class ChatResult:
    generations: List[ChatGeneration]
    llm_output: Optional[Dict] = None

# ===== 3. BaseChatModel =====
print("\n=== 2. BaseChatModel åŸºç±» ===")

class BaseChatModel(ABC):
    """èŠå¤©æ¨¡å‹æŠ½è±¡åŸºç±»"""

    @abstractmethod
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        **kwargs
    ) -> ChatResult:
        """å­ç±»å¿…é¡»å®ç°çš„æ ¸å¿ƒæ–¹æ³•"""
        pass

    @property
    @abstractmethod
    def _llm_type(self) -> str:
        """æ¨¡å‹ç±»å‹æ ‡è¯†"""
        pass

    def invoke(self, input: Any, **kwargs) -> BaseMessage:
        """ç»Ÿä¸€è°ƒç”¨æ¥å£"""
        messages = self._convert_input(input)
        result = self._generate(messages, **kwargs)
        return result.generations[0].message

    def stream(self, input: Any, **kwargs) -> Iterator[AIMessageChunk]:
        """æµå¼è¾“å‡º"""
        messages = self._convert_input(input)
        for chunk in self._stream(messages, **kwargs):
            yield chunk

    def _stream(
        self,
        messages: List[BaseMessage],
        **kwargs
    ) -> Iterator[AIMessageChunk]:
        """é»˜è®¤æµå¼å®ç°ï¼ˆå­ç±»å¯é‡å†™ï¼‰"""
        result = self._generate(messages, **kwargs)
        content = result.generations[0].message.content
        for char in content:
            yield AIMessageChunk(content=char)

    def _convert_input(self, input: Any) -> List[BaseMessage]:
        """è¾“å…¥è½¬æ¢"""
        if isinstance(input, str):
            return [HumanMessage(content=input)]
        elif isinstance(input, list):
            return input
        elif isinstance(input, BaseMessage):
            return [input]
        else:
            raise ValueError(f"Unsupported input type: {type(input)}")

    def batch(self, inputs: List[Any], **kwargs) -> List[BaseMessage]:
        """æ‰¹é‡å¤„ç†"""
        return [self.invoke(inp, **kwargs) for inp in inputs]

# ===== 4. å…·ä½“å®ç° =====
print("\n=== 3. å…·ä½“æ¨¡å‹å®ç° ===")

class FakeChatOpenAI(BaseChatModel):
    """æ¨¡æ‹Ÿ ChatOpenAI"""

    model: str = "gpt-4"
    temperature: float = 0.7

    def __init__(self, model: str = "gpt-4", temperature: float = 0.7):
        self.model = model
        self.temperature = temperature

    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        **kwargs
    ) -> ChatResult:
        # æ¨¡æ‹Ÿ API è°ƒç”¨
        last_message = messages[-1].content
        response_content = f"[{self.model}] Response to: {last_message[:30]}..."

        return ChatResult(
            generations=[ChatGeneration(message=AIMessage(content=response_content))],
            llm_output={"model": self.model, "tokens": len(response_content)}
        )

    @property
    def _llm_type(self) -> str:
        return "fake-openai"

class FakeChatAnthropic(BaseChatModel):
    """æ¨¡æ‹Ÿ ChatAnthropic"""

    model: str = "claude-3"

    def __init__(self, model: str = "claude-3"):
        self.model = model

    def _generate(
        self,
        messages: List[BaseMessage],
        **kwargs
    ) -> ChatResult:
        last_message = messages[-1].content
        response_content = f"[Claude] I understand: {last_message[:30]}..."

        return ChatResult(
            generations=[ChatGeneration(message=AIMessage(content=response_content))]
        )

    @property
    def _llm_type(self) -> str:
        return "fake-anthropic"

# ===== 5. ä½¿ç”¨ç¤ºä¾‹ =====
print("\n=== 4. ç»Ÿä¸€æ¥å£è°ƒç”¨ ===")

def process_query(llm: BaseChatModel, query: str) -> str:
    """ç»Ÿä¸€å¤„ç†å‡½æ•°ï¼šä¸å…³å¿ƒå…·ä½“æ¨¡å‹"""
    return llm.invoke(query).content

# ä½¿ç”¨ä¸åŒæ¨¡å‹
openai_llm = FakeChatOpenAI(model="gpt-4")
anthropic_llm = FakeChatAnthropic(model="claude-3-opus")

print(f"OpenAI: {process_query(openai_llm, 'Hello')}")
print(f"Anthropic: {process_query(anthropic_llm, 'Hello')}")

# ===== 6. æ¶ˆæ¯åˆ—è¡¨è¾“å…¥ =====
print("\n=== 5. æ¶ˆæ¯åˆ—è¡¨è¾“å…¥ ===")

messages = [
    SystemMessage(content="You are helpful"),
    HumanMessage(content="What is Python?"),
]

result = openai_llm.invoke(messages)
print(f"Result: {result.content}")

# ===== 7. æµå¼è¾“å‡º =====
print("\n=== 6. æµå¼è¾“å‡º ===")

print("Streaming: ", end="")
for chunk in openai_llm.stream("Tell me about AI"):
    print(chunk.content, end="", flush=True)
print()

# ===== 8. æ‰¹é‡å¤„ç† =====
print("\n=== 7. æ‰¹é‡å¤„ç† ===")

queries = ["Hello", "What is Python?", "How are you?"]
results = openai_llm.batch(queries)
for query, result in zip(queries, results):
    print(f"  Q: {query[:20]}... -> A: {result.content[:30]}...")

print("\n=== å®Œæˆ ===")
```

---

## 8. ã€é¢è¯•å¿…é—®ã€‘

### é—®é¢˜ï¼š"LangChain çš„ BaseChatModel æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ"

**æ™®é€šå›ç­”ï¼ˆâŒ ä¸å‡ºå½©ï¼‰ï¼š**
"BaseChatModel æ˜¯èŠå¤©æ¨¡å‹çš„åŸºç±»ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½ç»§æ‰¿å®ƒã€‚"

**å‡ºå½©å›ç­”ï¼ˆâœ… æ¨èï¼‰ï¼š**

> **BaseChatModel æœ‰ä¸‰ä¸ªè®¾è®¡ç›®æ ‡ï¼š**
>
> 1. **ç»Ÿä¸€æ¥å£**ï¼šå±è”½ä¸åŒ LLM æä¾›å•†çš„ API å·®å¼‚
>    - OpenAIã€Anthropicã€Ollama ç­‰éƒ½æœ‰ä¸åŒçš„ API
>    - BaseChatModel æä¾›ç»Ÿä¸€çš„ invoke/stream æ–¹æ³•
>
> 2. **æ¨¡æ¿æ–¹æ³•æ¨¡å¼**ï¼š
>    - æŠ½è±¡æ–¹æ³• `_generate`ï¼šå­ç±»å®ç°å…·ä½“ API è°ƒç”¨
>    - å…¬å¼€æ–¹æ³• `invoke`ï¼šå¤„ç†è¾“å…¥è½¬æ¢ã€å›è°ƒã€é”™è¯¯å¤„ç†
>
> 3. **Runnable é›†æˆ**ï¼š
>    - ç»§æ‰¿ Runnable åè®®ï¼Œå¯å‚ä¸ LCEL ç®¡é“
>    - è‡ªåŠ¨è·å¾— batch/stream/ainvoke ç­‰èƒ½åŠ›
>
> **å®é™…ä¾‹å­**ï¼š
> ```python
> # å¯ä»¥æ— ç¼åˆ‡æ¢æ¨¡å‹
> chain = prompt | llm | parser
> # llm å¯ä»¥æ˜¯ ChatOpenAIã€ChatAnthropic ç­‰ä»»ä½•å®ç°
> ```

---

## 9. ã€åŒ–éª¨ç»µæŒã€‘

### å¡ç‰‡1ï¼šBaseChatModel æ˜¯ä»€ä¹ˆ ğŸ¯

**ä¸€å¥è¯ï¼š** BaseChatModel æ˜¯æ‰€æœ‰èŠå¤©æ¨¡å‹çš„æŠ½è±¡åŸºç±»ã€‚

**æ ¸å¿ƒæ–¹æ³•ï¼š**
- `_generate`ï¼šå­ç±»å¿…é¡»å®ç°
- `invoke`ï¼šç»Ÿä¸€è°ƒç”¨æ¥å£

**åº”ç”¨ï¼š** ChatOpenAIã€ChatAnthropic éƒ½ç»§æ‰¿è‡ªå®ƒã€‚

---

### å¡ç‰‡2ï¼š_generate æ ¸å¿ƒæ–¹æ³• ğŸ“

**ä¸€å¥è¯ï¼š** _generate æ˜¯å­ç±»å¿…é¡»å®ç°çš„æ ¸å¿ƒç”Ÿæˆæ–¹æ³•ã€‚

**ç­¾åï¼š**
```python
def _generate(self, messages, stop, **kwargs) -> ChatResult
```

**åº”ç”¨ï¼š** å­ç±»åœ¨è¿™é‡Œè°ƒç”¨å…·ä½“çš„ LLM APIã€‚

---

### å¡ç‰‡3ï¼šæ¶ˆæ¯ç±»å‹ ğŸ’¬

**ä¸€å¥è¯ï¼š** LangChain å®šä¹‰äº†ç»Ÿä¸€çš„æ¶ˆæ¯ç±»å‹ã€‚

**ç±»å‹ï¼š**
- `HumanMessage`ï¼šç”¨æˆ·æ¶ˆæ¯
- `AIMessage`ï¼šAI æ¶ˆæ¯
- `SystemMessage`ï¼šç³»ç»Ÿæ¶ˆæ¯

**åº”ç”¨ï¼š** æ‰€æœ‰æ¨¡å‹ä½¿ç”¨ç›¸åŒçš„æ¶ˆæ¯æ ¼å¼ã€‚

---

### å¡ç‰‡4ï¼šinvoke æ–¹æ³• ğŸ”§

**ä¸€å¥è¯ï¼š** invoke æ˜¯ç»Ÿä¸€çš„è°ƒç”¨æ¥å£ã€‚

**æµç¨‹ï¼š**
1. è½¬æ¢è¾“å…¥ä¸ºæ¶ˆæ¯åˆ—è¡¨
2. è°ƒç”¨ _generate
3. è¿”å› AI æ¶ˆæ¯

**åº”ç”¨ï¼š** `llm.invoke("Hello")` æˆ– `llm.invoke(messages)`

---

### å¡ç‰‡5ï¼šstream æµå¼è¾“å‡º ğŸŒŠ

**ä¸€å¥è¯ï¼š** stream é€å—è¿”å› LLM å“åº”ã€‚

**ç”¨æ³•ï¼š**
```python
for chunk in llm.stream("Hello"):
    print(chunk.content, end="")
```

**åº”ç”¨ï¼š** å®æ—¶æ˜¾ç¤º AI å“åº”ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

---

### å¡ç‰‡6ï¼šbind_tools å·¥å…·ç»‘å®š ğŸ”§

**ä¸€å¥è¯ï¼š** bind_tools è®©æ¨¡å‹å¯ä»¥è°ƒç”¨å·¥å…·ã€‚

**ç”¨æ³•ï¼š**
```python
llm_with_tools = llm.bind_tools([search, calculator])
```

**åº”ç”¨ï¼š** å®ç° Function Calling / Tool Useã€‚

---

### å¡ç‰‡7ï¼šwith_structured_output ğŸ“‹

**ä¸€å¥è¯ï¼š** è·å–ç»“æ„åŒ–çš„ JSON è¾“å‡ºã€‚

**ç”¨æ³•ï¼š**
```python
structured_llm = llm.with_structured_output(Person)
result = structured_llm.invoke("...")  # è¿”å› Person å¯¹è±¡
```

**åº”ç”¨ï¼š** ä» LLM è¾“å‡ºä¸­æå–ç»“æ„åŒ–æ•°æ®ã€‚

---

### å¡ç‰‡8ï¼šCallback å›è°ƒ ğŸ“

**ä¸€å¥è¯ï¼š** BaseChatModel é›†æˆå›è°ƒç³»ç»Ÿè¿½è¸ªæ‰§è¡Œã€‚

**äº‹ä»¶ï¼š**
- `on_chat_model_start`
- `on_llm_end`
- `on_llm_error`

**åº”ç”¨ï¼š** ç›‘æ§ã€æ—¥å¿—ã€è¿½è¸ª LLM è°ƒç”¨ã€‚

---

### å¡ç‰‡9ï¼šRunnable é›†æˆ ğŸ”—

**ä¸€å¥è¯ï¼š** BaseChatModel å®ç° Runnable åè®®ã€‚

**èƒ½åŠ›ï¼š**
- å¯ä»¥ç”¨ `|` ç»„åˆ
- è‡ªåŠ¨è·å¾— batch/ainvoke

**åº”ç”¨ï¼š** `chain = prompt | llm | parser`

---

### å¡ç‰‡10ï¼šæ¨¡å‹åˆ‡æ¢ â­

**ä¸€å¥è¯ï¼š** ç»Ÿä¸€æ¥å£è®©æ¨¡å‹åˆ‡æ¢å˜å¾—ç®€å•ã€‚

**ç¤ºä¾‹ï¼š**
```python
# åªéœ€æ”¹ä¸€è¡Œ
llm = ChatOpenAI()  # åˆ‡æ¢ä¸º
llm = ChatAnthropic()

# å…¶ä»–ä»£ç ä¸å˜
chain = prompt | llm | parser
```

**åº”ç”¨ï¼š** çµæ´»é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹ã€‚

---

## 10. ã€ä¸€å¥è¯æ€»ç»“ã€‘

**BaseChatModel æ˜¯ LangChain èŠå¤©æ¨¡å‹çš„æŠ½è±¡åŸºç±»ï¼Œé€šè¿‡æ¨¡æ¿æ–¹æ³•æ¨¡å¼ç»Ÿä¸€ä¸åŒ LLM çš„æ¥å£ï¼Œé›†æˆ Runnable åè®®å‚ä¸ LCEL ç®¡é“ï¼Œæ˜¯æ„å»º LLM åº”ç”¨çš„æ ¸å¿ƒç»„ä»¶ã€‚**

---

## ğŸ“š å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£ BaseChatModel çš„è®¾è®¡ç›®çš„
- [ ] ä¼šä½¿ç”¨ invoke è°ƒç”¨èŠå¤©æ¨¡å‹
- [ ] ç†è§£ _generate æŠ½è±¡æ–¹æ³•çš„ä½œç”¨
- [ ] æŒæ¡æ¶ˆæ¯ç±»å‹ç³»ç»Ÿï¼ˆHuman/AI/Systemï¼‰
- [ ] ä¼šä½¿ç”¨ stream å®ç°æµå¼è¾“å‡º
- [ ] ä¼šä½¿ç”¨ bind_tools ç»‘å®šå·¥å…·
- [ ] ä¼šä½¿ç”¨ with_structured_output è·å–ç»“æ„åŒ–è¾“å‡º
- [ ] ç†è§£ BaseChatModel ä¸ Runnable çš„å…³ç³»

## ğŸ”— ä¸‹ä¸€æ­¥å­¦ä¹ 

- **Agent æ‰§è¡Œå¼•æ“**ï¼šç†è§£ Agent å¦‚ä½•ä½¿ç”¨ ChatModel
- **Callback å›è°ƒç³»ç»Ÿ**ï¼šæ·±å…¥ç†è§£æ‰§è¡Œè¿½è¸ªæœºåˆ¶
- **Tool Use**ï¼šå­¦ä¹ å¦‚ä½•å®ç°å·¥å…·è°ƒç”¨

---

**ç‰ˆæœ¬ï¼š** v1.0
**æœ€åæ›´æ–°ï¼š** 2025-12-12
