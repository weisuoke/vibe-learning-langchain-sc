# LCEL è¡¨è¾¾å¼è¯­è¨€

> åŸå­åŒ–çŸ¥è¯†ç‚¹ | LangChain æºç  | LangChain Expression Language

---

## 1. ã€30å­—æ ¸å¿ƒã€‘

**LCEL æ˜¯ LangChain çš„å£°æ˜å¼ç»„åˆè¯­æ³•ï¼Œé€šè¿‡ | æ“ä½œç¬¦å°† Runnable ç»„ä»¶ä¸²è”æˆå¯æ‰§è¡Œçš„å¤„ç†ç®¡é“ã€‚**

---

## 2. ã€ç¬¬ä¸€æ€§åŸç†ã€‘

### ä»€ä¹ˆæ˜¯ç¬¬ä¸€æ€§åŸç†ï¼Ÿ

**ç¬¬ä¸€æ€§åŸç†**ï¼šå›åˆ°äº‹ç‰©æœ€åŸºæœ¬çš„çœŸç†ï¼Œä»æºå¤´æ€è€ƒé—®é¢˜

### LCEL çš„ç¬¬ä¸€æ€§åŸç† ğŸ¯

#### 1. æœ€åŸºç¡€çš„å®šä¹‰

**LCEL = ç»„ä»¶ + ç»„åˆè§„åˆ™**

ä»…æ­¤è€Œå·²ï¼æ²¡æœ‰æ›´åŸºç¡€çš„äº†ã€‚

- **ç»„ä»¶ (Component)**ï¼šå®ç° Runnable åè®®çš„ä»»ä½•å¯¹è±¡
- **ç»„åˆè§„åˆ™ (Composition)**ï¼šç”¨ `|` ä¸²è”ï¼Œç”¨ `RunnableParallel` å¹¶è¡Œ

```python
# LCEL çš„æœ¬è´¨
chain = component_a | component_b | component_c
# ç­‰ä»·äºï¼šè¾“å…¥ â†’ Aå¤„ç† â†’ Bå¤„ç† â†’ Cå¤„ç† â†’ è¾“å‡º
```

#### 2. ä¸ºä»€ä¹ˆéœ€è¦ LCELï¼Ÿ

**æ ¸å¿ƒé—®é¢˜ï¼šå¦‚ä½•ç®€æ´åœ°è¡¨è¾¾å¤æ‚çš„ LLM å¤„ç†æµç¨‹ï¼Ÿ**

```python
# æ²¡æœ‰ LCEL çš„å†™æ³•ï¼ˆå‘½ä»¤å¼ï¼‰
def process(input_data):
    # 1. æ ¼å¼åŒ–æç¤º
    prompt_result = prompt_template.format(**input_data)

    # 2. è°ƒç”¨ LLM
    llm_result = llm.generate(prompt_result)

    # 3. è§£æè¾“å‡º
    parsed_result = parser.parse(llm_result)

    # 4. åå¤„ç†
    final_result = postprocess(parsed_result)

    return final_result

# é—®é¢˜ï¼š
# 1. ä»£ç å†—é•¿ï¼Œæµç¨‹ä¸ç›´è§‚
# 2. éš¾ä»¥å¤ç”¨å’Œä¿®æ”¹
# 3. æµå¼å¤„ç†ã€æ‰¹é‡å¤„ç†éœ€è¦é‡å†™
# 4. é”™è¯¯å¤„ç†å’Œå›è°ƒéœ€è¦æ‰‹åŠ¨æ·»åŠ 
```

```python
# æœ‰äº† LCELï¼ˆå£°æ˜å¼ï¼‰
chain = prompt | llm | parser | postprocess

result = chain.invoke(input_data)

# ä¼˜åŠ¿ï¼š
# 1. ä¸€è¡Œä»£ç è¡¨è¾¾å®Œæ•´æµç¨‹
# 2. è‡ªåŠ¨è·å¾— stream/batch/ainvoke
# 3. é…ç½®å’Œå›è°ƒè‡ªåŠ¨ä¼ é€’
# 4. æ˜“äºä¿®æ”¹å’Œå¤ç”¨
```

#### 3. LCEL çš„ä¸‰å±‚ä»·å€¼

##### ä»·å€¼1ï¼šå£°æ˜å¼è¯­æ³• - ä»£ç å³æ–‡æ¡£

```python
# ä»£ç æœ¬èº«å°±è¯´æ˜äº†å¤„ç†æµç¨‹
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | parser
)
# ä¸€çœ¼å°±èƒ½çœ‹å‡ºï¼šæ£€ç´¢ â†’ æç¤º â†’ LLM â†’ è§£æ
```

##### ä»·å€¼2ï¼šè‡ªåŠ¨èƒ½åŠ›ç»§æ‰¿ - å†™ä¸€æ¬¡ï¼Œå¤šç§æ‰§è¡Œ

```python
# å®šä¹‰ä¸€æ¬¡
chain = prompt | llm | parser

# è‡ªåŠ¨è·å¾—å¤šç§æ‰§è¡Œæ–¹å¼
chain.invoke(input)              # åŒæ­¥
chain.stream(input)              # æµå¼
chain.batch([input1, input2])    # æ‰¹é‡
await chain.ainvoke(input)       # å¼‚æ­¥
```

##### ä»·å€¼3ï¼šå¯ç»„åˆæ€§ - ç®¡é“å¯ä»¥åµŒå¥—

```python
# å­ç®¡é“
summarize = prompt1 | llm | parser1
translate = prompt2 | llm | parser2

# ç»„åˆæˆæ›´å¤§çš„ç®¡é“
full_chain = (
    RunnableParallel(
        summary=summarize,
        translation=translate
    )
    | combine_results
)
```

#### 4. ä»ç¬¬ä¸€æ€§åŸç†æ¨å¯¼ LCEL è®¾è®¡

**æ¨ç†é“¾ï¼š**

```
1. LLM åº”ç”¨æœ¬è´¨æ˜¯æ•°æ®å¤„ç†ç®¡é“
   â†“
2. ç®¡é“ç”±å¤šä¸ªå¤„ç†æ­¥éª¤ç»„æˆ
   â†“
3. éœ€è¦ä¸€ç§ç®€æ´çš„æ–¹å¼è¡¨è¾¾ç®¡é“
   â†“
4. Unix ç®¡é“ç”¨ | è¿æ¥å‘½ä»¤ï¼Œç›´è§‚æ˜“æ‡‚
   â†“
5. Python æ”¯æŒæ“ä½œç¬¦é‡è½½ï¼ˆ__or__ï¼‰
   â†“
6. è®©æ‰€æœ‰ç»„ä»¶å®ç° Runnable åè®®
   â†“
7. åœ¨ Runnable ä¸­é‡è½½ | æ“ä½œç¬¦
   â†“
8. LCEL è¯ç”Ÿï¼šprompt | llm | parser
```

#### 5. ä¸€å¥è¯æ€»ç»“ç¬¬ä¸€æ€§åŸç†

**LCEL æ˜¯ç”¨ | æ“ä½œç¬¦è¡¨è¾¾æ•°æ®å¤„ç†ç®¡é“çš„å£°æ˜å¼è¯­æ³•ï¼Œè®©å¤æ‚çš„ LLM åº”ç”¨æµç¨‹å˜å¾—ç®€æ´ã€å¯è¯»ã€å¯ç»„åˆã€‚**

---

## 3. ã€æ ¸å¿ƒæ¦‚å¿µï¼ˆå…¨é¢è¦†ç›–ï¼‰ã€‘

### æ ¸å¿ƒæ¦‚å¿µ1ï¼šç®¡é“æ“ä½œç¬¦ | ğŸ”—

**| æ“ä½œç¬¦å°†ä¸¤ä¸ª Runnable ä¸²è”æˆ RunnableSequence**

```python
from langchain_core.runnables import Runnable, RunnableSequence

# | æ“ä½œç¬¦çš„å®ç°åŸç†
class Runnable:
    def __or__(self, other: "Runnable") -> RunnableSequence:
        """a | b æ—¶è°ƒç”¨ a.__or__(b)"""
        return RunnableSequence(first=self, last=other)

    def __ror__(self, other) -> RunnableSequence:
        """å½“å·¦æ“ä½œæ•°ä¸æ˜¯ Runnable æ—¶è°ƒç”¨"""
        # å°†å·¦æ“ä½œæ•°è½¬æ¢ä¸º Runnable
        return RunnableSequence(
            first=coerce_to_runnable(other),
            last=self
        )

# ä½¿ç”¨ç¤ºä¾‹
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_template("Tell me about {topic}")
llm = ChatOpenAI()
parser = StrOutputParser()

# åˆ›å»ºç®¡é“
chain = prompt | llm | parser

# æ‰§è¡Œæµç¨‹
# input â†’ prompt.invoke() â†’ llm.invoke() â†’ parser.invoke() â†’ output
```

**é“¾å¼è°ƒç”¨çš„å±•å¼€ï¼š**

```python
# prompt | llm | parser çš„å±•å¼€è¿‡ç¨‹

# ç¬¬ä¸€æ­¥ï¼šprompt | llm
step1 = prompt.__or__(llm)
# step1 = RunnableSequence(first=prompt, last=llm)

# ç¬¬äºŒæ­¥ï¼šstep1 | parser
step2 = step1.__or__(parser)
# step2 = RunnableSequence(
#     first=prompt,
#     middle=[llm],
#     last=parser
# )
```

**åœ¨ LangChain æºç ä¸­çš„ä½ç½®ï¼š**

```python
# langchain_core/runnables/base.py
class Runnable(Generic[Input, Output], ABC):
    def __or__(
        self,
        other: Union[
            Runnable[Any, Other],
            Callable[[Any], Other],
            Callable[[Iterator[Any]], Iterator[Other]],
            Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other]]],
        ],
    ) -> RunnableSerializable[Input, Other]:
        return RunnableSequence(self, coerce_to_runnable(other))
```

---

### æ ¸å¿ƒæ¦‚å¿µ2ï¼šRunnableSequence ä¸²è¡Œç»„åˆ ğŸ“

**RunnableSequence æ˜¯ LCEL ç®¡é“çš„æ ¸å¿ƒæ•°æ®ç»“æ„**

```python
from typing import List, Any, Optional, Iterator

class RunnableSequence(Runnable[Input, Output]):
    """ä¸²è¡Œæ‰§è¡Œå¤šä¸ª Runnable

    æ•°æ®æµï¼šinput â†’ first â†’ middle[0] â†’ ... â†’ middle[n] â†’ last â†’ output
    """

    first: Runnable[Input, Any]
    middle: List[Runnable[Any, Any]]
    last: Runnable[Any, Output]

    def __init__(
        self,
        *steps: Runnable,
        first: Runnable = None,
        middle: List[Runnable] = None,
        last: Runnable = None
    ):
        if steps:
            # ä»ä½ç½®å‚æ•°æ„å»º
            self.first = steps[0]
            self.middle = list(steps[1:-1]) if len(steps) > 2 else []
            self.last = steps[-1] if len(steps) > 1 else steps[0]
        else:
            # ä»å…³é”®å­—å‚æ•°æ„å»º
            self.first = first
            self.middle = middle or []
            self.last = last

    @property
    def steps(self) -> List[Runnable]:
        """æ‰€æœ‰æ­¥éª¤çš„åˆ—è¡¨"""
        return [self.first] + self.middle + [self.last]

    def invoke(self, input: Input, config: Optional[dict] = None) -> Output:
        """ä¸²è¡Œæ‰§è¡Œæ‰€æœ‰æ­¥éª¤"""
        result = input
        for step in self.steps:
            result = step.invoke(result, config)
        return result

    def stream(self, input: Input, config: Optional[dict] = None) -> Iterator[Output]:
        """æµå¼æ‰§è¡Œï¼šåªæœ‰æœ€åä¸€æ­¥æµå¼è¾“å‡º"""
        # å‰é¢çš„æ­¥éª¤æ­£å¸¸æ‰§è¡Œ
        result = input
        for step in self.steps[:-1]:
            result = step.invoke(result, config)

        # æœ€åä¸€æ­¥æµå¼è¾“å‡º
        for chunk in self.last.stream(result, config):
            yield chunk

    @property
    def input_schema(self):
        """è¾“å…¥ schema ç”±ç¬¬ä¸€æ­¥å†³å®š"""
        return self.first.input_schema

    @property
    def output_schema(self):
        """è¾“å‡º schema ç”±æœ€åä¸€æ­¥å†³å®š"""
        return self.last.output_schema

# ä½¿ç”¨ç¤ºä¾‹
chain = prompt | llm | parser

# æŸ¥çœ‹ç»“æ„
print(chain.steps)  # [prompt, llm, parser]
print(chain.first)  # prompt
print(chain.last)   # parser
```

---

### æ ¸å¿ƒæ¦‚å¿µ3ï¼šRunnableParallel å¹¶è¡Œç»„åˆ ğŸ”€

**RunnableParallel å¹¶è¡Œæ‰§è¡Œå¤šä¸ªåˆ†æ”¯ï¼Œç»“æœåˆå¹¶ä¸ºå­—å…¸**

```python
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

class RunnableParallel(Runnable[Input, Dict[str, Any]]):
    """å¹¶è¡Œæ‰§è¡Œå¤šä¸ª Runnable

    è¾“å…¥ï¼šåŒä¸€ä¸ª input ä¼ ç»™æ‰€æœ‰åˆ†æ”¯
    è¾“å‡ºï¼š{"key1": result1, "key2": result2, ...}
    """

    steps: Dict[str, Runnable]

    def __init__(self, steps: Dict[str, Runnable] = None, **kwargs):
        self.steps = steps or kwargs

    def invoke(self, input: Input, config: Optional[dict] = None) -> Dict[str, Any]:
        """å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰åˆ†æ”¯"""
        return {
            key: step.invoke(input, config)
            for key, step in self.steps.items()
        }

    async def ainvoke(self, input: Input, config: Optional[dict] = None) -> Dict[str, Any]:
        """çœŸæ­£çš„å¹¶è¡Œæ‰§è¡Œï¼ˆå¼‚æ­¥ï¼‰"""
        import asyncio

        async def run_step(key: str, step: Runnable):
            result = await step.ainvoke(input, config)
            return key, result

        results = await asyncio.gather(*[
            run_step(k, s) for k, s in self.steps.items()
        ])
        return dict(results)

# ä½¿ç”¨ç¤ºä¾‹

# æ–¹å¼1ï¼šå­—å…¸è¯­æ³•
parallel = RunnableParallel({
    "summary": summary_chain,
    "keywords": keyword_chain,
    "sentiment": sentiment_chain,
})

# æ–¹å¼2ï¼šå…³é”®å­—å‚æ•°
parallel = RunnableParallel(
    summary=summary_chain,
    keywords=keyword_chain,
    sentiment=sentiment_chain,
)

# æ–¹å¼3ï¼šç›´æ¥ç”¨å­—å…¸ï¼ˆè‡ªåŠ¨è½¬æ¢ï¼‰
chain = {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
```

**RAG ç»å…¸æ¨¡å¼ï¼š**

```python
from langchain_core.runnables import RunnablePassthrough

# RAG ç®¡é“çš„æ ‡å‡†å†™æ³•
rag_chain = (
    RunnableParallel(
        context=retriever | format_docs,    # æ£€ç´¢å¹¶æ ¼å¼åŒ–
        question=RunnablePassthrough()       # åŸæ ·ä¼ é€’é—®é¢˜
    )
    | prompt    # ä½¿ç”¨ context å’Œ question
    | llm
    | parser
)

# è¾“å…¥
input_data = {"question": "What is LangChain?"}

# æ‰§è¡Œæµç¨‹
# 1. RunnableParallel å¹¶è¡Œæ‰§è¡Œï¼š
#    - context: retriever.invoke(input) | format_docs.invoke(docs)
#    - question: RunnablePassthrough().invoke(input) â†’ input
# 2. ç»“æœåˆå¹¶ï¼š{"context": "...", "question": {"question": "..."}}
# 3. prompt.invoke(merged) â†’ formatted_prompt
# 4. llm.invoke(formatted_prompt) â†’ response
# 5. parser.invoke(response) â†’ final_result
```

---

### æ ¸å¿ƒæ¦‚å¿µ4ï¼šRunnablePassthrough æ•°æ®é€ä¼  â¡ï¸

**RunnablePassthrough åŸæ ·ä¼ é€’è¾“å…¥ï¼Œæ˜¯æ„å»ºå¤æ‚ç®¡é“çš„å…³é”®å·¥å…·**

```python
class RunnablePassthrough(Runnable[Input, Input]):
    """åŸæ ·ä¼ é€’è¾“å…¥

    çœ‹èµ·æ¥ä»€ä¹ˆéƒ½ä¸åšï¼Œä½†åœ¨æ„å»ºç®¡é“æ—¶éå¸¸æœ‰ç”¨
    """

    def invoke(self, input: Input, config: Optional[dict] = None) -> Input:
        return input

    @classmethod
    def assign(cls, **kwargs: Runnable) -> "RunnableAssign":
        """åœ¨è¾“å…¥åŸºç¡€ä¸Šæ·»åŠ æ–°å­—æ®µ"""
        return RunnableAssign(mapper=RunnableParallel(kwargs))

# ä½¿ç”¨åœºæ™¯1ï¼šåœ¨ RunnableParallel ä¸­ä¿ç•™åŸå§‹è¾“å…¥
chain = RunnableParallel(
    processed=some_processor,
    original=RunnablePassthrough()  # ä¿ç•™åŸå§‹è¾“å…¥
)

# ä½¿ç”¨åœºæ™¯2ï¼šRAG ä¸­ä¿ç•™é—®é¢˜
rag = RunnableParallel(
    context=retriever,
    question=RunnablePassthrough()  # é—®é¢˜åŸæ ·ä¼ é€’
) | prompt | llm

# ä½¿ç”¨åœºæ™¯3ï¼šassign æ·»åŠ å­—æ®µ
chain = RunnablePassthrough.assign(
    context=retriever  # åœ¨åŸè¾“å…¥åŸºç¡€ä¸Šæ·»åŠ  context å­—æ®µ
) | prompt | llm

# assign çš„æ•ˆæœ
input_data = {"question": "What is AI?"}
# ç»è¿‡ assign åå˜æˆï¼š
# {"question": "What is AI?", "context": "retrieved docs..."}
```

---

### æ ¸å¿ƒæ¦‚å¿µ5ï¼šRunnableBranch æ¡ä»¶åˆ†æ”¯ ğŸ”€

**RunnableBranch æ ¹æ®æ¡ä»¶é€‰æ‹©ä¸åŒçš„æ‰§è¡Œè·¯å¾„**

```python
from langchain_core.runnables import RunnableBranch

class RunnableBranch(Runnable[Input, Output]):
    """æ¡ä»¶åˆ†æ”¯æ‰§è¡Œ

    ç±»ä¼¼ if-elif-else é€»è¾‘
    """

    branches: List[Tuple[Callable[[Input], bool], Runnable]]
    default: Runnable

    def __init__(
        self,
        *branches: Tuple[Callable[[Input], bool], Runnable],
        default: Runnable
    ):
        self.branches = list(branches)
        self.default = default

    def invoke(self, input: Input, config: Optional[dict] = None) -> Output:
        for condition, runnable in self.branches:
            if condition(input):
                return runnable.invoke(input, config)
        return self.default.invoke(input, config)

# ä½¿ç”¨ç¤ºä¾‹
branch = RunnableBranch(
    # (æ¡ä»¶å‡½æ•°, å¯¹åº”çš„å¤„ç†é“¾)
    (lambda x: "code" in x["question"].lower(), code_chain),
    (lambda x: "translate" in x["question"].lower(), translate_chain),
    (lambda x: len(x["question"]) > 200, long_question_chain),
    default=general_chain
)

# æ‰§è¡Œ
result = branch.invoke({"question": "Write code for sorting"})
# ä¼šé€‰æ‹© code_chain æ‰§è¡Œ
```

**ä½¿ç”¨ RunnableLambda å®ç°åŠ¨æ€è·¯ç”±ï¼š**

```python
from langchain_core.runnables import RunnableLambda

def route(input: dict) -> Runnable:
    """æ ¹æ®è¾“å…¥åŠ¨æ€é€‰æ‹©é“¾"""
    question = input.get("question", "").lower()
    if "code" in question:
        return code_chain
    elif "translate" in question:
        return translate_chain
    else:
        return general_chain

# åŠ¨æ€è·¯ç”±
chain = RunnableLambda(route) | RunnableLambda(lambda chain: chain.invoke)
# æˆ–è€…æ›´ç®€æ´
chain = RunnableLambda(lambda x: route(x).invoke(x))
```

---

### æ ¸å¿ƒæ¦‚å¿µ6ï¼šRunnableLambda å‡½æ•°åŒ…è£… ğŸ”§

**RunnableLambda å°†æ™®é€šå‡½æ•°è½¬æ¢ä¸º Runnable**

```python
from langchain_core.runnables import RunnableLambda

# ä½¿ç”¨æ–¹å¼1ï¼šç›´æ¥åŒ…è£…
def format_output(text: str) -> str:
    return f"[AI] {text.strip()}"

formatter = RunnableLambda(format_output)
chain = prompt | llm | parser | formatter

# ä½¿ç”¨æ–¹å¼2ï¼šè£…é¥°å™¨è¯­æ³•
@RunnableLambda
def add_timestamp(text: str) -> str:
    from datetime import datetime
    return f"[{datetime.now()}] {text}"

chain = prompt | llm | parser | add_timestamp

# ä½¿ç”¨æ–¹å¼3ï¼šlambda è¡¨è¾¾å¼
chain = prompt | llm | RunnableLambda(lambda x: x.content.upper())
```

---

### æ ¸å¿ƒæ¦‚å¿µ7ï¼šbind å’Œ with_config âš™ï¸

**bind é¢„è®¾å‚æ•°ï¼Œwith_config é¢„è®¾è¿è¡Œé…ç½®**

```python
# bindï¼šé¢„è®¾è°ƒç”¨å‚æ•°
llm = ChatOpenAI()
llm_with_temp = llm.bind(temperature=0.9)
llm_with_tools = llm.bind(tools=[tool1, tool2])

# with_configï¼šé¢„è®¾è¿è¡Œé…ç½®
chain = prompt | llm | parser
configured_chain = chain.with_config(
    tags=["production"],
    metadata={"user_id": "123"},
    run_name="my-chain"
)

# é…ç½®ä¼šä¼ é€’ç»™æ‰€æœ‰ç»„ä»¶
result = configured_chain.invoke(input)
```

---

## 4. ã€æœ€å°å¯ç”¨ã€‘

æŒæ¡ä»¥ä¸‹å†…å®¹ï¼Œå°±èƒ½ä½¿ç”¨ LCEL æ„å»º LLM åº”ç”¨ï¼š

### 4.1 åŸºæœ¬ç®¡é“ |

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# åˆ›å»ºç»„ä»¶
prompt = ChatPromptTemplate.from_template("Tell me about {topic}")
llm = ChatOpenAI()
parser = StrOutputParser()

# ç”¨ | ç»„åˆ
chain = prompt | llm | parser

# æ‰§è¡Œ
result = chain.invoke({"topic": "Python"})
```

### 4.2 å¹¶è¡Œæ‰§è¡Œ RunnableParallel

```python
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

# å¹¶è¡Œè·å–å¤šä¸ªç»“æœ
chain = RunnableParallel(
    summary=summary_chain,
    keywords=keyword_chain,
) | combine_chain
```

### 4.3 ä¿ç•™åŸå§‹è¾“å…¥ RunnablePassthrough

```python
# RAG æ ‡å‡†æ¨¡å¼
chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
)
```

### 4.4 è‡ªå®šä¹‰å‡½æ•° RunnableLambda

```python
from langchain_core.runnables import RunnableLambda

chain = prompt | llm | RunnableLambda(lambda x: x.content.upper())
```

**è¿™äº›çŸ¥è¯†è¶³ä»¥ï¼š**
- æ„å»ºåŸºæœ¬çš„ LLM å¤„ç†ç®¡é“
- å®ç° RAG åº”ç”¨
- æ·»åŠ è‡ªå®šä¹‰å¤„ç†é€»è¾‘

---

## 5. ã€1ä¸ªç±»æ¯”ã€‘ï¼ˆåŒè½¨åˆ¶ï¼‰

### ç±»æ¯”1ï¼šLCEL ç®¡é“

#### ğŸ¨ å‰ç«¯è§†è§’ï¼šRxJS Pipe / Redux Middleware

```typescript
// RxJS pipeï¼šæ•°æ®æµç»ä¸€ç³»åˆ—æ“ä½œç¬¦
observable.pipe(
  map(x => x * 2),
  filter(x => x > 10),
  take(5)
);

// Redux middlewareï¼šaction æµç»ä¸­é—´ä»¶
const store = createStore(
  reducer,
  applyMiddleware(logger, thunk, api)
);
```

```python
# LCELï¼šæ•°æ®æµç»ä¸€ç³»åˆ—ç»„ä»¶
chain = prompt | llm | parser
```

#### ğŸ§’ å°æœ‹å‹è§†è§’ï¼šå·¥å‚æµæ°´çº¿

```
LCEL å°±åƒå·¥å‚çš„æµæ°´çº¿ï¼š

åŸææ–™ â†’ [åˆ‡å‰²æœº] â†’ [æ‰“ç£¨æœº] â†’ [å–·æ¼†æœº] â†’ æˆå“
   â†“         â†“          â†“          â†“
 input â†’  prompt  â†’   llm   â†’  parser â†’ output

æ¯ä¸ªæœºå™¨ï¼ˆç»„ä»¶ï¼‰åšä¸€ä»¶äº‹ï¼Œ
äº§å“ï¼ˆæ•°æ®ï¼‰ä¾æ¬¡ç»è¿‡æ¯ä¸ªæœºå™¨ï¼Œ
æœ€åå˜æˆæˆå“ï¼ˆç»“æœï¼‰ï¼
```

---

### ç±»æ¯”2ï¼šRunnableParallel

#### ğŸ¨ å‰ç«¯è§†è§’ï¼šPromise.all

```typescript
const results = await Promise.all([
  fetchUser(id),
  fetchOrders(id),
  fetchReviews(id),
]);
```

#### ğŸ§’ å°æœ‹å‹è§†è§’ï¼šåˆ†ç»„åšä½œä¸š

```
RunnableParallel å°±åƒåˆ†ç»„åšä½œä¸šï¼š

è€å¸ˆå¸ƒç½®äº†ä¸‰é“é¢˜ï¼Œä½ æ‰¾ä¸‰ä¸ªæœ‹å‹åŒæ—¶åšï¼š
- å°æ˜åšç¬¬1é¢˜ â†’ ç­”æ¡ˆ1
- å°çº¢åšç¬¬2é¢˜ â†’ ç­”æ¡ˆ2
- å°åˆšåšç¬¬3é¢˜ â†’ ç­”æ¡ˆ3

æœ€åæŠŠç­”æ¡ˆåˆåœ¨ä¸€èµ·äº¤ç»™è€å¸ˆï¼
æ¯”ä¸€ä¸ªäººåšä¸‰é“é¢˜å¿«å¤šäº†ï¼
```

---

### ç±»æ¯”æ€»ç»“è¡¨

| LCEL æ¦‚å¿µ | å‰ç«¯ç±»æ¯” | å°æœ‹å‹ç±»æ¯” |
|----------|---------|-----------|
| `\|` ç®¡é“ | RxJS pipe | å·¥å‚æµæ°´çº¿ |
| RunnableSequence | middleware chain | æ¥åŠ›èµ› |
| RunnableParallel | Promise.all | åˆ†ç»„åšä½œä¸š |
| RunnablePassthrough | identity å‡½æ•° | å¤å°æœº |
| RunnableBranch | if-else / switch | èµ°è¿·å®«é€‰è·¯ |
| RunnableLambda | é«˜é˜¶å‡½æ•° | ä¸‡èƒ½è½¬æ¢å™¨ |

---

## 6. ã€åç›´è§‰ç‚¹ã€‘

### è¯¯åŒº1ï¼šLCEL åªæ˜¯è¯­æ³•ç³– âŒ

**ä¸ºä»€ä¹ˆé”™ï¼Ÿ**
- LCEL ä¸ä»…æ˜¯è¯­æ³•ç®€åŒ–ï¼Œè¿˜æä¾›äº†å®Œæ•´çš„æ‰§è¡Œæ¡†æ¶
- è‡ªåŠ¨è·å¾— stream/batch/ainvoke èƒ½åŠ›
- é…ç½®å’Œå›è°ƒè‡ªåŠ¨ä¼ é€’

**æ­£ç¡®ç†è§£ï¼š**
```python
# LCEL æä¾›çš„ä¸åªæ˜¯ç®€æ´è¯­æ³•
chain = prompt | llm | parser

# è‡ªåŠ¨è·å¾—è¿™äº›èƒ½åŠ›
chain.stream(input)      # æµå¼
chain.batch(inputs)      # æ‰¹é‡
await chain.ainvoke(input)  # å¼‚æ­¥
```

---

### è¯¯åŒº2ï¼šRunnablePassthrough æ²¡ä»€ä¹ˆç”¨ âŒ

**ä¸ºä»€ä¹ˆé”™ï¼Ÿ**
- åœ¨ RunnableParallel ä¸­ä¿ç•™åŸå§‹è¾“å…¥æ˜¯å…³é”®æ“ä½œ
- assign æ–¹æ³•å¯ä»¥åœ¨åŸè¾“å…¥åŸºç¡€ä¸Šæ·»åŠ å­—æ®µ

**æ­£ç¡®ç†è§£ï¼š**
```python
# æ²¡æœ‰ RunnablePassthroughï¼Œæ— æ³•åŒæ—¶ä¼ é€’ context å’Œ question
chain = {
    "context": retriever,
    "question": RunnablePassthrough()  # å…³é”®ï¼
} | prompt | llm
```

---

### è¯¯åŒº3ï¼š| æ“ä½œç¬¦ä»å·¦åˆ°å³ç«‹å³æ‰§è¡Œ âŒ

**ä¸ºä»€ä¹ˆé”™ï¼Ÿ**
- `|` åªæ˜¯æ„å»ºæ•°æ®ç»“æ„ï¼Œä¸æ‰§è¡Œä»»ä½•é€»è¾‘
- åªæœ‰è°ƒç”¨ invoke/stream æ—¶æ‰çœŸæ­£æ‰§è¡Œ

**æ­£ç¡®ç†è§£ï¼š**
```python
# è¿™ä¸€è¡Œä¸æ‰§è¡Œä»»ä½• LLM è°ƒç”¨
chain = prompt | llm | parser  # åªæ˜¯æ„å»º RunnableSequence

# è¿™ä¸€è¡Œæ‰çœŸæ­£æ‰§è¡Œ
result = chain.invoke(input)  # ç°åœ¨æ‰è°ƒç”¨ LLM
```

---

## 7. ã€å®æˆ˜ä»£ç ã€‘

```python
"""
ç¤ºä¾‹ï¼šLCEL è¡¨è¾¾å¼è¯­è¨€å®æˆ˜
æ¼”ç¤º LCEL çš„æ ¸å¿ƒç”¨æ³•
"""

from typing import Any, Dict, List, Iterator, Optional

# ===== 1. åŸºç¡€ç®¡é“ =====
print("=== 1. åŸºç¡€ç®¡é“ ===")

# æ¨¡æ‹Ÿç»„ä»¶
class FakePrompt:
    def __init__(self, template: str):
        self.template = template

    def invoke(self, input: Dict, config=None) -> str:
        return self.template.format(**input)

    def __or__(self, other):
        return Sequence(self, other)

class FakeLLM:
    def invoke(self, input: str, config=None) -> str:
        return f"LLM: {input[:30]}..."

    def stream(self, input: str, config=None) -> Iterator[str]:
        response = self.invoke(input)
        for char in response:
            yield char

    def __or__(self, other):
        return Sequence(self, other)

class Sequence:
    def __init__(self, first, last, middle=None):
        self.first = first
        self.middle = middle or []
        self.last = last

    def invoke(self, input, config=None):
        result = self.first.invoke(input, config)
        for step in self.middle:
            result = step.invoke(result, config)
        return self.last.invoke(result, config)

    def __or__(self, other):
        return Sequence(self.first, other, self.middle + [self.last])

prompt = FakePrompt("Tell me about {topic}")
llm = FakeLLM()

chain = prompt | llm
result = chain.invoke({"topic": "Python"})
print(f"Result: {result}")
```

---

## 8. ã€é¢è¯•å¿…é—®ã€‘

### é—®é¢˜ï¼š"LCEL æ˜¯ä»€ä¹ˆï¼Ÿæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ"

**æ™®é€šå›ç­”ï¼ˆâŒ ä¸å‡ºå½©ï¼‰ï¼š**
"LCEL æ˜¯ LangChain Expression Languageï¼Œç”¨ | è¿æ¥ç»„ä»¶ã€‚"

**å‡ºå½©å›ç­”ï¼ˆâœ… æ¨èï¼‰ï¼š**

> **LCEL æœ‰ä¸‰ä¸ªæ ¸å¿ƒä¼˜åŠ¿ï¼š**
>
> 1. **å£°æ˜å¼è¯­æ³•**ï¼š`prompt | llm | parser` ä¸€çœ¼çœ‹å‡ºå¤„ç†æµç¨‹
>
> 2. **è‡ªåŠ¨èƒ½åŠ›ç»§æ‰¿**ï¼šå®šä¹‰ä¸€æ¬¡ï¼Œè‡ªåŠ¨è·å¾— stream/batch/ainvoke
>
> 3. **å¯ç»„åˆæ€§**ï¼šç®¡é“å¯ä»¥åµŒå¥—ï¼Œæ”¯æŒä¸²è¡Œã€å¹¶è¡Œã€æ¡ä»¶åˆ†æ”¯
>
> **å®ç°åŸç†**ï¼šé€šè¿‡ `__or__` æ“ä½œç¬¦é‡è½½ï¼Œ`a | b` è¿”å› `RunnableSequence`

---

## 9. ã€åŒ–éª¨ç»µæŒã€‘

### å¡ç‰‡1ï¼šLCEL æ˜¯ä»€ä¹ˆ ğŸ¯

**ä¸€å¥è¯ï¼š** LCEL æ˜¯ç”¨ | æ“ä½œç¬¦ç»„åˆ LangChain ç»„ä»¶çš„å£°æ˜å¼è¯­æ³•ã€‚

**ä¸¾ä¾‹ï¼š**
```python
chain = prompt | llm | parser
```

**åº”ç”¨ï¼š** ä¸€è¡Œä»£ç æ„å»ºå®Œæ•´çš„ LLM å¤„ç†ç®¡é“ã€‚

---

### å¡ç‰‡2ï¼š| æ“ä½œç¬¦åŸç† ğŸ“

**ä¸€å¥è¯ï¼š** | é€šè¿‡ `__or__` æ–¹æ³•åˆ›å»º RunnableSequenceã€‚

**ä¸¾ä¾‹ï¼š**
```python
# a | b ç­‰ä»·äº
a.__or__(b)  # è¿”å› RunnableSequence(a, b)
```

**åº”ç”¨ï¼š** ç†è§£ | çš„æœ¬è´¨æ˜¯æ„å»ºæ•°æ®ç»“æ„ï¼Œä¸æ˜¯ç«‹å³æ‰§è¡Œã€‚

---

### å¡ç‰‡3ï¼šRunnableSequence ğŸ”—

**ä¸€å¥è¯ï¼š** RunnableSequence ä¸²è¡Œæ‰§è¡Œå¤šä¸ªç»„ä»¶ã€‚

**ä¸¾ä¾‹ï¼š**
```python
chain = prompt | llm | parser
# æ‰§è¡Œï¼šinput â†’ prompt â†’ llm â†’ parser â†’ output
```

**åº”ç”¨ï¼š** LCEL ç®¡é“çš„æ ¸å¿ƒæ•°æ®ç»“æ„ã€‚

---

### å¡ç‰‡4ï¼šRunnableParallel ğŸ”€

**ä¸€å¥è¯ï¼š** RunnableParallel å¹¶è¡Œæ‰§è¡Œå¤šä¸ªåˆ†æ”¯ã€‚

**ä¸¾ä¾‹ï¼š**
```python
parallel = RunnableParallel(
    summary=chain1,
    keywords=chain2,
)
```

**åº”ç”¨ï¼š** åŒæ—¶è·å–å¤šä¸ªç»“æœï¼Œå¦‚ RAG ä¸­çš„ context å’Œ questionã€‚

---

### å¡ç‰‡5ï¼šRunnablePassthrough â¡ï¸

**ä¸€å¥è¯ï¼š** RunnablePassthrough åŸæ ·ä¼ é€’è¾“å…¥ã€‚

**ä¸¾ä¾‹ï¼š**
```python
{"context": retriever, "question": RunnablePassthrough()}
```

**åº”ç”¨ï¼š** åœ¨å¹¶è¡Œæ‰§è¡Œæ—¶ä¿ç•™åŸå§‹è¾“å…¥ã€‚

---

### å¡ç‰‡6ï¼šRunnableLambda ğŸ”§

**ä¸€å¥è¯ï¼š** RunnableLambda å°†æ™®é€šå‡½æ•°åŒ…è£…æˆ Runnableã€‚

**ä¸¾ä¾‹ï¼š**
```python
formatter = RunnableLambda(lambda x: x.upper())
chain = prompt | llm | formatter
```

**åº”ç”¨ï¼š** è®©ä»»æ„è‡ªå®šä¹‰å‡½æ•°åŠ å…¥ LCEL ç®¡é“ã€‚

---

### å¡ç‰‡7ï¼šRunnableBranch ğŸ”€

**ä¸€å¥è¯ï¼š** RunnableBranch æ ¹æ®æ¡ä»¶é€‰æ‹©æ‰§è¡Œè·¯å¾„ã€‚

**ä¸¾ä¾‹ï¼š**
```python
branch = RunnableBranch(
    (lambda x: "code" in x, code_chain),
    default=general_chain
)
```

**åº”ç”¨ï¼š** å®ç°åŠ¨æ€è·¯ç”±ï¼Œæ ¹æ®è¾“å…¥é€‰æ‹©ä¸åŒå¤„ç†é€»è¾‘ã€‚

---

### å¡ç‰‡8ï¼šbind ç»‘å®šå‚æ•° âš™ï¸

**ä¸€å¥è¯ï¼š** bind é¢„è®¾ç»„ä»¶çš„è°ƒç”¨å‚æ•°ã€‚

**ä¸¾ä¾‹ï¼š**
```python
llm_creative = llm.bind(temperature=0.9)
llm_with_tools = llm.bind(tools=[tool1])
```

**åº”ç”¨ï¼š** åˆ›å»ºé¢„é…ç½®çš„ç»„ä»¶å˜ä½“ã€‚

---

### å¡ç‰‡9ï¼šstream æµå¼è¾“å‡º ğŸŒŠ

**ä¸€å¥è¯ï¼š** stream æ–¹æ³•å®ç°æµå¼è¾“å‡ºã€‚

**ä¸¾ä¾‹ï¼š**
```python
for chunk in chain.stream(input):
    print(chunk, end="")
```

**åº”ç”¨ï¼š** å®æ—¶æ˜¾ç¤º LLM å“åº”ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

---

### å¡ç‰‡10ï¼šLCEL æœ€ä½³å®è·µ â­

**ä¸€å¥è¯ï¼š** æŒæ¡ LCEL æ¨¡å¼å¯ä»¥å¿«é€Ÿæ„å»ºå¤æ‚ LLM åº”ç”¨ã€‚

**RAG æ ‡å‡†æ¨¡å¼ï¼š**
```python
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | parser
)
```

**åº”ç”¨ï¼š** è¿™æ˜¯ LangChain æœ€å¸¸ç”¨çš„è®¾è®¡æ¨¡å¼ã€‚

---

## 10. ã€ä¸€å¥è¯æ€»ç»“ã€‘

**LCEL æ˜¯ LangChain çš„å£°æ˜å¼ç»„åˆè¯­æ³•ï¼Œé€šè¿‡ | æ“ä½œç¬¦å°† Runnable ç»„ä»¶ä¸²è”æˆå¤„ç†ç®¡é“ï¼Œæä¾›ç®€æ´çš„è¯­æ³•å’Œè‡ªåŠ¨çš„æ‰§è¡Œèƒ½åŠ›ï¼ˆstream/batch/asyncï¼‰ã€‚**

---

## ğŸ“š å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£ LCEL æ˜¯ä»€ä¹ˆä»¥åŠä¸ºä»€ä¹ˆéœ€è¦å®ƒ
- [ ] ä¼šä½¿ç”¨ | æ“ä½œç¬¦ç»„åˆç»„ä»¶
- [ ] ç†è§£ RunnableSequence çš„å·¥ä½œåŸç†
- [ ] ä¼šä½¿ç”¨ RunnableParallel å¹¶è¡Œæ‰§è¡Œ
- [ ] ä¼šä½¿ç”¨ RunnablePassthrough ä¿ç•™è¾“å…¥
- [ ] èƒ½ç”¨ RunnableLambda åŒ…è£…è‡ªå®šä¹‰å‡½æ•°
- [ ] ç†è§£ RunnableBranch æ¡ä»¶åˆ†æ”¯
- [ ] ä¼šä½¿ç”¨ bind é¢„è®¾å‚æ•°
- [ ] èƒ½å¤Ÿæ„å»º RAG é£æ ¼çš„ LCEL ç®¡é“

## ğŸ”— ä¸‹ä¸€æ­¥å­¦ä¹ 

- **BaseChatModel å®ç°**ï¼šç†è§£ LLM ç»„ä»¶å¦‚ä½•å®ç° Runnable
- **Agent æ‰§è¡Œå¼•æ“**ï¼šç†è§£ Agent å¦‚ä½•ä½¿ç”¨ LCEL
- **Callback å›è°ƒç³»ç»Ÿ**ï¼šç†è§£æ‰§è¡Œè¿‡ç¨‹ä¸­çš„äº‹ä»¶å¤„ç†

---

**ç‰ˆæœ¬ï¼š** v1.0
**æœ€åæ›´æ–°ï¼š** 2025-12-12