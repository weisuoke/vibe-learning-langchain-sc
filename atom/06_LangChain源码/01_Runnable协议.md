# Runnable åè®®

> åŸå­åŒ–çŸ¥è¯†ç‚¹ | LangChain æºç  | æ ¸å¿ƒç»„ä»¶åè®®

---

## 1. ã€30å­—æ ¸å¿ƒã€‘

**Runnable æ˜¯ LangChain æ‰€æœ‰å¯æ‰§è¡Œç»„ä»¶çš„ç»Ÿä¸€åè®®ï¼Œå®šä¹‰äº† invoke/stream/batch ç­‰æ ‡å‡†æ¥å£ï¼Œæ˜¯ LCEL çš„åŸºçŸ³ã€‚**

---

## 2. ã€ç¬¬ä¸€æ€§åŸç†ã€‘

### ä»€ä¹ˆæ˜¯ç¬¬ä¸€æ€§åŸç†ï¼Ÿ

**ç¬¬ä¸€æ€§åŸç†**ï¼šå›åˆ°äº‹ç‰©æœ€åŸºæœ¬çš„çœŸç†ï¼Œä»æºå¤´æ€è€ƒé—®é¢˜

### Runnable åè®®çš„ç¬¬ä¸€æ€§åŸç† ğŸ¯

#### 1. æœ€åŸºç¡€çš„å®šä¹‰

**Runnable = è¾“å…¥ â†’ å¤„ç† â†’ è¾“å‡º**

ä»…æ­¤è€Œå·²ï¼æ²¡æœ‰æ›´åŸºç¡€çš„äº†ã€‚

- **è¾“å…¥ (Input)**ï¼šç»„ä»¶æ¥æ”¶çš„æ•°æ®
- **å¤„ç† (Process)**ï¼šç»„ä»¶æ‰§è¡Œçš„é€»è¾‘
- **è¾“å‡º (Output)**ï¼šç»„ä»¶äº§ç”Ÿçš„ç»“æœ

```python
# æœ€ç®€å•çš„ Runnable æœ¬è´¨
def runnable(input: Input) -> Output:
    return process(input)
```

#### 2. ä¸ºä»€ä¹ˆéœ€è¦ Runnable åè®®ï¼Ÿ

**æ ¸å¿ƒé—®é¢˜ï¼šå¦‚ä½•è®© LLM åº”ç”¨çš„å„ç§ç»„ä»¶èƒ½å¤Ÿæ— ç¼åä½œï¼Ÿ**

```python
# æ²¡æœ‰ç»Ÿä¸€åè®®çš„å›°å¢ƒ
prompt_template.format(query="Hello")      # è¿”å› str
llm.generate("Hello")                       # è¿”å› Generation
parser.parse("response")                    # è¿”å› dict
retriever.get_relevant_docs("query")        # è¿”å› List[Document]

# é—®é¢˜ï¼š
# 1. æ¯ä¸ªç»„ä»¶çš„è°ƒç”¨æ–¹å¼ä¸åŒ
# 2. æ— æ³•ç®€å•åœ°ä¸²è”ç»„ä»¶
# 3. éš¾ä»¥å®ç°æ‰¹é‡å¤„ç†ã€æµå¼è¾“å‡ºã€å¼‚æ­¥è°ƒç”¨
```

```python
# æœ‰äº† Runnable åè®®
prompt.invoke({"query": "Hello"})          # ç»Ÿä¸€çš„ invoke æ¥å£
llm.invoke("Hello")                         # ç»Ÿä¸€çš„ invoke æ¥å£
parser.invoke("response")                   # ç»Ÿä¸€çš„ invoke æ¥å£
retriever.invoke("query")                   # ç»Ÿä¸€çš„ invoke æ¥å£

# ä¼˜åŠ¿ï¼š
# 1. æ‰€æœ‰ç»„ä»¶ç”¨ç›¸åŒæ–¹å¼è°ƒç”¨
# 2. å¯ä»¥ç”¨ | æ“ä½œç¬¦ä¸²è”ï¼šprompt | llm | parser
# 3. è‡ªåŠ¨è·å¾— batch/stream/ainvoke èƒ½åŠ›
```

#### 3. Runnable åè®®çš„ä¸‰å±‚ä»·å€¼

##### ä»·å€¼1ï¼šç»Ÿä¸€æ¥å£ - ä¸€è‡´çš„è°ƒç”¨æ–¹å¼

```python
from langchain_core.runnables import Runnable

# æ— è®ºæ˜¯ä»€ä¹ˆç»„ä»¶ï¼Œéƒ½ç”¨ invoke è°ƒç”¨
result1 = prompt.invoke(input_data)
result2 = llm.invoke(input_data)
result3 = chain.invoke(input_data)

# å¤šæ€çš„å¨åŠ›ï¼šä¸å…³å¿ƒå…·ä½“ç±»å‹
def process(runnable: Runnable, data):
    return runnable.invoke(data)
```

##### ä»·å€¼2ï¼šå¯ç»„åˆæ€§ - LCEL ç®¡é“

```python
# Runnable æ”¯æŒ | æ“ä½œç¬¦ç»„åˆ
chain = prompt | llm | parser

# ç­‰ä»·äº
chain = RunnableSequence(
    first=prompt,
    middle=[llm],
    last=parser
)

# æ‰§è¡Œæ—¶è‡ªåŠ¨ä¸²è”
result = chain.invoke({"query": "Hello"})
```

##### ä»·å€¼3ï¼šå¤šæ¨¡å¼æ‰§è¡Œ - åŒæ­¥/å¼‚æ­¥/æ‰¹é‡/æµå¼

```python
# ä¸€ä¸ª Runnable è‡ªåŠ¨è·å¾—å››ç§æ‰§è¡Œæ¨¡å¼
runnable.invoke(input)              # åŒæ­¥å•æ¬¡
runnable.batch([input1, input2])    # æ‰¹é‡å¤„ç†
runnable.stream(input)              # æµå¼è¾“å‡º
await runnable.ainvoke(input)       # å¼‚æ­¥å•æ¬¡
await runnable.astream(input)       # å¼‚æ­¥æµå¼
```

#### 4. ä»ç¬¬ä¸€æ€§åŸç†æ¨å¯¼ LangChain æºç æ¶æ„

**æ¨ç†é“¾ï¼š**

```
1. LLM åº”ç”¨éœ€è¦ç»„åˆå¤šç§ç»„ä»¶ï¼ˆæç¤ºã€æ¨¡å‹ã€è§£æå™¨ã€æ£€ç´¢å™¨...ï¼‰
   â†“
2. æ¯ç§ç»„ä»¶éƒ½æ˜¯"è¾“å…¥â†’å¤„ç†â†’è¾“å‡º"çš„å‡½æ•°
   â†“
3. éœ€è¦ä¸€ä¸ªç»Ÿä¸€çš„"å¯æ‰§è¡Œ"æŠ½è±¡
   â†“
4. å®šä¹‰ Runnable åè®®ï¼šinvoke(input) -> output
   â†“
5. æ‰€æœ‰ç»„ä»¶å®ç° Runnable åè®®
   â†“
6. åˆ©ç”¨ Python çš„ __or__ å®ç° | æ“ä½œç¬¦
   â†“
7. LCEL è¯ç”Ÿï¼šprompt | llm | parser
   â†“
8. åœ¨ Runnable åŸºç±»ä¸­å®ç° batch/stream/ainvoke
   â†“
9. æ‰€æœ‰ç»„ä»¶è‡ªåŠ¨è·å¾—å¤šæ¨¡å¼æ‰§è¡Œèƒ½åŠ›
```

#### 5. ä¸€å¥è¯æ€»ç»“ç¬¬ä¸€æ€§åŸç†

**Runnable æ˜¯"è¾“å…¥â†’è¾“å‡º"çš„ç»Ÿä¸€æŠ½è±¡ï¼Œé€šè¿‡åè®®æ ‡å‡†åŒ–è®©æ‰€æœ‰ç»„ä»¶å¯ç»„åˆã€å¯æ›¿æ¢ã€è‡ªåŠ¨è·å¾—å¤šæ¨¡å¼æ‰§è¡Œèƒ½åŠ›ã€‚**

---

## 3. ã€æ ¸å¿ƒæ¦‚å¿µï¼ˆå…¨é¢è¦†ç›–ï¼‰ã€‘

### æ ¸å¿ƒæ¦‚å¿µ1ï¼šRunnable æŠ½è±¡åŸºç±» ğŸ—ï¸

**Runnable æ˜¯æ‰€æœ‰å¯æ‰§è¡Œç»„ä»¶çš„æŠ½è±¡åŸºç±»ï¼Œå®šä¹‰äº†æ ‡å‡†æ‰§è¡Œæ¥å£**

```python
from abc import ABC, abstractmethod
from typing import TypeVar, Generic, Optional, List, Iterator, Any

Input = TypeVar("Input")
Output = TypeVar("Output")

class Runnable(ABC, Generic[Input, Output]):
    """LangChain æ ¸å¿ƒæŠ½è±¡ï¼šå¯æ‰§è¡Œç»„ä»¶åè®®

    æ‰€æœ‰ LangChain ç»„ä»¶éƒ½å®ç°è¿™ä¸ªåè®®ï¼š
    - PromptTemplate
    - ChatModel / LLM
    - OutputParser
    - Retriever
    - Chain
    - Agent
    """

    # ===== æ ¸å¿ƒæ‰§è¡Œæ–¹æ³• =====

    @abstractmethod
    def invoke(self, input: Input, config: Optional[dict] = None) -> Output:
        """åŒæ­¥æ‰§è¡Œï¼ˆæ ¸å¿ƒæ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°ï¼‰"""
        pass

    def batch(self, inputs: List[Input], config: Optional[dict] = None) -> List[Output]:
        """æ‰¹é‡æ‰§è¡Œï¼ˆé»˜è®¤å®ç°ï¼šå¾ªç¯è°ƒç”¨ invokeï¼‰"""
        return [self.invoke(x, config) for x in inputs]

    def stream(self, input: Input, config: Optional[dict] = None) -> Iterator[Output]:
        """æµå¼æ‰§è¡Œï¼ˆé»˜è®¤å®ç°ï¼šyield å®Œæ•´ç»“æœï¼‰"""
        yield self.invoke(input, config)

    # ===== å¼‚æ­¥ç‰ˆæœ¬ =====

    async def ainvoke(self, input: Input, config: Optional[dict] = None) -> Output:
        """å¼‚æ­¥æ‰§è¡Œï¼ˆé»˜è®¤å®ç°ï¼šè°ƒç”¨åŒæ­¥ç‰ˆæœ¬ï¼‰"""
        return self.invoke(input, config)

    async def abatch(self, inputs: List[Input], config: Optional[dict] = None) -> List[Output]:
        """å¼‚æ­¥æ‰¹é‡æ‰§è¡Œ"""
        import asyncio
        return await asyncio.gather(*[self.ainvoke(x, config) for x in inputs])

    # ===== ç»„åˆæ“ä½œç¬¦ =====

    def __or__(self, other: "Runnable") -> "RunnableSequence":
        """é‡è½½ | æ“ä½œç¬¦ï¼Œå®ç° LCEL ç®¡é“"""
        return RunnableSequence(first=self, last=other)

    def __ror__(self, other: Any) -> "RunnableSequence":
        """å¤„ç†å·¦æ“ä½œæ•°ä¸æ˜¯ Runnable çš„æƒ…å†µ"""
        return RunnableSequence(first=coerce_to_runnable(other), last=self)
```

**æ ¸å¿ƒæ–¹æ³•å¯¹æ¯”ï¼š**

| æ–¹æ³• | è¯´æ˜ | è¾“å…¥ | è¾“å‡º |
|------|------|------|------|
| `invoke` | åŒæ­¥å•æ¬¡æ‰§è¡Œ | `Input` | `Output` |
| `batch` | æ‰¹é‡æ‰§è¡Œ | `List[Input]` | `List[Output]` |
| `stream` | æµå¼è¾“å‡º | `Input` | `Iterator[Output]` |
| `ainvoke` | å¼‚æ­¥æ‰§è¡Œ | `Input` | `Output` |
| `astream` | å¼‚æ­¥æµå¼ | `Input` | `AsyncIterator[Output]` |

**åœ¨ LangChain æºç ä¸­çš„ä½ç½®ï¼š**

```python
# langchain_core/runnables/base.py
class Runnable(Generic[Input, Output], ABC):
    """æ‰€æœ‰ LangChain ç»„ä»¶çš„åŸºç±»"""

    @property
    def InputType(self) -> Type[Input]:
        """è¾“å…¥ç±»å‹"""
        # é€šè¿‡æ³›å‹å‚æ•°æ¨æ–­
        ...

    @property
    def OutputType(self) -> Type[Output]:
        """è¾“å‡ºç±»å‹"""
        ...

    @property
    def input_schema(self) -> Type[BaseModel]:
        """è¾“å…¥çš„ Pydantic schema"""
        ...

    @property
    def output_schema(self) -> Type[BaseModel]:
        """è¾“å‡ºçš„ Pydantic schema"""
        ...
```

---

### æ ¸å¿ƒæ¦‚å¿µ2ï¼šRunnableSequence åºåˆ—ç»„åˆ ğŸ“

**RunnableSequence æ˜¯ LCEL ç®¡é“çš„æ ¸å¿ƒå®ç°ï¼Œä¸²è”å¤šä¸ª Runnable**

```python
from typing import List, Any

class RunnableSequence(Runnable[Input, Output]):
    """Runnable åºåˆ—ï¼šA | B | C çš„å®ç°

    æ‰§è¡Œæµç¨‹ï¼š
    input â†’ A.invoke() â†’ B.invoke() â†’ C.invoke() â†’ output
    """

    first: Runnable       # ç¬¬ä¸€ä¸ªç»„ä»¶
    middle: List[Runnable] # ä¸­é—´ç»„ä»¶ï¼ˆå¯ä»¥ä¸ºç©ºï¼‰
    last: Runnable        # æœ€åä¸€ä¸ªç»„ä»¶

    def __init__(self, first: Runnable, last: Runnable, middle: List[Runnable] = None):
        self.first = first
        self.middle = middle or []
        self.last = last

    def invoke(self, input: Input, config: Optional[dict] = None) -> Output:
        """ä¸²è”æ‰§è¡Œæ‰€æœ‰ç»„ä»¶"""
        # 1. æ‰§è¡Œç¬¬ä¸€ä¸ª
        result = self.first.invoke(input, config)

        # 2. æ‰§è¡Œä¸­é—´çš„
        for runnable in self.middle:
            result = runnable.invoke(result, config)

        # 3. æ‰§è¡Œæœ€åä¸€ä¸ª
        return self.last.invoke(result, config)

    def stream(self, input: Input, config: Optional[dict] = None) -> Iterator[Output]:
        """æµå¼æ‰§è¡Œï¼šåªæœ‰æœ€åä¸€ä¸ªç»„ä»¶æµå¼è¾“å‡º"""
        # 1. å‰é¢çš„ç»„ä»¶æ­£å¸¸æ‰§è¡Œ
        result = self.first.invoke(input, config)
        for runnable in self.middle:
            result = runnable.invoke(result, config)

        # 2. æœ€åä¸€ä¸ªç»„ä»¶æµå¼è¾“å‡º
        for chunk in self.last.stream(result, config):
            yield chunk

    @property
    def input_schema(self):
        """è¾“å…¥ schema ç”±ç¬¬ä¸€ä¸ªç»„ä»¶å†³å®š"""
        return self.first.input_schema

    @property
    def output_schema(self):
        """è¾“å‡º schema ç”±æœ€åä¸€ä¸ªç»„ä»¶å†³å®š"""
        return self.last.output_schema

    def __or__(self, other: Runnable) -> "RunnableSequence":
        """æ”¯æŒç»§ç»­é“¾æ¥ï¼š(A | B) | C"""
        return RunnableSequence(
            first=self.first,
            middle=self.middle + [self.last],
            last=other
        )

# ä½¿ç”¨ç¤ºä¾‹
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_template("Tell me about {topic}")
llm = ChatOpenAI()
parser = StrOutputParser()

# åˆ›å»ºåºåˆ—ï¼ˆä¸‰ç§ç­‰ä»·æ–¹å¼ï¼‰
chain1 = prompt | llm | parser                         # LCEL è¯­æ³•
chain2 = RunnableSequence(first=prompt, last=parser, middle=[llm])  # æ˜¾å¼åˆ›å»º
chain3 = prompt.__or__(llm).__or__(parser)            # æ‰‹åŠ¨è°ƒç”¨

# æ‰§è¡Œ
result = chain1.invoke({"topic": "Python"})
```

**RunnableSequence çš„æµå¼è¡Œä¸ºï¼š**

```python
# æµå¼è¾“å‡ºæ—¶ï¼Œåªæœ‰æœ€åä¸€ä¸ªæ”¯æŒæµå¼çš„ç»„ä»¶ä¼šäº§ç”Ÿå¤šä¸ª chunk
chain = prompt | llm | parser

for chunk in chain.stream({"topic": "AI"}):
    print(chunk, end="", flush=True)
    # é€å­—è¾“å‡º LLM çš„å“åº”
```

---

### æ ¸å¿ƒæ¦‚å¿µ3ï¼šRunnableParallel å¹¶è¡Œç»„åˆ ğŸ”€

**RunnableParallel å¹¶è¡Œæ‰§è¡Œå¤šä¸ª Runnableï¼Œç»“æœåˆå¹¶ä¸ºå­—å…¸**

```python
from typing import Dict, Any

class RunnableParallel(Runnable[Input, Dict[str, Any]]):
    """å¹¶è¡Œæ‰§è¡Œå¤šä¸ª Runnable

    è¾“å…¥ï¼šåŒä¸€ä¸ª input ä¼ ç»™æ‰€æœ‰åˆ†æ”¯
    è¾“å‡ºï¼š{key1: result1, key2: result2, ...}
    """

    steps: Dict[str, Runnable]

    def __init__(self, steps: Dict[str, Runnable] = None, **kwargs):
        self.steps = steps or kwargs

    def invoke(self, input: Input, config: Optional[dict] = None) -> Dict[str, Any]:
        """å¹¶è¡Œæ‰§è¡Œï¼ˆåŒæ­¥ç‰ˆæœ¬å®é™…ä¸Šæ˜¯é¡ºåºæ‰§è¡Œï¼‰"""
        return {
            key: runnable.invoke(input, config)
            for key, runnable in self.steps.items()
        }

    async def ainvoke(self, input: Input, config: Optional[dict] = None) -> Dict[str, Any]:
        """çœŸæ­£çš„å¹¶è¡Œæ‰§è¡Œï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        import asyncio

        async def run_one(key: str, runnable: Runnable):
            result = await runnable.ainvoke(input, config)
            return key, result

        results = await asyncio.gather(*[
            run_one(key, runnable)
            for key, runnable in self.steps.items()
        ])
        return dict(results)

# ä½¿ç”¨ç¤ºä¾‹
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

# å¹¶è¡Œæ‰§è¡Œå¤šä¸ªä»»åŠ¡
parallel = RunnableParallel(
    summary=summary_chain,
    translation=translation_chain,
    keywords=keyword_chain,
)

# åŒä¸€ä¸ªè¾“å…¥å¹¶è¡Œå¤„ç†
result = parallel.invoke({"text": "Hello World"})
# result = {
#     "summary": "...",
#     "translation": "...",
#     "keywords": ["..."]
# }

# å¸¸è§ç”¨æ³•ï¼šæ„é€  context + question
chain = (
    RunnableParallel(
        context=retriever,           # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        question=RunnablePassthrough() # åŸæ ·ä¼ é€’é—®é¢˜
    )
    | prompt  # ä½¿ç”¨ context å’Œ question
    | llm
)
```

---

### æ ¸å¿ƒæ¦‚å¿µ4ï¼šRunnableLambda å‡½æ•°åŒ…è£… ğŸ”§

**RunnableLambda å°†æ™®é€šå‡½æ•°åŒ…è£…ä¸º Runnable**

```python
from typing import Callable

class RunnableLambda(Runnable[Input, Output]):
    """å°†æ™®é€šå‡½æ•°åŒ…è£…ä¸º Runnable

    è®©ä»»æ„å‡½æ•°éƒ½èƒ½å‚ä¸ LCEL ç®¡é“
    """

    func: Callable[[Input], Output]
    afunc: Optional[Callable[[Input], Awaitable[Output]]] = None

    def __init__(
        self,
        func: Callable[[Input], Output],
        afunc: Optional[Callable] = None
    ):
        self.func = func
        self.afunc = afunc

    def invoke(self, input: Input, config: Optional[dict] = None) -> Output:
        """è°ƒç”¨åŒ…è£…çš„å‡½æ•°"""
        return self.func(input)

    async def ainvoke(self, input: Input, config: Optional[dict] = None) -> Output:
        """å¼‚æ­¥è°ƒç”¨"""
        if self.afunc:
            return await self.afunc(input)
        return self.func(input)

# ä½¿ç”¨ç¤ºä¾‹
from langchain_core.runnables import RunnableLambda

# åŒ…è£…æ™®é€šå‡½æ•°
def format_output(text: str) -> str:
    return text.upper()

formatter = RunnableLambda(format_output)

# å‚ä¸ LCEL ç®¡é“
chain = prompt | llm | parser | formatter

# ä½¿ç”¨è£…é¥°å™¨è¯­æ³•
@RunnableLambda
def add_prefix(text: str) -> str:
    return f"[AI] {text}"

chain = prompt | llm | parser | add_prefix
```

---

### æ ¸å¿ƒæ¦‚å¿µ5ï¼šRunnablePassthrough æ•°æ®é€ä¼  â¡ï¸

**RunnablePassthrough åŸæ ·ä¼ é€’è¾“å…¥ï¼Œå¸¸ç”¨äºä¿ç•™åŸå§‹æ•°æ®**

```python
class RunnablePassthrough(Runnable[Input, Input]):
    """åŸæ ·ä¼ é€’è¾“å…¥

    çœ‹èµ·æ¥ä»€ä¹ˆéƒ½ä¸åšï¼Œä½†åœ¨æ„é€ å¤æ‚ç®¡é“æ—¶éå¸¸æœ‰ç”¨
    """

    def invoke(self, input: Input, config: Optional[dict] = None) -> Input:
        return input

    @classmethod
    def assign(cls, **kwargs) -> "RunnableAssign":
        """æ·»åŠ æ–°å­—æ®µåˆ°è¾“å…¥"""
        return RunnableAssign(mapper=RunnableParallel(kwargs))

# ä½¿ç”¨ç¤ºä¾‹
from langchain_core.runnables import RunnablePassthrough, RunnableParallel

# åœºæ™¯1ï¼šä¿ç•™åŸå§‹è¾“å…¥
chain = RunnableParallel(
    original=RunnablePassthrough(),  # ä¿ç•™åŸå§‹è¾“å…¥
    processed=some_processor         # å¤„ç†åçš„ç»“æœ
)

# åœºæ™¯2ï¼šRAG å¸¸è§æ¨¡å¼
rag_chain = (
    RunnableParallel(
        context=retriever | format_docs,  # æ£€ç´¢å¹¶æ ¼å¼åŒ–æ–‡æ¡£
        question=RunnablePassthrough()    # åŸæ ·ä¼ é€’é—®é¢˜
    )
    | prompt
    | llm
    | parser
)

# åœºæ™¯3ï¼šassign æ·»åŠ å­—æ®µ
chain = RunnablePassthrough.assign(
    context=retriever,  # æ·»åŠ  context å­—æ®µ
    # question å­—æ®µè‡ªåŠ¨ä¿ç•™
) | prompt | llm
```

---

### æ ¸å¿ƒæ¦‚å¿µ6ï¼šRunnableBranch æ¡ä»¶åˆ†æ”¯ ğŸ”€

**RunnableBranch æ ¹æ®æ¡ä»¶é€‰æ‹©ä¸åŒçš„æ‰§è¡Œè·¯å¾„**

```python
from typing import Tuple, Callable

class RunnableBranch(Runnable[Input, Output]):
    """æ¡ä»¶åˆ†æ”¯ï¼šæ ¹æ®æ¡ä»¶é€‰æ‹©ä¸åŒçš„ Runnable

    ç±»ä¼¼ if-elif-else é€»è¾‘
    """

    branches: List[Tuple[Callable[[Input], bool], Runnable]]
    default: Runnable

    def __init__(
        self,
        *branches: Tuple[Callable[[Input], bool], Runnable],
        default: Runnable
    ):
        self.branches = list(branches)
        self.default = default

    def invoke(self, input: Input, config: Optional[dict] = None) -> Output:
        """æ ¹æ®æ¡ä»¶é€‰æ‹©åˆ†æ”¯æ‰§è¡Œ"""
        for condition, runnable in self.branches:
            if condition(input):
                return runnable.invoke(input, config)
        return self.default.invoke(input, config)

# ä½¿ç”¨ç¤ºä¾‹
from langchain_core.runnables import RunnableBranch

# æ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹©ä¸åŒçš„å¤„ç†é“¾
branch = RunnableBranch(
    # (æ¡ä»¶å‡½æ•°, å¯¹åº”çš„ Runnable)
    (lambda x: "ä»£ç " in x["question"], code_chain),
    (lambda x: "ç¿»è¯‘" in x["question"], translation_chain),
    (lambda x: len(x["question"]) > 100, long_question_chain),
    default=general_chain  # é»˜è®¤åˆ†æ”¯
)

# æ‰§è¡Œ
result = branch.invoke({"question": "å¸®æˆ‘å†™ä¸€æ®µä»£ç "})
# ä¼šé€‰æ‹© code_chain æ‰§è¡Œ
```

---

### æ‰©å±•æ¦‚å¿µ7ï¼šRunnableConfig è¿è¡Œé…ç½® âš™ï¸

```python
from typing import TypedDict, Optional, List, Dict, Any

class RunnableConfig(TypedDict, total=False):
    """Runnable æ‰§è¡Œæ—¶çš„é…ç½®"""

    # å›è°ƒå¤„ç†å™¨
    callbacks: Optional[List[BaseCallbackHandler]]

    # æ ‡ç­¾ï¼ˆç”¨äºè¿½è¸ªï¼‰
    tags: Optional[List[str]]

    # å…ƒæ•°æ®
    metadata: Optional[Dict[str, Any]]

    # è¿è¡Œåç§°
    run_name: Optional[str]

    # æœ€å¤§å¹¶å‘æ•°
    max_concurrency: Optional[int]

    # é€’å½’æ·±åº¦é™åˆ¶
    recursion_limit: Optional[int]

    # å¯é…ç½®å­—æ®µ
    configurable: Optional[Dict[str, Any]]

# ä½¿ç”¨ç¤ºä¾‹
config = {
    "callbacks": [MyCallbackHandler()],
    "tags": ["production", "user-123"],
    "metadata": {"user_id": "123"},
    "run_name": "my-chain-run",
    "max_concurrency": 5,
}

result = chain.invoke(input, config=config)

# ä½¿ç”¨ with_config é¢„è®¾é…ç½®
configured_chain = chain.with_config(
    tags=["production"],
    run_name="production-chain"
)
result = configured_chain.invoke(input)
```

---

## 4. ã€æœ€å°å¯ç”¨ã€‘

æŒæ¡ä»¥ä¸‹å†…å®¹ï¼Œå°±èƒ½å¼€å§‹ä½¿ç”¨å’Œç†è§£ Runnable åè®®ï¼š

### 4.1 invoke åŸºæœ¬è°ƒç”¨

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

# æ‰€æœ‰ LangChain ç»„ä»¶éƒ½ç”¨ invoke è°ƒç”¨
prompt = ChatPromptTemplate.from_template("Tell me about {topic}")
llm = ChatOpenAI()

# è°ƒç”¨ prompt
formatted = prompt.invoke({"topic": "Python"})

# è°ƒç”¨ llm
response = llm.invoke(formatted)
```

### 4.2 ä½¿ç”¨ | æ“ä½œç¬¦ç»„åˆ

```python
from langchain_core.output_parsers import StrOutputParser

# ç”¨ | ä¸²è”ç»„ä»¶
chain = prompt | llm | StrOutputParser()

# ä¸€æ¬¡è°ƒç”¨å®Œæˆæ•´ä¸ªæµç¨‹
result = chain.invoke({"topic": "Python"})
print(result)  # ç›´æ¥å¾—åˆ°å­—ç¬¦ä¸²ç»“æœ
```

### 4.3 stream æµå¼è¾“å‡º

```python
# æµå¼è¾“å‡º LLM å“åº”
for chunk in chain.stream({"topic": "AI"}):
    print(chunk, end="", flush=True)
```

### 4.4 batch æ‰¹é‡å¤„ç†

```python
# æ‰¹é‡å¤„ç†å¤šä¸ªè¾“å…¥
inputs = [
    {"topic": "Python"},
    {"topic": "JavaScript"},
    {"topic": "Rust"},
]

results = chain.batch(inputs)
# results = ["...", "...", "..."]
```

### 4.5 RunnableLambda åŒ…è£…å‡½æ•°

```python
from langchain_core.runnables import RunnableLambda

# å°†è‡ªå®šä¹‰å‡½æ•°åŠ å…¥ç®¡é“
def postprocess(text: str) -> str:
    return text.strip().upper()

chain = prompt | llm | StrOutputParser() | RunnableLambda(postprocess)
```

**è¿™äº›çŸ¥è¯†è¶³ä»¥ï¼š**
- ç†è§£ LangChain ç»„ä»¶çš„ç»Ÿä¸€è°ƒç”¨æ–¹å¼
- ä½¿ç”¨ LCEL æ„å»ºå¤„ç†ç®¡é“
- å®ç°æµå¼è¾“å‡ºå’Œæ‰¹é‡å¤„ç†
- å°†è‡ªå®šä¹‰é€»è¾‘é›†æˆåˆ°ç®¡é“ä¸­

---

## 5. ã€1ä¸ªç±»æ¯”ã€‘ï¼ˆåŒè½¨åˆ¶ï¼‰

### ç±»æ¯”1ï¼šRunnable åè®®

#### ğŸ¨ å‰ç«¯è§†è§’ï¼šReact Component åè®®

Runnable å°±åƒ React ä¸­æ‰€æœ‰ç»„ä»¶éƒ½å¿…é¡»å®ç°çš„ `render()` æ–¹æ³•ã€‚

```typescript
// React: æ‰€æœ‰ç»„ä»¶éƒ½å¿…é¡»å®ç° render
interface Component<Props, State> {
  render(): ReactNode;          // ç±»ä¼¼ invoke
  componentDidMount?(): void;   // ç”Ÿå‘½å‘¨æœŸ
  componentDidUpdate?(): void;
}

// å‡½æ•°ç»„ä»¶ä¹Ÿæ˜¯ä¸€ç§ "Runnable"
function MyComponent(props: Props): ReactNode {
  return <div>{props.text}</div>;
}
```

```python
# LangChain: æ‰€æœ‰ç»„ä»¶éƒ½å¿…é¡»å®ç° invoke
class Runnable(ABC, Generic[Input, Output]):
    @abstractmethod
    def invoke(self, input: Input) -> Output:  # æ ¸å¿ƒæ–¹æ³•
        pass

    def stream(self, input: Input): ...  # é¢å¤–èƒ½åŠ›
    def batch(self, inputs: List[Input]): ...
```

**ç›¸ä¼¼ç‚¹ï¼š**
- éƒ½æ˜¯å®šä¹‰ç»„ä»¶çš„æ ‡å‡†æ¥å£
- éƒ½æ”¯æŒç»„åˆï¼ˆReact ç»„ä»¶åµŒå¥— â‰ˆ LCEL ç®¡é“ï¼‰
- éƒ½æœ‰è¾“å…¥è¾“å‡ºï¼ˆProps â†’ ReactNode â‰ˆ Input â†’ Outputï¼‰

#### ğŸ§’ å°æœ‹å‹è§†è§’ï¼šUSB æ¥å£

Runnable å°±åƒ **USB æ¥å£**ï¼š

```
æ‰€æœ‰ USB è®¾å¤‡éƒ½éµå®ˆåŒä¸€ä¸ªæ ‡å‡†ï¼š
- é¼ æ ‡ ğŸ–±ï¸ â”€â”€ USB æ¥å£ â”€â”€ ç”µè„‘
- é”®ç›˜ âŒ¨ï¸ â”€â”€ USB æ¥å£ â”€â”€ ç”µè„‘
- Uç›˜ ğŸ’¾ â”€â”€ USB æ¥å£ â”€â”€ ç”µè„‘
- æ‰‹æŸ„ ğŸ® â”€â”€ USB æ¥å£ â”€â”€ ç”µè„‘

ä¸ç®¡æ˜¯ä»€ä¹ˆè®¾å¤‡ï¼Œåªè¦æœ‰ USB æ¥å£ï¼Œå°±èƒ½æ’åˆ°ç”µè„‘ä¸Šç”¨ï¼
è¿™å°±æ˜¯ "åè®®" çš„å¨åŠ›ï¼

LangChain çš„ Runnable ä¹Ÿæ˜¯è¿™æ ·ï¼š
- æç¤ºæ¨¡æ¿ ğŸ“ â”€â”€ invoke â”€â”€ ç»“æœ
- è¯­è¨€æ¨¡å‹ ğŸ¤– â”€â”€ invoke â”€â”€ ç»“æœ
- è§£æå™¨ ğŸ” â”€â”€ invoke â”€â”€ ç»“æœ

ä¸ç®¡æ˜¯ä»€ä¹ˆç»„ä»¶ï¼Œåªè¦å®ç°äº† invokeï¼Œå°±èƒ½ç»„åˆåœ¨ä¸€èµ·ï¼
```

---

### ç±»æ¯”2ï¼šRunnableSequence (| ç®¡é“)

#### ğŸ¨ å‰ç«¯è§†è§’ï¼šRedux Middleware / RxJS Pipe

```typescript
// Redux middlewareï¼šè¯·æ±‚ç»è¿‡ä¸€ç³»åˆ—ä¸­é—´ä»¶
const middleware = applyMiddleware(
  logger,     // ç¬¬ä¸€ä¸ª
  thunk,      // ç¬¬äºŒä¸ª
  api         // ç¬¬ä¸‰ä¸ª
);
// è¯·æ±‚æµï¼šaction â†’ logger â†’ thunk â†’ api â†’ store

// RxJS pipeï¼šæ•°æ®ç»è¿‡ä¸€ç³»åˆ—æ“ä½œç¬¦
observable.pipe(
  map(x => x * 2),
  filter(x => x > 10),
  take(5)
);
// æ•°æ®æµï¼šsource â†’ map â†’ filter â†’ take â†’ subscribe
```

```python
# LangChain LCELï¼šæ•°æ®ç»è¿‡ä¸€ç³»åˆ—ç»„ä»¶
chain = prompt | llm | parser
# æ•°æ®æµï¼šinput â†’ prompt â†’ llm â†’ parser â†’ output
```

**ç›¸ä¼¼ç‚¹ï¼š**
- éƒ½æ˜¯æ•°æ®ç®¡é“æ¨¡å¼
- éƒ½æ˜¯ä»å·¦åˆ°å³ä¾æ¬¡å¤„ç†
- ä¸Šä¸€æ­¥çš„è¾“å‡ºæ˜¯ä¸‹ä¸€æ­¥çš„è¾“å…¥

#### ğŸ§’ å°æœ‹å‹è§†è§’ï¼šæ¥åŠ›èµ›è·‘

```
LCEL ç®¡é“å°±åƒæ¥åŠ›èµ›è·‘ï¼š

ç¬¬ä¸€æ£’ï¼ˆpromptï¼‰ğŸƒ
  â†“ ä¼ é€’æ¥åŠ›æ£’
ç¬¬äºŒæ£’ï¼ˆllmï¼‰ğŸƒ
  â†“ ä¼ é€’æ¥åŠ›æ£’
ç¬¬ä¸‰æ£’ï¼ˆparserï¼‰ğŸƒ
  â†“
ç»ˆç‚¹ï¼ğŸ

æ¯ä¸ªé€‰æ‰‹ï¼ˆç»„ä»¶ï¼‰ï¼š
1. æ¥è¿‡æ¥åŠ›æ£’ï¼ˆæ¥æ”¶ä¸Šä¸€æ­¥çš„è¾“å‡ºï¼‰
2. è·‘è‡ªå·±çš„é‚£æ®µè·¯ï¼ˆæ‰§è¡Œè‡ªå·±çš„é€»è¾‘ï¼‰
3. æŠŠæ¥åŠ›æ£’ä¼ ç»™ä¸‹ä¸€ä¸ªäººï¼ˆè¾“å‡ºç»™ä¸‹ä¸€æ­¥ï¼‰

prompt | llm | parser
å°±åƒï¼šå°æ˜ â†’ å°çº¢ â†’ å°åˆš â†’ ç»ˆç‚¹
```

---

### ç±»æ¯”3ï¼šRunnableParallel (å¹¶è¡Œ)

#### ğŸ¨ å‰ç«¯è§†è§’ï¼šPromise.all

```typescript
// Promise.allï¼šå¹¶è¡Œæ‰§è¡Œå¤šä¸ªå¼‚æ­¥æ“ä½œ
const results = await Promise.all([
  fetchUser(id),
  fetchOrders(id),
  fetchReviews(id),
]);
// results = [user, orders, reviews]
```

```python
# RunnableParallelï¼šå¹¶è¡Œæ‰§è¡Œå¤šä¸ªç»„ä»¶
parallel = RunnableParallel(
    user=fetch_user_chain,
    orders=fetch_orders_chain,
    reviews=fetch_reviews_chain,
)
result = parallel.invoke({"id": "123"})
# result = {"user": ..., "orders": ..., "reviews": ...}
```

#### ğŸ§’ å°æœ‹å‹è§†è§’ï¼šåŒæ—¶åšä½œä¸š

```
RunnableParallel å°±åƒåŒæ—¶åšä¸åŒç§‘ç›®çš„ä½œä¸šï¼š

ä½ æœ‰ä¸‰ä¸ªå¥½æœ‹å‹å¸®ä½ åŒæ—¶åšä½œä¸šï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        åŒä¸€é“é¢˜          â”‚
â”‚     "1+1ç­‰äºå‡ ï¼Ÿ"        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“â†“â†“
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â†“         â†“         â†“
  å°æ˜      å°çº¢       å°åˆš
 (æ•°å­¦)    (è¯­æ–‡)     (è‹±è¯­)
    â†“         â†“         â†“
  ç­”æ¡ˆ1     ç­”æ¡ˆ2      ç­”æ¡ˆ3
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â†“
   {æ•°å­¦: 2, è¯­æ–‡: "äºŒ", è‹±è¯­: "two"}

ä¸‰ä¸ªäººåŒæ—¶åšï¼Œæ¯”ä¸€ä¸ªäººåšä¸‰éå¿«å¤šäº†ï¼
```

---

### ç±»æ¯”4ï¼šRunnableLambda

#### ğŸ¨ å‰ç«¯è§†è§’ï¼šé«˜é˜¶å‡½æ•° / Array.map

```typescript
// ä»»ä½•å‡½æ•°éƒ½èƒ½å‚ä¸ map ç®¡é“
const numbers = [1, 2, 3];
const doubled = numbers.map(x => x * 2);  // æ™®é€šå‡½æ•°åŒ…è£…è¿› map

// ä»»ä½•å‡½æ•°éƒ½èƒ½å˜æˆä¸­é—´ä»¶
const myMiddleware = (next) => (action) => {
  console.log(action);
  return next(action);
};
```

```python
# ä»»ä½•å‡½æ•°éƒ½èƒ½å˜æˆ Runnable
def my_function(x):
    return x * 2

runnable = RunnableLambda(my_function)
chain = other_runnable | runnable | another_runnable
```

#### ğŸ§’ å°æœ‹å‹è§†è§’ï¼šæŠŠæ™®é€šå·¥å…·å˜æˆä¹é«˜é›¶ä»¶

```
RunnableLambda å°±åƒç»™æ™®é€šå·¥å…·è£…ä¸Šä¹é«˜æ¥å£ï¼š

ä½ æœ‰ä¸€æŠŠæ™®é€šçš„å°é”¤å­ï¼ˆæ™®é€šå‡½æ•°ï¼‰ï¼š
ğŸ”¨ é”¤å­

ä½†ä¹é«˜ç§¯æœ¨éœ€è¦ç‰¹æ®Šæ¥å£æ‰èƒ½æ‹¼æ¥...

ç”¨ RunnableLambda åŒ…è£…ä¸€ä¸‹ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ”¨ é”¤å­        â”‚
â”‚  â—‹â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—‹  â”‚  â† åŠ ä¸Šä¹é«˜æ¥å£
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç°åœ¨å®ƒå¯ä»¥å’Œå…¶ä»–ä¹é«˜é›¶ä»¶æ‹¼åœ¨ä¸€èµ·äº†ï¼
```

---

### ç±»æ¯”æ€»ç»“è¡¨

| Runnable æ¦‚å¿µ | å‰ç«¯ç±»æ¯” | å°æœ‹å‹ç±»æ¯” |
|--------------|---------|-----------|
| Runnable åè®® | React Component æ¥å£ | USB æ¥å£æ ‡å‡† |
| invoke() | render() / å‡½æ•°è°ƒç”¨ | æŒ‰ä¸‹å¼€å§‹æŒ‰é’® |
| RunnableSequence (`\|`) | Redux middleware / RxJS pipe | æ¥åŠ›èµ›è·‘ |
| RunnableParallel | Promise.all | åŒæ—¶åšä¸åŒä½œä¸š |
| RunnableLambda | é«˜é˜¶å‡½æ•°åŒ…è£… | ç»™æ™®é€šå·¥å…·è£…ä¹é«˜æ¥å£ |
| RunnablePassthrough | identity å‡½æ•° | åŸæ ·å¤å° |
| RunnableBranch | if-else / switch | èµ°è¿·å®«é€‰è·¯ |
| stream() | Observable / Event Stream | æ°´é¾™å¤´æµæ°´ |
| batch() | Promise.all + map | æ‰¹é‡ç”Ÿäº§ |
| with_config() | React Context | è´´æ ‡ç­¾ |

---

## 6. ã€åç›´è§‰ç‚¹ã€‘

### è¯¯åŒº1ï¼šRunnable åªæ˜¯ç®€å•çš„å‡½æ•°åŒ…è£… âŒ

**ä¸ºä»€ä¹ˆé”™ï¼Ÿ**
- Runnable ä¸ä»…ä»…æ˜¯åŒ…è£…å‡½æ•°ï¼Œå®ƒæä¾›äº†**å®Œæ•´çš„æ‰§è¡Œèƒ½åŠ›çŸ©é˜µ**
- å®ç° invoke åè‡ªåŠ¨è·å¾— batch/stream/ainvoke/astream
- æ”¯æŒé…ç½®ä¼ é€’ã€å›è°ƒç³»ç»Ÿã€ç±»å‹æ¨æ–­

**ä¸ºä»€ä¹ˆäººä»¬å®¹æ˜“è¿™æ ·é”™ï¼Ÿ**
çœ‹åˆ° `invoke(input) -> output`ï¼Œå¾ˆå®¹æ˜“è®¤ä¸ºè¿™åªæ˜¯ç»™å‡½æ•°æ¢äº†ä¸ªåå­—ã€‚å®é™…ä¸Š Runnable æ˜¯ä¸€å¥—å®Œæ•´çš„æ‰§è¡Œæ¡†æ¶ã€‚

**æ­£ç¡®ç†è§£ï¼š**

```python
# âŒ é”™è¯¯ç†è§£ï¼šåªæ˜¯æ¢ä¸ªåå­—
def my_func(x):
    return x * 2

# "è¿™ä¸å°±æ˜¯ my_func(x) å—ï¼Ÿ"

# âœ… æ­£ç¡®ç†è§£ï¼šè·å¾—äº†å®Œæ•´çš„æ‰§è¡Œèƒ½åŠ›
from langchain_core.runnables import RunnableLambda

runnable = RunnableLambda(my_func)

# 1. åŒæ­¥å•æ¬¡
result = runnable.invoke(5)

# 2. æ‰¹é‡å¤„ç†ï¼ˆè‡ªåŠ¨å¹¶è¡Œä¼˜åŒ–ï¼‰
results = runnable.batch([1, 2, 3, 4, 5])

# 3. æµå¼è¾“å‡º
for chunk in runnable.stream(5):
    print(chunk)

# 4. å¼‚æ­¥æ‰§è¡Œ
result = await runnable.ainvoke(5)

# 5. é…ç½®å’Œå›è°ƒ
result = runnable.invoke(5, config={
    "callbacks": [MyCallback()],
    "tags": ["production"]
})

# 6. ç»„åˆèƒ½åŠ›
chain = other | runnable | another
```

---

### è¯¯åŒº2ï¼š| æ“ä½œç¬¦åªæ˜¯è¯­æ³•ç³–ï¼Œæ²¡ä»€ä¹ˆç‰¹åˆ«çš„ âŒ

**ä¸ºä»€ä¹ˆé”™ï¼Ÿ**
- `|` åˆ›å»ºçš„ RunnableSequence æœ‰æ™ºèƒ½çš„ç±»å‹æ¨æ–­
- æµå¼æ‰§è¡Œæ—¶ä¼šè‡ªåŠ¨ä¼˜åŒ–ï¼ˆåªæœ‰æœ€åä¸€ä¸ªæµå¼ï¼‰
- æ”¯æŒåµŒå¥—ç»„åˆå’Œåˆ†æ”¯é€»è¾‘

**ä¸ºä»€ä¹ˆäººä»¬å®¹æ˜“è¿™æ ·é”™ï¼Ÿ**
åœ¨ Unix shell ä¸­ `|` ç¡®å®åªæ˜¯ç®€å•çš„ç®¡é“ï¼Œä½† LangChain çš„ `|` æ›´å¼ºå¤§ã€‚

**æ­£ç¡®ç†è§£ï¼š**

```python
# âŒ é”™è¯¯ç†è§£ï¼šåªæ˜¯ç®€å•ä¸²è”
# "prompt | llm ä¸å°±æ˜¯å…ˆæ‰§è¡Œ prompt å†æ‰§è¡Œ llm å—ï¼Ÿ"

# âœ… æ­£ç¡®ç†è§£ï¼šæ™ºèƒ½çš„åºåˆ—ç»„åˆ

# 1. ç±»å‹æ¨æ–­
chain = prompt | llm | parser
# chain.input_schema æ¥è‡ª prompt
# chain.output_schema æ¥è‡ª parser

# 2. æµå¼ä¼˜åŒ–
for chunk in chain.stream(input):
    # prompt å’Œ parser ä¸äº§ç”Ÿæµ
    # åªæœ‰ llm äº§ç”Ÿæµå¼è¾“å‡º
    print(chunk)

# 3. åµŒå¥—ç»„åˆ
chain = (
    RunnableParallel(
        context=retriever,
        question=RunnablePassthrough()
    )  # å¹¶è¡Œ
    | prompt  # ç„¶åä¸²è¡Œ
    | llm
    | parser
)

# 4. é”™è¯¯ä¼ æ’­å’Œé…ç½®ä¼ é€’
# é…ç½®ä¼šè‡ªåŠ¨ä¼ é€’ç»™æ‰€æœ‰ç»„ä»¶
# é”™è¯¯ä¼šå¸¦ä¸Šå®Œæ•´çš„æ‰§è¡Œè·¯å¾„ä¿¡æ¯
```

---

### è¯¯åŒº3ï¼šbatch å°±æ˜¯å¾ªç¯è°ƒç”¨ invoke âŒ

**ä¸ºä»€ä¹ˆé”™ï¼Ÿ**
- é»˜è®¤å®ç°ç¡®å®æ˜¯å¾ªç¯ï¼Œä½†å¯ä»¥è¢«é‡å†™ä¸ºå¹¶è¡Œæ‰§è¡Œ
- å¾ˆå¤šç»„ä»¶æœ‰ä¼˜åŒ–çš„ batch å®ç°ï¼ˆå¦‚ LLM çš„æ‰¹é‡ API è°ƒç”¨ï¼‰
- å¼‚æ­¥ç‰ˆæœ¬ `abatch` ä¼šè‡ªåŠ¨å¹¶å‘æ‰§è¡Œ

**ä¸ºä»€ä¹ˆäººä»¬å®¹æ˜“è¿™æ ·é”™ï¼Ÿ**
çœ‹åˆ°é»˜è®¤å®ç°æ˜¯ `[self.invoke(x) for x in inputs]`ï¼Œå°±ä»¥ä¸ºæ°¸è¿œæ˜¯è¿™æ ·ã€‚

**æ­£ç¡®ç†è§£ï¼š**

```python
# âŒ é”™è¯¯ç†è§£ï¼šbatch æ²¡æœ‰æ€§èƒ½ä¼˜åŠ¿
# "batch([1,2,3]) ä¸å°±æ˜¯ [invoke(1), invoke(2), invoke(3)] å—ï¼Ÿ"

# âœ… æ­£ç¡®ç†è§£ï¼šbatch å¯ä»¥è¢«ä¼˜åŒ–

# 1. LLM çš„ batch ä¼šåˆå¹¶ API è°ƒç”¨
llm = ChatOpenAI()
results = llm.batch(["Hello", "Hi", "Hey"])
# å¯èƒ½åªå‘é€ä¸€æ¬¡ API è¯·æ±‚ï¼ˆå–å†³äºå®ç°ï¼‰

# 2. å¼‚æ­¥ batch è‡ªåŠ¨å¹¶å‘
results = await chain.abatch(inputs, config={"max_concurrency": 10})
# æœ€å¤šåŒæ—¶æ‰§è¡Œ 10 ä¸ª

# 3. è‡ªå®šä¹‰ batch ä¼˜åŒ–
class OptimizedRunnable(Runnable):
    def batch(self, inputs, config=None):
        # æ‰¹é‡æŸ¥è¯¢æ•°æ®åº“ï¼Œè€Œä¸æ˜¯é€ä¸ªæŸ¥è¯¢
        return self.db.bulk_query(inputs)
```

---

## 7. ã€å®æˆ˜ä»£ç ã€‘

```python
"""
ç¤ºä¾‹ï¼šæ·±å…¥ç†è§£ Runnable åè®®
æ¼”ç¤º Runnable çš„æ ¸å¿ƒæ¦‚å¿µå’Œå®é™…åº”ç”¨
"""

from abc import ABC, abstractmethod
from typing import Any, Dict, List, Iterator, Optional, TypeVar, Generic, Callable
import asyncio

# ===== 1. å®ç°è‡ªå®šä¹‰ Runnable =====
print("=== 1. è‡ªå®šä¹‰ Runnable ===")

Input = TypeVar("Input")
Output = TypeVar("Output")

class Runnable(ABC, Generic[Input, Output]):
    """ç®€åŒ–ç‰ˆ Runnable åŸºç±»"""

    @abstractmethod
    def invoke(self, input: Input, config: Optional[Dict] = None) -> Output:
        pass

    def batch(self, inputs: List[Input], config: Optional[Dict] = None) -> List[Output]:
        return [self.invoke(x, config) for x in inputs]

    def stream(self, input: Input, config: Optional[Dict] = None) -> Iterator[Output]:
        yield self.invoke(input, config)

    def __or__(self, other: "Runnable") -> "RunnableSequence":
        return RunnableSequence(first=self, last=other)

    def __repr__(self) -> str:
        return f"{self.__class__.__name__}()"

class RunnableSequence(Runnable[Input, Output]):
    """Runnable åºåˆ—"""

    def __init__(self, first: Runnable, last: Runnable, middle: List[Runnable] = None):
        self.first = first
        self.middle = middle or []
        self.last = last

    def invoke(self, input: Input, config: Optional[Dict] = None) -> Output:
        result = self.first.invoke(input, config)
        for step in self.middle:
            result = step.invoke(result, config)
        return self.last.invoke(result, config)

    def stream(self, input: Input, config: Optional[Dict] = None) -> Iterator[Output]:
        # å‰é¢çš„æ­¥éª¤æ­£å¸¸æ‰§è¡Œ
        result = self.first.invoke(input, config)
        for step in self.middle:
            result = step.invoke(result, config)
        # æœ€åä¸€æ­¥æµå¼æ‰§è¡Œ
        for chunk in self.last.stream(result, config):
            yield chunk

    def __or__(self, other: Runnable) -> "RunnableSequence":
        return RunnableSequence(
            first=self.first,
            middle=self.middle + [self.last],
            last=other
        )

    def __repr__(self) -> str:
        steps = [self.first] + self.middle + [self.last]
        return " | ".join(repr(s) for s in steps)

class RunnableLambda(Runnable[Input, Output]):
    """åŒ…è£…æ™®é€šå‡½æ•°"""

    def __init__(self, func: Callable[[Input], Output]):
        self.func = func

    def invoke(self, input: Input, config: Optional[Dict] = None) -> Output:
        return self.func(input)

    def __repr__(self) -> str:
        return f"RunnableLambda({self.func.__name__})"

# åˆ›å»ºå…·ä½“çš„ Runnable
class PromptTemplate(Runnable[Dict[str, Any], str]):
    """æç¤ºæ¨¡æ¿"""

    def __init__(self, template: str):
        self.template = template

    def invoke(self, input: Dict[str, Any], config: Optional[Dict] = None) -> str:
        return self.template.format(**input)

class FakeLLM(Runnable[str, str]):
    """æ¨¡æ‹Ÿ LLM"""

    def __init__(self, prefix: str = "AI:"):
        self.prefix = prefix

    def invoke(self, input: str, config: Optional[Dict] = None) -> str:
        return f"{self.prefix} Response to '{input[:20]}...'"

    def stream(self, input: str, config: Optional[Dict] = None) -> Iterator[str]:
        response = self.invoke(input, config)
        # æ¨¡æ‹Ÿæµå¼è¾“å‡ºï¼šé€å­—è¾“å‡º
        for char in response:
            yield char

class OutputParser(Runnable[str, Dict[str, Any]]):
    """è¾“å‡ºè§£æå™¨"""

    def invoke(self, input: str, config: Optional[Dict] = None) -> Dict[str, Any]:
        return {"text": input, "length": len(input)}

# æ¼”ç¤º
prompt = PromptTemplate("Tell me about {topic} in {style} style")
llm = FakeLLM()
parser = OutputParser()

# ä½¿ç”¨ | ç»„åˆ
chain = prompt | llm | parser
print(f"Chain: {chain}")

result = chain.invoke({"topic": "Python", "style": "simple"})
print(f"Result: {result}")

# ===== 2. æµå¼è¾“å‡º =====
print("\n=== 2. æµå¼è¾“å‡º ===")

simple_chain = prompt | llm
print("Streaming: ", end="")
for chunk in simple_chain.stream({"topic": "AI", "style": "fun"}):
    print(chunk, end="", flush=True)
print()

# ===== 3. RunnableParallel =====
print("\n=== 3. RunnableParallel ===")

class RunnableParallel(Runnable[Input, Dict[str, Any]]):
    """å¹¶è¡Œæ‰§è¡Œå¤šä¸ª Runnable"""

    def __init__(self, **steps: Runnable):
        self.steps = steps

    def invoke(self, input: Input, config: Optional[Dict] = None) -> Dict[str, Any]:
        return {
            key: runnable.invoke(input, config)
            for key, runnable in self.steps.items()
        }

    def __repr__(self) -> str:
        return f"RunnableParallel({list(self.steps.keys())})"

class RunnablePassthrough(Runnable[Input, Input]):
    """é€ä¼ è¾“å…¥"""

    def invoke(self, input: Input, config: Optional[Dict] = None) -> Input:
        return input

# RAG é£æ ¼çš„ç®¡é“
def fake_retriever(query: Dict) -> str:
    return f"Retrieved docs for: {query.get('question', query)}"

rag_chain = (
    RunnableParallel(
        context=RunnableLambda(fake_retriever),
        question=RunnablePassthrough()
    )
    | RunnableLambda(lambda x: f"Context: {x['context']}\nQuestion: {x['question']}")
    | llm
)

print(f"RAG Chain: {rag_chain}")
result = rag_chain.invoke({"question": "What is LangChain?"})
print(f"Result: {result}")

# ===== 4. æ‰¹é‡å¤„ç† =====
print("\n=== 4. æ‰¹é‡å¤„ç† ===")

inputs = [
    {"topic": "Python", "style": "technical"},
    {"topic": "JavaScript", "style": "casual"},
    {"topic": "Rust", "style": "detailed"},
]

results = chain.batch(inputs)
for inp, res in zip(inputs, results):
    print(f"  {inp['topic']}: {res['length']} chars")

# ===== 5. RunnableBranch =====
print("\n=== 5. RunnableBranch ===")

class RunnableBranch(Runnable[Input, Output]):
    """æ¡ä»¶åˆ†æ”¯"""

    def __init__(self, *branches, default: Runnable):
        self.branches = branches  # List of (condition, runnable)
        self.default = default

    def invoke(self, input: Input, config: Optional[Dict] = None) -> Output:
        for condition, runnable in self.branches:
            if condition(input):
                return runnable.invoke(input, config)
        return self.default.invoke(input, config)

# åˆ›å»ºåˆ†æ”¯
branch = RunnableBranch(
    (lambda x: "code" in x.get("question", "").lower(),
     RunnableLambda(lambda x: f"[CODE MODE] {x}")),
    (lambda x: "translate" in x.get("question", "").lower(),
     RunnableLambda(lambda x: f"[TRANSLATE MODE] {x}")),
    default=RunnableLambda(lambda x: f"[GENERAL MODE] {x}")
)

test_inputs = [
    {"question": "Write code for sorting"},
    {"question": "Translate hello to Chinese"},
    {"question": "What is the weather?"},
]

for inp in test_inputs:
    result = branch.invoke(inp)
    print(f"  {inp['question'][:25]}... -> {result[:30]}...")

# ===== 6. with_config æ¨¡å¼ =====
print("\n=== 6. é…ç½®ä¼ é€’ ===")

class ConfigAwareRunnable(Runnable[str, str]):
    """é…ç½®æ„ŸçŸ¥çš„ Runnable"""

    def invoke(self, input: str, config: Optional[Dict] = None) -> str:
        config = config or {}
        tags = config.get("tags", [])
        run_name = config.get("run_name", "unnamed")
        return f"[{run_name}][tags:{tags}] Processed: {input}"

aware = ConfigAwareRunnable()

# æ— é…ç½®
print(aware.invoke("Hello"))

# æœ‰é…ç½®
print(aware.invoke("Hello", config={
    "run_name": "production",
    "tags": ["important", "user-123"]
}))

# ===== 7. ç»„åˆå¤æ‚ç®¡é“ =====
print("\n=== 7. å¤æ‚ç®¡é“ç¤ºä¾‹ ===")

# æ¨¡æ‹Ÿä¸€ä¸ªå®Œæ•´çš„ RAG ç®¡é“
def format_docs(docs: str) -> str:
    return f"<docs>{docs}</docs>"

def extract_answer(response: Dict) -> str:
    return response.get("text", "")[:50]

complex_chain = (
    # ç¬¬ä¸€æ­¥ï¼šå¹¶è¡Œè·å– context å’Œä¿ç•™ question
    RunnableParallel(
        context=RunnableLambda(fake_retriever) | RunnableLambda(format_docs),
        question=RunnablePassthrough()
    )
    # ç¬¬äºŒæ­¥ï¼šæ ¼å¼åŒ–ä¸º prompt
    | RunnableLambda(lambda x: f"Context: {x['context']}\n\nQuestion: {x['question']}\n\nAnswer:")
    # ç¬¬ä¸‰æ­¥ï¼šè°ƒç”¨ LLM
    | llm
    # ç¬¬å››æ­¥ï¼šè§£æè¾“å‡º
    | parser
    # ç¬¬äº”æ­¥ï¼šæå–ç­”æ¡ˆ
    | RunnableLambda(extract_answer)
)

print(f"Complex chain structure:")
print(f"  {complex_chain}")
print()

final_result = complex_chain.invoke({"question": "How does LangChain work?"})
print(f"Final answer: {final_result}")

print("\n=== å®Œæˆ ===")
```

**è¿è¡Œè¾“å‡ºç¤ºä¾‹ï¼š**

```
=== 1. è‡ªå®šä¹‰ Runnable ===
Chain: PromptTemplate() | FakeLLM() | OutputParser()
Result: {'text': "AI: Response to 'Tell me about Pytho...'", 'length': 42}

=== 2. æµå¼è¾“å‡º ===
Streaming: AI: Response to 'Tell me about AI i...'

=== 3. RunnableParallel ===
RAG Chain: RunnableParallel(['context', 'question']) | RunnableLambda(<lambda>) | FakeLLM()
Result: AI: Response to 'Context: Retrieved...'

=== 4. æ‰¹é‡å¤„ç† ===
  Python: 45 chars
  JavaScript: 49 chars
  Rust: 45 chars

=== 5. RunnableBranch ===
  Write code for sorting... -> [CODE MODE] {'question': 'Wri...
  Translate hello to Chine... -> [TRANSLATE MODE] {'question...
  What is the weather?... -> [GENERAL MODE] {'question':...

=== 6. é…ç½®ä¼ é€’ ===
[unnamed][tags:[]] Processed: Hello
[production][tags:['important', 'user-123']] Processed: Hello

=== 7. å¤æ‚ç®¡é“ç¤ºä¾‹ ===
Complex chain structure:
  RunnableParallel(['context', 'question']) | RunnableLambda(<lambda>) | FakeLLM() | OutputParser() | RunnableLambda(extract_answer)

Final answer: AI: Response to 'Context: <docs>Ret

=== å®Œæˆ ===
```

---

## 8. ã€é¢è¯•å¿…é—®ã€‘

### é—®é¢˜ï¼š"LangChain çš„ Runnable åè®®æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆè¦è®¾è®¡è¿™ä¸ªï¼Ÿ"

**æ™®é€šå›ç­”ï¼ˆâŒ ä¸å‡ºå½©ï¼‰ï¼š**
"Runnable æ˜¯ LangChain çš„åŸºç±»ï¼Œæ‰€æœ‰ç»„ä»¶éƒ½ç»§æ‰¿å®ƒï¼Œå¯ä»¥ç”¨ invoke æ–¹æ³•è°ƒç”¨ã€‚"

**å‡ºå½©å›ç­”ï¼ˆâœ… æ¨èï¼‰ï¼š**

> **Runnable åè®®æœ‰ä¸‰ä¸ªå±‚é¢çš„æ„ä¹‰ï¼š**
>
> 1. **ç»Ÿä¸€æ¥å£å±‚é¢**ï¼š
>    - æ‰€æœ‰ç»„ä»¶ï¼ˆPromptã€LLMã€Parserã€Retrieverï¼‰éƒ½å®ç° `invoke(input) -> output`
>    - è¿™è®©ç»„ä»¶å¯ä»¥åƒä¹é«˜ç§¯æœ¨ä¸€æ ·éšæ„ç»„åˆ
>    - ç±»ä¼¼äº React ç»„ä»¶éƒ½è¦å®ç° `render()`
>
> 2. **æ‰§è¡Œèƒ½åŠ›å±‚é¢**ï¼š
>    - å®ç° `invoke` åè‡ªåŠ¨è·å¾— `batch`ã€`stream`ã€`ainvoke`ã€`astream`
>    - è¿™æ˜¯æ¨¡æ¿æ–¹æ³•æ¨¡å¼çš„å…¸å‹åº”ç”¨
>    - å­ç±»åªéœ€å…³æ³¨æ ¸å¿ƒé€»è¾‘ï¼Œæ‰§è¡Œæ¡†æ¶ç”±åŸºç±»æä¾›
>
> 3. **ç»„åˆèƒ½åŠ›å±‚é¢**ï¼š
>    - é€šè¿‡ `__or__` é‡è½½å®ç° `|` æ“ä½œç¬¦ï¼ˆLCEL è¯­æ³•ï¼‰
>    - `prompt | llm | parser` åˆ›å»º `RunnableSequence`
>    - æ”¯æŒä¸²è¡Œã€å¹¶è¡Œã€æ¡ä»¶åˆ†æ”¯ç­‰å¤æ‚ç»„åˆ
>
> **ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ**
> - **é—®é¢˜**ï¼šLLM åº”ç”¨éœ€è¦ç»„åˆå¤šç§ç»„ä»¶ï¼Œæ¯ç§ç»„ä»¶ API ä¸åŒ
> - **è§£å†³**ï¼šå®šä¹‰ç»Ÿä¸€åè®®ï¼Œè®©æ‰€æœ‰ç»„ä»¶å¯äº’æ¢ã€å¯ç»„åˆ
> - **å¥½å¤„**ï¼šç”¨æˆ·å¯ä»¥ç”¨å£°æ˜å¼è¯­æ³•ï¼ˆLCELï¼‰æ„å»ºå¤æ‚ç®¡é“
>
> **å®é™…ä¾‹å­ï¼š**
> ```python
> # ä¸€è¡Œä»£ç æ„å»º RAG ç®¡é“
> rag = retriever | prompt | llm | parser
> ```

**ä¸ºä»€ä¹ˆè¿™ä¸ªå›ç­”å‡ºå½©ï¼Ÿ**
1. âœ… åˆ†å±‚æ¬¡è§£é‡Šï¼ˆæ¥å£ã€æ‰§è¡Œã€ç»„åˆï¼‰
2. âœ… è¯´æ˜äº†è®¾è®¡åŠ¨æœºå’Œè§£å†³çš„é—®é¢˜
3. âœ… è”ç³»äº†è®¾è®¡æ¨¡å¼ï¼ˆæ¨¡æ¿æ–¹æ³•ï¼‰
4. âœ… ç»™å‡ºäº†ç®€æ´çš„ä»£ç ç¤ºä¾‹

---

### é—®é¢˜ï¼š"LCEL çš„ | æ“ä½œç¬¦æ˜¯æ€ä¹ˆå®ç°çš„ï¼Ÿ"

**æ™®é€šå›ç­”ï¼ˆâŒ ä¸å‡ºå½©ï¼‰ï¼š**
"| æ˜¯ Python çš„ä½æˆ–æ“ä½œç¬¦ï¼ŒLangChain é‡è½½äº†å®ƒæ¥è¿æ¥ç»„ä»¶ã€‚"

**å‡ºå½©å›ç­”ï¼ˆâœ… æ¨èï¼‰ï¼š**

> **LCEL çš„ `|` å®ç°æ¶‰åŠä¸‰ä¸ªè¦ç‚¹ï¼š**
>
> 1. **Python é­”æœ¯æ–¹æ³•**ï¼š
>    - `|` æ“ä½œç¬¦å¯¹åº” `__or__` æ–¹æ³•
>    - `a | b` å®é™…è°ƒç”¨ `a.__or__(b)`
>    - å¦‚æœ `a` æ²¡æœ‰ `__or__`ï¼Œä¼šè°ƒç”¨ `b.__ror__(a)`
>
> 2. **RunnableSequence åˆ›å»º**ï¼š
>    ```python
>    class Runnable:
>        def __or__(self, other: Runnable) -> RunnableSequence:
>            return RunnableSequence(first=self, last=other)
>    ```
>    - `prompt | llm` è¿”å› `RunnableSequence(prompt, llm)`
>
> 3. **é“¾å¼è°ƒç”¨æ”¯æŒ**ï¼š
>    - `RunnableSequence` ä¹Ÿæ˜¯ `Runnable`
>    - `(prompt | llm) | parser` ä»ç„¶æœ‰æ•ˆ
>    - é€šè¿‡é‡å†™ `__or__` æ”¯æŒä»»æ„é•¿åº¦çš„é“¾
>
> **æ‰§è¡Œæµç¨‹ï¼š**
> ```python
> chain = prompt | llm | parser
> # ç­‰ä»·äº
> chain = RunnableSequence(
>     first=prompt,
>     middle=[llm],
>     last=parser
> )
>
> # invoke æ‰§è¡Œ
> result = chain.invoke(input)
> # -> prompt.invoke(input)
> # -> llm.invoke(result1)
> # -> parser.invoke(result2)
> ```

---

## 9. ã€åŒ–éª¨ç»µæŒã€‘

### å¡ç‰‡1ï¼šRunnable æ˜¯ä»€ä¹ˆ ğŸ¯

**ä¸€å¥è¯ï¼š** Runnable æ˜¯ LangChain æ‰€æœ‰å¯æ‰§è¡Œç»„ä»¶çš„ç»Ÿä¸€æ¥å£ã€‚

**ä¸¾ä¾‹ï¼š**
```python
from langchain_core.runnables import Runnable

# æ‰€æœ‰è¿™äº›éƒ½æ˜¯ Runnable
prompt: Runnable      # æç¤ºæ¨¡æ¿
llm: Runnable         # è¯­è¨€æ¨¡å‹
parser: Runnable      # è¾“å‡ºè§£æå™¨
retriever: Runnable   # æ£€ç´¢å™¨
chain: Runnable       # é“¾
```

**åº”ç”¨ï¼š** åªè¦æ˜¯ Runnableï¼Œå°±èƒ½ç”¨ `invoke()` è°ƒç”¨ï¼Œå°±èƒ½ç”¨ `|` ç»„åˆã€‚

---

### å¡ç‰‡2ï¼šinvoke æ ¸å¿ƒæ–¹æ³• ğŸ“

**ä¸€å¥è¯ï¼š** `invoke(input) -> output` æ˜¯ Runnable çš„æ ¸å¿ƒæ‰§è¡Œæ–¹æ³•ã€‚

**ä¸¾ä¾‹ï¼š**
```python
# æ‰€æœ‰ç»„ä»¶éƒ½ç”¨ invoke è°ƒç”¨
result = prompt.invoke({"topic": "AI"})
result = llm.invoke("Hello")
result = parser.invoke(text)
result = chain.invoke(input)
```

**åº”ç”¨ï¼š** ä¸ç®¡ä»€ä¹ˆç»„ä»¶ï¼Œåªè¦çŸ¥é“è¾“å…¥æ ¼å¼ï¼Œå°±èƒ½ç”¨ `invoke` æ‰§è¡Œã€‚

---

### å¡ç‰‡3ï¼š| æ“ä½œç¬¦ï¼ˆLCELï¼‰ğŸ”—

**ä¸€å¥è¯ï¼š** `|` æ“ä½œç¬¦å°†å¤šä¸ª Runnable ä¸²è”æˆç®¡é“ã€‚

**ä¸¾ä¾‹ï¼š**
```python
# åˆ›å»ºå¤„ç†ç®¡é“
chain = prompt | llm | parser

# ç­‰ä»·äº
def chain(input):
    x = prompt.invoke(input)
    x = llm.invoke(x)
    return parser.invoke(x)
```

**åº”ç”¨ï¼š** ç”¨ `|` å¯ä»¥ä¸€è¡Œä»£ç æ„å»ºå¤æ‚çš„å¤„ç†æµç¨‹ã€‚

---

### å¡ç‰‡4ï¼šRunnableSequence ğŸ”„

**ä¸€å¥è¯ï¼š** RunnableSequence æ˜¯ `|` æ“ä½œç¬¦çš„è¿”å›å€¼ï¼Œè¡¨ç¤ºä¸²è¡Œæ‰§è¡Œåºåˆ—ã€‚

**ä¸¾ä¾‹ï¼š**
```python
chain = prompt | llm  # è¿”å› RunnableSequence

# å†…éƒ¨ç»“æ„
class RunnableSequence:
    first: Runnable   # prompt
    last: Runnable    # llm

    def invoke(self, input):
        x = self.first.invoke(input)
        return self.last.invoke(x)
```

**åº”ç”¨ï¼š** ç†è§£ RunnableSequence æ‰èƒ½ç†è§£ LCEL çš„å·¥ä½œåŸç†ã€‚

---

### å¡ç‰‡5ï¼šRunnableParallel å¹¶è¡Œ ğŸ”€

**ä¸€å¥è¯ï¼š** RunnableParallel å¹¶è¡Œæ‰§è¡Œå¤šä¸ª Runnableï¼Œç»“æœåˆå¹¶ä¸ºå­—å…¸ã€‚

**ä¸¾ä¾‹ï¼š**
```python
from langchain_core.runnables import RunnableParallel

parallel = RunnableParallel(
    summary=summary_chain,
    keywords=keyword_chain,
)

result = parallel.invoke(text)
# result = {"summary": "...", "keywords": [...]}
```

**åº”ç”¨ï¼š** RAG ä¸­å¸¸ç”¨æ¥åŒæ—¶è·å– context å’Œä¼ é€’ questionã€‚

---

### å¡ç‰‡6ï¼šRunnableLambda åŒ…è£… ğŸ”§

**ä¸€å¥è¯ï¼š** RunnableLambda å°†æ™®é€šå‡½æ•°åŒ…è£…æˆ Runnableã€‚

**ä¸¾ä¾‹ï¼š**
```python
from langchain_core.runnables import RunnableLambda

def postprocess(text: str) -> str:
    return text.upper()

# åŒ…è£…æˆ Runnable
wrapped = RunnableLambda(postprocess)

# ç°åœ¨å¯ä»¥å‚ä¸ç®¡é“
chain = prompt | llm | wrapped
```

**åº”ç”¨ï¼š** è®©ä»»ä½•è‡ªå®šä¹‰å‡½æ•°éƒ½èƒ½åŠ å…¥ LCEL ç®¡é“ã€‚

---

### å¡ç‰‡7ï¼šstream æµå¼è¾“å‡º ğŸŒŠ

**ä¸€å¥è¯ï¼š** `stream()` æ–¹æ³•å®ç°æµå¼è¾“å‡ºï¼Œé€æ­¥è¿”å›ç»“æœã€‚

**ä¸¾ä¾‹ï¼š**
```python
# æµå¼è·å– LLM å“åº”
for chunk in chain.stream({"topic": "AI"}):
    print(chunk, end="", flush=True)
    # é€å­—æ‰“å°ï¼Œä¸ç­‰å®Œæ•´å“åº”
```

**åº”ç”¨ï¼š** å®æ—¶æ˜¾ç¤º LLM è¾“å‡ºï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

---

### å¡ç‰‡8ï¼šbatch æ‰¹é‡å¤„ç† ğŸ“¦

**ä¸€å¥è¯ï¼š** `batch()` æ–¹æ³•æ‰¹é‡å¤„ç†å¤šä¸ªè¾“å…¥ã€‚

**ä¸¾ä¾‹ï¼š**
```python
inputs = [
    {"topic": "Python"},
    {"topic": "JavaScript"},
    {"topic": "Rust"},
]

# æ‰¹é‡å¤„ç†
results = chain.batch(inputs)
# results = [result1, result2, result3]
```

**åº”ç”¨ï¼š** æ‰¹é‡å¤„ç†æ•°æ®ï¼Œéƒ¨åˆ†ç»„ä»¶ä¼šä¼˜åŒ–ä¸ºå•æ¬¡ API è°ƒç”¨ã€‚

---

### å¡ç‰‡9ï¼šainvoke å¼‚æ­¥æ‰§è¡Œ âš¡

**ä¸€å¥è¯ï¼š** `ainvoke()` æ˜¯ `invoke()` çš„å¼‚æ­¥ç‰ˆæœ¬ã€‚

**ä¸¾ä¾‹ï¼š**
```python
import asyncio

async def main():
    # å¼‚æ­¥æ‰§è¡Œ
    result = await chain.ainvoke({"topic": "AI"})

    # å¼‚æ­¥å¹¶å‘æ‰¹å¤„ç†
    results = await chain.abatch(inputs)

    # å¼‚æ­¥æµå¼
    async for chunk in chain.astream(input):
        print(chunk)

asyncio.run(main())
```

**åº”ç”¨ï¼š** åœ¨å¼‚æ­¥åº”ç”¨ï¼ˆå¦‚ FastAPIï¼‰ä¸­ä½¿ç”¨ LangChainã€‚

---

### å¡ç‰‡10ï¼šRunnable ç”Ÿæ€å…¨æ™¯ â­

**ä¸€å¥è¯ï¼š** æŒæ¡ Runnable åè®®å°±æŒæ¡äº† LangChain çš„æ ¸å¿ƒã€‚

**æ ¸å¿ƒç»„ä»¶éƒ½æ˜¯ Runnableï¼š**
```python
# å…¨éƒ¨å®ç° Runnable åè®®
ChatPromptTemplate    # æç¤ºæ¨¡æ¿
ChatOpenAI            # èŠå¤©æ¨¡å‹
StrOutputParser       # è¾“å‡ºè§£æå™¨
VectorStoreRetriever  # å‘é‡æ£€ç´¢å™¨
AgentExecutor         # Agent æ‰§è¡Œå™¨
```

**ç»Ÿä¸€çš„ä½¿ç”¨æ–¹å¼ï¼š**
```python
# æ‰€æœ‰ç»„ä»¶éƒ½èƒ½è¿™æ ·ç”¨
component.invoke(input)
component.stream(input)
component.batch(inputs)
await component.ainvoke(input)

# æ‰€æœ‰ç»„ä»¶éƒ½èƒ½ç»„åˆ
chain = a | b | c
```

**åº”ç”¨ï¼š** ç†è§£ Runnable æ˜¯é˜…è¯» LangChain æºç çš„é‡‘é’¥åŒ™ã€‚

---

## 10. ã€ä¸€å¥è¯æ€»ç»“ã€‘

**Runnable æ˜¯ LangChain çš„æ ¸å¿ƒåè®®ï¼Œé€šè¿‡ç»Ÿä¸€çš„ invoke/stream/batch æ¥å£å’Œ | æ“ä½œç¬¦ç»„åˆèƒ½åŠ›ï¼Œè®©æ‰€æœ‰ç»„ä»¶å¯ä»¥åƒä¹é«˜ç§¯æœ¨ä¸€æ ·è‡ªç”±ç»„åˆï¼Œæ˜¯ LCEL è¡¨è¾¾å¼è¯­è¨€çš„åŸºçŸ³ã€‚**

---

## ğŸ“š å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£ Runnable æ˜¯ä»€ä¹ˆä»¥åŠä¸ºä»€ä¹ˆéœ€è¦å®ƒ
- [ ] èƒ½å¤Ÿä½¿ç”¨ invoke è°ƒç”¨å„ç§ LangChain ç»„ä»¶
- [ ] ä¼šä½¿ç”¨ | æ“ä½œç¬¦ç»„åˆç»„ä»¶
- [ ] ç†è§£ RunnableSequence çš„å·¥ä½œåŸç†
- [ ] ä¼šä½¿ç”¨ RunnableParallel å¹¶è¡Œæ‰§è¡Œ
- [ ] èƒ½ç”¨ RunnableLambda åŒ…è£…è‡ªå®šä¹‰å‡½æ•°
- [ ] ä¼šä½¿ç”¨ stream å®ç°æµå¼è¾“å‡º
- [ ] ç†è§£ batch å’Œ ainvoke çš„ç”¨é€”
- [ ] èƒ½å¤Ÿé˜…è¯» LangChain æºç ä¸­çš„ Runnable ç›¸å…³ä»£ç 
- [ ] èƒ½å¤Ÿå®ç°è‡ªå®šä¹‰ Runnable

## ğŸ”— ä¸‹ä¸€æ­¥å­¦ä¹ 

- **LCEL è¡¨è¾¾å¼è¯­è¨€**ï¼šæ·±å…¥å­¦ä¹  LCEL çš„é«˜çº§ç”¨æ³•
- **BaseChatModel å®ç°**ï¼šç†è§£ LLM å¦‚ä½•å®ç° Runnable
- **Callback å›è°ƒç³»ç»Ÿ**ï¼šç†è§£æ‰§è¡Œè¿‡ç¨‹ä¸­çš„äº‹ä»¶å¤„ç†

---

**ç‰ˆæœ¬ï¼š** v1.0
**æœ€åæ›´æ–°ï¼š** 2025-12-12
