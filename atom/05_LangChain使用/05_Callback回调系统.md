# Callback å›è°ƒç³»ç»Ÿ

> åŸå­åŒ–çŸ¥è¯†ç‚¹ | LangChain ä½¿ç”¨ | LangChain æºç å­¦ä¹ æ ¸å¿ƒçŸ¥è¯†

---

## 1. ã€30å­—æ ¸å¿ƒã€‘

**Callback æ˜¯ LangChain çš„äº‹ä»¶ç›‘å¬æœºåˆ¶ï¼Œé€šè¿‡ Handler å¯ä»¥å®ç°æ—¥å¿—è®°å½•ã€æµå¼è¾“å‡ºã€æ€§èƒ½ç›‘æ§å’Œè°ƒè¯•è¿½è¸ªã€‚**

---

## 2. ã€ç¬¬ä¸€æ€§åŸç†ã€‘

### ä»€ä¹ˆæ˜¯ç¬¬ä¸€æ€§åŸç†ï¼Ÿ

**ç¬¬ä¸€æ€§åŸç†**ï¼šå›åˆ°äº‹ç‰©æœ€åŸºæœ¬çš„çœŸç†ï¼Œä»æºå¤´æ€è€ƒé—®é¢˜

### Callback å›è°ƒç³»ç»Ÿçš„ç¬¬ä¸€æ€§åŸç† ğŸ¯

#### 1. æœ€åŸºç¡€çš„å®šä¹‰

**Callback = å½“äº‹ä»¶å‘ç”Ÿæ—¶æ‰§è¡Œçš„å‡½æ•°**

ä»…æ­¤è€Œå·²ï¼æ²¡æœ‰æ›´åŸºç¡€çš„äº†ã€‚

```python
# Callback çš„æœ¬è´¨
def on_event(event_data):
    # äº‹ä»¶å‘ç”Ÿæ—¶æ‰§è¡Œè¿™ä¸ªå‡½æ•°
    print(f"äº‹ä»¶å‘ç”Ÿäº†: {event_data}")

# æ³¨å†Œ callback
llm.on("start", on_event)

# å½“ LLM å¼€å§‹æ—¶ï¼Œon_event è¢«è°ƒç”¨
```

#### 2. ä¸ºä»€ä¹ˆéœ€è¦ Callbackï¼Ÿ

**æ ¸å¿ƒé—®é¢˜ï¼šéœ€è¦åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­æ’å…¥è‡ªå®šä¹‰é€»è¾‘**

```python
# æ²¡æœ‰ Callback çš„é—®é¢˜
result = chain.invoke(input)
# åªèƒ½çœ‹åˆ°æœ€ç»ˆç»“æœ
# ä¸çŸ¥é“ä¸­é—´å‘ç”Ÿäº†ä»€ä¹ˆ
# æ— æ³•ï¼š
# âŒ å®æ—¶çœ‹åˆ°ç”Ÿæˆè¿‡ç¨‹ï¼ˆæµå¼è¾“å‡ºï¼‰
# âŒ è®°å½•æ‰§è¡Œæ—¥å¿—
# âŒ ç›‘æ§æ€§èƒ½å’Œæˆæœ¬
# âŒ è°ƒè¯•é—®é¢˜

# æœ‰ Callback çš„è§£å†³æ–¹æ¡ˆ
# âœ… on_llm_new_tokenï¼šæ¯ç”Ÿæˆä¸€ä¸ª token å°±é€šçŸ¥
# âœ… on_llm_start/endï¼šè®°å½•å¼€å§‹å’Œç»“æŸæ—¶é—´
# âœ… on_chain_errorï¼šæ•è·é”™è¯¯
```

#### 3. Callback çš„ä¸‰å±‚ä»·å€¼

##### ä»·å€¼1ï¼šæµå¼è¾“å‡º - å®æ—¶åé¦ˆ

```python
# ä¸ç­‰ LLM å®Œæˆï¼Œè¾¹ç”Ÿæˆè¾¹æ˜¾ç¤º
class StreamHandler(BaseCallbackHandler):
    def on_llm_new_token(self, token, **kwargs):
        print(token, end="", flush=True)  # å®æ—¶æ‰“å°

# ç”¨æˆ·çœ‹åˆ°ï¼š
# "ä½ " â†’ "ä½ å¥½" â†’ "ä½ å¥½ï¼" â†’ "ä½ å¥½ï¼æˆ‘" â†’ ...
# è€Œä¸æ˜¯ç­‰ 5 ç§’åçœ‹åˆ°å®Œæ•´å›å¤
```

##### ä»·å€¼2ï¼šç›‘æ§è¿½è¸ª - äº†è§£æ‰§è¡Œè¿‡ç¨‹

```python
# è®°å½•æ¯ä¸€æ­¥çš„æ‰§è¡Œ
class MonitorHandler(BaseCallbackHandler):
    def on_chain_start(self, serialized, inputs, **kwargs):
        print(f"Chain å¼€å§‹: {inputs}")

    def on_llm_start(self, serialized, prompts, **kwargs):
        print(f"LLM è°ƒç”¨: {len(prompts)} ä¸ª prompt")

    def on_llm_end(self, response, **kwargs):
        print(f"LLM è¿”å›: {response.generations[0][0].text[:50]}...")
```

##### ä»·å€¼3ï¼šæˆæœ¬æ§åˆ¶ - ç»Ÿè®¡ Token ä½¿ç”¨

```python
# ç»Ÿè®¡ Token æ¶ˆè€—
class CostHandler(BaseCallbackHandler):
    def __init__(self):
        self.total_tokens = 0

    def on_llm_end(self, response, **kwargs):
        usage = response.llm_output.get("token_usage", {})
        self.total_tokens += usage.get("total_tokens", 0)
        print(f"æœ¬æ¬¡: {usage}, ç´¯è®¡: {self.total_tokens}")
```

#### 4. ä»ç¬¬ä¸€æ€§åŸç†æ¨å¯¼ Callback è®¾è®¡

**æ¨ç†é“¾ï¼š**

```
1. æ‰§è¡Œè¿‡ç¨‹æ˜¯é»‘ç›’
   â†“
2. éœ€è¦è§‚å¯Ÿå†…éƒ¨çŠ¶æ€
   â†“
3. åœ¨å…³é”®èŠ‚ç‚¹æ’å…¥é’©å­
   â†“
4. å®šä¹‰æ ‡å‡†çš„äº‹ä»¶ç±»å‹
   â†“
5. on_llm_start, on_llm_end, on_chain_start...
   â†“
6. ç”¨æˆ·å®ç° Handler å¤„ç†äº‹ä»¶
   â†“
7. é€šè¿‡ CallbackManager ç®¡ç†å¤šä¸ª Handler
```

#### 5. ä¸€å¥è¯æ€»ç»“ç¬¬ä¸€æ€§åŸç†

**Callback æ˜¯åœ¨æ‰§è¡Œè¿‡ç¨‹çš„å…³é”®èŠ‚ç‚¹æ’å…¥çš„é’©å­ï¼Œè®©å¼€å‘è€…èƒ½å¤Ÿè§‚å¯Ÿã€è®°å½•ã€æ§åˆ¶ LLM åº”ç”¨çš„æ‰§è¡Œè¿‡ç¨‹ã€‚**

---

## 3. ã€æ ¸å¿ƒæ¦‚å¿µï¼ˆå…¨é¢è¦†ç›–ï¼‰ã€‘

### æ ¸å¿ƒæ¦‚å¿µ1ï¼šBaseCallbackHandler å›è°ƒå¤„ç†å™¨ ğŸ“¡

**BaseCallbackHandler å®šä¹‰äº†æ‰€æœ‰å¯ç›‘å¬çš„äº‹ä»¶é’©å­**

```python
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.messages import BaseMessage
from typing import Any, Dict, List

class MyCallbackHandler(BaseCallbackHandler):
    """è‡ªå®šä¹‰å›è°ƒå¤„ç†å™¨"""

    # ===== LLM ç›¸å…³äº‹ä»¶ =====
    def on_llm_start(
        self,
        serialized: Dict[str, Any],
        prompts: List[str],
        **kwargs
    ) -> None:
        """LLM å¼€å§‹è°ƒç”¨æ—¶"""
        print(f"LLM å¼€å§‹ï¼Œprompt æ•°é‡: {len(prompts)}")

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """ç”Ÿæˆæ–° token æ—¶ï¼ˆæµå¼ï¼‰"""
        print(token, end="", flush=True)

    def on_llm_end(self, response, **kwargs) -> None:
        """LLM è°ƒç”¨ç»“æŸæ—¶"""
        print(f"\nLLM ç»“æŸ")

    def on_llm_error(self, error: Exception, **kwargs) -> None:
        """LLM è°ƒç”¨å‡ºé”™æ—¶"""
        print(f"LLM é”™è¯¯: {error}")

    # ===== Chain ç›¸å…³äº‹ä»¶ =====
    def on_chain_start(
        self,
        serialized: Dict[str, Any],
        inputs: Dict[str, Any],
        **kwargs
    ) -> None:
        """Chain å¼€å§‹æ‰§è¡Œæ—¶"""
        print(f"Chain å¼€å§‹: {inputs}")

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs) -> None:
        """Chain æ‰§è¡Œç»“æŸæ—¶"""
        print(f"Chain ç»“æŸ: {outputs}")

    def on_chain_error(self, error: Exception, **kwargs) -> None:
        """Chain æ‰§è¡Œå‡ºé”™æ—¶"""
        print(f"Chain é”™è¯¯: {error}")

    # ===== Tool ç›¸å…³äº‹ä»¶ =====
    def on_tool_start(
        self,
        serialized: Dict[str, Any],
        input_str: str,
        **kwargs
    ) -> None:
        """Tool å¼€å§‹æ‰§è¡Œæ—¶"""
        print(f"Tool å¼€å§‹: {input_str}")

    def on_tool_end(self, output: str, **kwargs) -> None:
        """Tool æ‰§è¡Œç»“æŸæ—¶"""
        print(f"Tool ç»“æŸ: {output}")

    def on_tool_error(self, error: Exception, **kwargs) -> None:
        """Tool æ‰§è¡Œå‡ºé”™æ—¶"""
        print(f"Tool é”™è¯¯: {error}")

    # ===== Agent ç›¸å…³äº‹ä»¶ =====
    def on_agent_action(self, action, **kwargs) -> None:
        """Agent æ‰§è¡ŒåŠ¨ä½œæ—¶"""
        print(f"Agent åŠ¨ä½œ: {action.tool}")

    def on_agent_finish(self, finish, **kwargs) -> None:
        """Agent å®Œæˆæ—¶"""
        print(f"Agent å®Œæˆ: {finish.return_values}")

    # ===== Retriever ç›¸å…³äº‹ä»¶ =====
    def on_retriever_start(self, serialized, query, **kwargs) -> None:
        """Retriever å¼€å§‹æ£€ç´¢æ—¶"""
        print(f"æ£€ç´¢: {query}")

    def on_retriever_end(self, documents, **kwargs) -> None:
        """Retriever æ£€ç´¢ç»“æŸæ—¶"""
        print(f"æ£€ç´¢åˆ° {len(documents)} ä¸ªæ–‡æ¡£")
```

**äº‹ä»¶ç±»å‹é€ŸæŸ¥è¡¨ï¼š**

| ç»„ä»¶ | å¼€å§‹äº‹ä»¶ | ç»“æŸäº‹ä»¶ | é”™è¯¯äº‹ä»¶ | ç‰¹æ®Šäº‹ä»¶ |
|-----|---------|---------|---------|---------|
| LLM | `on_llm_start` | `on_llm_end` | `on_llm_error` | `on_llm_new_token` |
| Chain | `on_chain_start` | `on_chain_end` | `on_chain_error` | - |
| Tool | `on_tool_start` | `on_tool_end` | `on_tool_error` | - |
| Agent | - | - | - | `on_agent_action`, `on_agent_finish` |
| Retriever | `on_retriever_start` | `on_retriever_end` | `on_retriever_error` | - |

---

### æ ¸å¿ƒæ¦‚å¿µ2ï¼šCallbackManager å›è°ƒç®¡ç†å™¨ ğŸ›ï¸

**CallbackManager ç®¡ç†å¤šä¸ª Handlerï¼Œè´Ÿè´£äº‹ä»¶çš„åˆ†å‘**

```python
from langchain_core.callbacks import CallbackManager

# åˆ›å»ºå¤šä¸ª Handler
stream_handler = StreamingHandler()
monitor_handler = MonitorHandler()
cost_handler = CostHandler()

# é€šè¿‡ CallbackManager ç®¡ç†
callback_manager = CallbackManager(
    handlers=[stream_handler, monitor_handler, cost_handler]
)

# æˆ–è€…ç›´æ¥ä¼ é€’åˆ—è¡¨
llm = ChatOpenAI(callbacks=[stream_handler, monitor_handler])
```

**ä¸¤ç§ä¼ é€’ Callback çš„æ–¹å¼ï¼š**

```python
# æ–¹å¼1ï¼šæ„é€ æ—¶ä¼ é€’ï¼ˆå…¨å±€ç”Ÿæ•ˆï¼‰
llm = ChatOpenAI(callbacks=[handler1, handler2])

# æ–¹å¼2ï¼šè°ƒç”¨æ—¶ä¼ é€’ï¼ˆå•æ¬¡ç”Ÿæ•ˆï¼‰
result = chain.invoke(
    {"input": "ä½ å¥½"},
    config={"callbacks": [handler3]}
)

# ä¸¤ç§æ–¹å¼å¯ä»¥ç»„åˆä½¿ç”¨
# æ„é€ æ—¶çš„æ˜¯"å¸¸é©»"ï¼Œè°ƒç”¨æ—¶çš„æ˜¯"ä¸´æ—¶"
```

**åœ¨ LangChain æºç ä¸­çš„åº”ç”¨ï¼š**

```python
# langchain_core/callbacks/manager.py
class CallbackManager:
    """å›è°ƒç®¡ç†å™¨"""

    def __init__(
        self,
        handlers: List[BaseCallbackHandler] = None,
        inheritable_handlers: List[BaseCallbackHandler] = None,
    ):
        self.handlers = handlers or []
        self.inheritable_handlers = inheritable_handlers or []

    def on_llm_start(self, serialized, prompts, **kwargs):
        """å¹¿æ’­ LLM å¼€å§‹äº‹ä»¶"""
        for handler in self.handlers:
            handler.on_llm_start(serialized, prompts, **kwargs)

    # ... å…¶ä»–äº‹ä»¶æ–¹æ³•ç±»ä¼¼
```

---

### æ ¸å¿ƒæ¦‚å¿µ3ï¼šæµå¼è¾“å‡º Streaming ğŸŒŠ

**æµå¼è¾“å‡ºæ˜¯ Callback æœ€å¸¸ç”¨çš„åº”ç”¨åœºæ™¯**

```python
from langchain_core.callbacks import StreamingStdOutCallbackHandler
from langchain_openai import ChatOpenAI

# æ–¹å¼1ï¼šä½¿ç”¨å†…ç½®çš„ StreamingStdOutCallbackHandler
llm = ChatOpenAI(
    streaming=True,  # å¼€å¯æµå¼
    callbacks=[StreamingStdOutCallbackHandler()]
)

response = llm.invoke("å†™ä¸€é¦–è¯—")
# å®æ—¶æ‰“å°æ¯ä¸ª token

# æ–¹å¼2ï¼šè‡ªå®šä¹‰æµå¼å¤„ç†
class CustomStreamHandler(BaseCallbackHandler):
    def __init__(self):
        self.tokens = []

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.tokens.append(token)
        # å¯ä»¥å‘é€åˆ° WebSocketã€æ›´æ–° UI ç­‰
        print(token, end="", flush=True)

# æ–¹å¼3ï¼šä½¿ç”¨ astream æ–¹æ³•
async for chunk in llm.astream("å†™ä¸€é¦–è¯—"):
    print(chunk.content, end="", flush=True)

# æ–¹å¼4ï¼šä½¿ç”¨ astream_events è·å–è¯¦ç»†äº‹ä»¶
async for event in chain.astream_events({"input": "ä½ å¥½"}, version="v2"):
    kind = event["event"]
    if kind == "on_llm_stream":
        print(event["data"]["chunk"].content, end="")
```

**æµå¼è¾“å‡ºçš„ä¸‰ç§æ–¹å¼å¯¹æ¯”ï¼š**

| æ–¹å¼ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|-----|------|------|---------|
| Callback | çµæ´»ï¼Œå¯è‡ªå®šä¹‰å¤„ç† | éœ€è¦å®ç° Handler | å¤æ‚çš„æµå¼å¤„ç† |
| `astream` | ç®€å•ï¼Œasync for | åªèƒ½è·å–å†…å®¹ | ç®€å•çš„æµå¼æ˜¾ç¤º |
| `astream_events` | ä¿¡æ¯æœ€å…¨ | å¤æ‚ | éœ€è¦è¯¦ç»†äº‹ä»¶ä¿¡æ¯ |

---

### æ ¸å¿ƒæ¦‚å¿µ4ï¼šRunnableConfig é…ç½®ä¼ é€’ âš™ï¸

**é€šè¿‡ config å‚æ•°ä¼ é€’ callbacks å’Œå…¶ä»–é…ç½®**

```python
from langchain_core.runnables import RunnableConfig

# å®šä¹‰é…ç½®
config = RunnableConfig(
    callbacks=[MyHandler()],
    tags=["production", "user_123"],
    metadata={"user_id": "123", "session_id": "abc"},
    max_concurrency=5,
)

# è°ƒç”¨æ—¶ä¼ é€’
result = chain.invoke(input, config=config)

# æˆ–è€…ä½¿ç”¨å­—å…¸å½¢å¼
result = chain.invoke(
    input,
    config={
        "callbacks": [handler],
        "tags": ["test"],
        "metadata": {"key": "value"}
    }
)

# config ä¼šè‡ªåŠ¨ä¼ é€’ç»™ Chain ä¸­çš„æ‰€æœ‰ç»„ä»¶
chain = prompt | llm | parser
# ä¸‰ä¸ªç»„ä»¶éƒ½ä¼šæ”¶åˆ°ç›¸åŒçš„ config
```

---

### æ‰©å±•æ¦‚å¿µ5ï¼šå¼‚æ­¥ Callback ğŸ”„

**AsyncCallbackHandler ç”¨äºå¼‚æ­¥åœºæ™¯**

```python
from langchain_core.callbacks import AsyncCallbackHandler
import asyncio

class AsyncStreamHandler(AsyncCallbackHandler):
    """å¼‚æ­¥æµå¼å¤„ç†å™¨"""

    async def on_llm_new_token(self, token: str, **kwargs) -> None:
        # å¯ä»¥è¿›è¡Œå¼‚æ­¥æ“ä½œ
        await send_to_websocket(token)

    async def on_llm_end(self, response, **kwargs) -> None:
        await notify_completion()

# ä½¿ç”¨
handler = AsyncStreamHandler()
async for chunk in llm.astream("ä½ å¥½", config={"callbacks": [handler]}):
    pass
```

---

### æ‰©å±•æ¦‚å¿µ6ï¼šå†…ç½® Callback Handler ğŸ“¦

**LangChain æä¾›å¤šä¸ªå†…ç½®çš„ Handler**

```python
from langchain_core.callbacks import (
    StreamingStdOutCallbackHandler,  # æ ‡å‡†è¾“å‡ºæµå¼
    StdOutCallbackHandler,           # æ ‡å‡†è¾“å‡ºï¼ˆéæµå¼ï¼‰
    FileCallbackHandler,             # å†™å…¥æ–‡ä»¶
)

# 1. æµå¼è¾“å‡ºåˆ°ç»ˆç«¯
streaming_handler = StreamingStdOutCallbackHandler()

# 2. è¯¦ç»†æ—¥å¿—åˆ°ç»ˆç«¯
stdout_handler = StdOutCallbackHandler()

# 3. æ—¥å¿—å†™å…¥æ–‡ä»¶
file_handler = FileCallbackHandler("output.log")

# ç»„åˆä½¿ç”¨
llm = ChatOpenAI(
    streaming=True,
    callbacks=[streaming_handler, file_handler]
)
```

---

## 4. ã€æœ€å°å¯ç”¨ã€‘

æŒæ¡ä»¥ä¸‹å†…å®¹ï¼Œå°±èƒ½åœ¨ LangChain ä¸­ä½¿ç”¨ Callbackï¼š

### 4.1 æµå¼è¾“å‡º

```python
from langchain_core.callbacks import StreamingStdOutCallbackHandler
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()]
)

response = llm.invoke("å†™ä¸€é¦–è¯—")
# å®æ—¶è¾“å‡ºæ¯ä¸ª token
```

### 4.2 è‡ªå®šä¹‰ Handler

```python
from langchain_core.callbacks import BaseCallbackHandler

class MyHandler(BaseCallbackHandler):
    def on_llm_start(self, serialized, prompts, **kwargs):
        print("å¼€å§‹...")

    def on_llm_end(self, response, **kwargs):
        print("ç»“æŸï¼")

llm = ChatOpenAI(callbacks=[MyHandler()])
```

### 4.3 é€šè¿‡ config ä¼ é€’

```python
result = chain.invoke(
    {"input": "ä½ å¥½"},
    config={"callbacks": [MyHandler()]}
)
```

### 4.4 å¼‚æ­¥æµå¼

```python
async for chunk in llm.astream("ä½ å¥½"):
    print(chunk.content, end="")
```

**è¿™äº›çŸ¥è¯†è¶³ä»¥ï¼š**
- å®ç°æµå¼è¾“å‡ºæå‡ç”¨æˆ·ä½“éªŒ
- æ·»åŠ æ—¥å¿—è®°å½•å’Œç›‘æ§
- è¿½è¸ªæ‰§è¡Œè¿‡ç¨‹å’Œè°ƒè¯•
- ç»Ÿè®¡ Token ä½¿ç”¨å’Œæˆæœ¬

---

## 5. ã€1ä¸ªç±»æ¯”ã€‘ï¼ˆåŒè½¨åˆ¶ï¼‰

### ç±»æ¯”1ï¼šCallback æ˜¯äº‹ä»¶ç›‘å¬å™¨

#### ğŸ¨ å‰ç«¯è§†è§’ï¼šaddEventListener / React Hooks

Callback å°±åƒå‰ç«¯çš„äº‹ä»¶ç›‘å¬ï¼Œåœ¨ç‰¹å®šæ—¶æœºè§¦å‘ã€‚

```javascript
// DOM äº‹ä»¶ç›‘å¬
button.addEventListener('click', (event) => {
  console.log('æŒ‰é’®è¢«ç‚¹å‡»äº†');
});

// React useEffect
useEffect(() => {
  console.log('ç»„ä»¶æŒ‚è½½äº†');
  return () => console.log('ç»„ä»¶å¸è½½äº†');
}, []);

// è‡ªå®šä¹‰ Hook
function useLoading() {
  const [loading, setLoading] = useState(false);
  const onStart = () => setLoading(true);
  const onEnd = () => setLoading(false);
  return { loading, onStart, onEnd };
}
```

```python
# LangChain Callback
class MyHandler(BaseCallbackHandler):
    def on_llm_start(self, **kwargs):  # ç±»ä¼¼ onStart
        print("å¼€å§‹")

    def on_llm_end(self, **kwargs):    # ç±»ä¼¼ onEnd
        print("ç»“æŸ")
```

**å…³é”®ç›¸ä¼¼ç‚¹ï¼š**
- éƒ½æ˜¯äº‹ä»¶é©±åŠ¨
- éƒ½åœ¨ç‰¹å®šæ—¶æœºè§¦å‘
- éƒ½å¯ä»¥æœ‰å¤šä¸ªç›‘å¬å™¨

#### ğŸ§’ å°æœ‹å‹è§†è§’ï¼šé—¹é’Ÿæé†’

Callback å°±åƒè®¾ç½®çš„å„ç§é—¹é’Ÿï¼š

```
ä½ è®¾ç½®äº†å‡ ä¸ªé—¹é’Ÿï¼š
- 7:00 èµ·åºŠé—¹é’Ÿ â†’ on_llm_startï¼ˆå¼€å§‹äº†æé†’æˆ‘ï¼‰
- 7:30 åƒæ—©é¥­é—¹é’Ÿ â†’ on_llm_new_tokenï¼ˆæ¯æ¬¡æœ‰æ–°ä¸œè¥¿æé†’æˆ‘ï¼‰
- 8:00 ä¸Šå­¦é—¹é’Ÿ â†’ on_llm_endï¼ˆç»“æŸäº†æé†’æˆ‘ï¼‰

é—¹é’Ÿå“äº†ï¼Œä½ å°±çŸ¥é“è¯¥åšä»€ä¹ˆäº†ï¼
```

---

### ç±»æ¯”2ï¼šæµå¼è¾“å‡ºæ˜¯æ°´é¾™å¤´

#### ğŸ¨ å‰ç«¯è§†è§’ï¼šServer-Sent Events (SSE) / WebSocket

æµå¼è¾“å‡ºå°±åƒ SSEï¼ŒæœåŠ¡å™¨ä¸æ–­æ¨é€æ•°æ®ã€‚

```javascript
// Server-Sent Events
const eventSource = new EventSource('/stream');

eventSource.onmessage = (event) => {
  // æ”¶åˆ°ä¸€å°å—æ•°æ®
  appendToUI(event.data);
};

// WebSocket
const ws = new WebSocket('ws://server');

ws.onmessage = (event) => {
  // æ”¶åˆ°æ¶ˆæ¯
  updateUI(event.data);
};
```

```python
# LangChain æµå¼
class StreamHandler(BaseCallbackHandler):
    def on_llm_new_token(self, token, **kwargs):
        # æ”¶åˆ°ä¸€ä¸ª token
        send_to_client(token)
```

#### ğŸ§’ å°æœ‹å‹è§†è§’ï¼šæ°´é¾™å¤´æµæ°´

æµå¼è¾“å‡ºå°±åƒæ°´é¾™å¤´ï¼š

```
æ™®é€šæ–¹å¼ï¼ˆéæµå¼ï¼‰ï¼š
æ‰“å¼€æ°´é¾™å¤´ â†’ ç­‰æ°´è£…æ»¡ä¸€æ¡¶ â†’ å…³æ°´é¾™å¤´ â†’ æ‹¿èµ°æ•´æ¡¶æ°´
ï¼ˆç­‰å¾ˆä¹…æ‰èƒ½ç”¨æ°´ï¼‰

æµå¼æ–¹å¼ï¼š
æ‰“å¼€æ°´é¾™å¤´ â†’ æ°´ä¸€ç›´æµ â†’ éœ€è¦å¤šå°‘æ¥å¤šå°‘ â†’ éšæ—¶èƒ½ç”¨æ°´
ï¼ˆé©¬ä¸Šå°±èƒ½ç”¨æ°´ï¼‰

LLM æµå¼è¾“å‡ºï¼š
é—®é—®é¢˜ â†’ AI ä¸€ä¸ªå­—ä¸€ä¸ªå­—å›ç­” â†’ ä½ è¾¹çœ‹è¾¹è¯»
ï¼ˆä¸ç”¨ç­‰å…¨éƒ¨ç”Ÿæˆå®Œï¼‰
```

---

### ç±»æ¯”3ï¼šCallbackManager æ˜¯å¹¿æ’­ç«™

#### ğŸ¨ å‰ç«¯è§†è§’ï¼šEventEmitter / å‘å¸ƒè®¢é˜…

CallbackManager å°±åƒä¸€ä¸ªäº‹ä»¶å¹¿æ’­ç³»ç»Ÿã€‚

```javascript
// Node.js EventEmitter
const emitter = new EventEmitter();

// è®¢é˜…è€…1
emitter.on('message', (data) => console.log('è®¢é˜…è€…1:', data));

// è®¢é˜…è€…2
emitter.on('message', (data) => saveToLog(data));

// å‘å¸ƒæ¶ˆæ¯
emitter.emit('message', 'Hello');
// ä¸¤ä¸ªè®¢é˜…è€…éƒ½æ”¶åˆ°æ¶ˆæ¯
```

```python
# LangChain CallbackManager
manager = CallbackManager(handlers=[handler1, handler2, handler3])

# å½“äº‹ä»¶å‘ç”Ÿæ—¶ï¼Œæ‰€æœ‰ handler éƒ½è¢«é€šçŸ¥
manager.on_llm_start(...)  # handler1, handler2, handler3 éƒ½è¢«è°ƒç”¨
```

#### ğŸ§’ å°æœ‹å‹è§†è§’ï¼šå­¦æ ¡å¹¿æ’­ç«™

CallbackManager å°±åƒå­¦æ ¡çš„å¹¿æ’­ç«™ï¼š

```
å¹¿æ’­ç«™ï¼ˆCallbackManagerï¼‰å¹¿æ’­ä¸€æ¡æ¶ˆæ¯

æ‰€æœ‰æ•™å®¤ï¼ˆHandlerï¼‰éƒ½èƒ½å¬åˆ°ï¼š
- ä¸€å¹´çº§æ•™å®¤ â†’ è®°å½•åˆ°æ—¥å¿—
- äºŒå¹´çº§æ•™å®¤ â†’ æ›´æ–°å¤§å±å¹•
- ä¸‰å¹´çº§æ•™å®¤ â†’ é€šçŸ¥å®¶é•¿

ä¸€æ¡å¹¿æ’­ï¼Œå¤šä¸ªåœ°æ–¹å“åº”ï¼
```

---

### ç±»æ¯”æ€»ç»“è¡¨

| LangChain æ¦‚å¿µ | å‰ç«¯ç±»æ¯” | å°æœ‹å‹ç±»æ¯” |
|---------------|---------|-----------|
| Callback | addEventListener | é—¹é’Ÿæé†’ |
| CallbackHandler | äº‹ä»¶å¤„ç†å‡½æ•° | å¬åˆ°é—¹é’Ÿååšçš„äº‹ |
| CallbackManager | EventEmitter | å¹¿æ’­ç«™ |
| on_llm_new_token | SSE onmessage | æ°´é¾™å¤´æµæ°´ |
| streaming | Server Push | è¾¹åšè¾¹çœ‹ |
| config | Context | ä¼ é€’è®¾ç½® |

---

## 6. ã€åç›´è§‰ç‚¹ã€‘

### è¯¯åŒº1ï¼šCallback ä¼šé˜»å¡ä¸»æµç¨‹ âŒ

**ä¸ºä»€ä¹ˆé”™ï¼Ÿ**
- Callback æ˜¯æ—è·¯å¤„ç†ï¼Œä¸å½±å“ä¸»æµç¨‹çš„è¿”å›å€¼
- å³ä½¿ Callback å‡ºé”™ï¼Œä¸»æµç¨‹ä¹Ÿå¯ä»¥ç»§ç»­
- Callback çš„æ‰§è¡Œæ—¶é—´ä¸è®¡å…¥ä¸»æµç¨‹

**ä¸ºä»€ä¹ˆäººä»¬å®¹æ˜“è¿™æ ·é”™ï¼Ÿ**
ä»¥ä¸º Callback æ˜¯ä¸²è¡Œæ‰§è¡Œçš„"ä¸­é—´ä»¶"ã€‚

**æ­£ç¡®ç†è§£ï¼š**

```python
# Callback æ˜¯æ—è·¯ï¼Œä¸å½±å“è¿”å›å€¼
class SlowHandler(BaseCallbackHandler):
    def on_llm_end(self, response, **kwargs):
        time.sleep(10)  # æ…¢å¤„ç†
        print("å¤„ç†å®Œæˆ")

# ä¸»æµç¨‹ä¸å—å½±å“
result = llm.invoke("ä½ å¥½")  # æ­£å¸¸è¿”å›
# ä¹‹å SlowHandler.on_llm_end æ‰æ‰§è¡Œ

# å³ä½¿ Callback å‡ºé”™
class BuggyHandler(BaseCallbackHandler):
    def on_llm_end(self, response, **kwargs):
        raise Exception("å‡ºé”™äº†ï¼")

# ä¸»æµç¨‹ä»ç„¶å¯ä»¥å¾—åˆ°ç»“æœ
result = llm.invoke("ä½ å¥½")  # ä¾ç„¶è¿”å›ç»“æœ
# Callback é”™è¯¯è¢«è®°å½•ä½†ä¸å½±å“ä¸»æµç¨‹
```

---

### è¯¯åŒº2ï¼šæµå¼è¾“å‡ºåªæ˜¯æ‰“å° âŒ

**ä¸ºä»€ä¹ˆé”™ï¼Ÿ**
- æµå¼æ¶‰åŠ LLM çš„æµå¼ç”Ÿæˆæ¨¡å¼
- éœ€è¦æ­£ç¡®çš„ streaming=True è®¾ç½®
- Token çš„ä¼ é€’ã€ç´¯ç§¯ã€å±•ç¤ºæ˜¯å®Œæ•´é“¾è·¯
- å‰ç«¯é›†æˆéœ€è¦ WebSocket/SSE

**ä¸ºä»€ä¹ˆäººä»¬å®¹æ˜“è¿™æ ·é”™ï¼Ÿ**
åªçœ‹åˆ° print() è¿™ä¸€æ­¥ã€‚

**æ­£ç¡®ç†è§£ï¼š**

```python
# å®Œæ•´çš„æµå¼é“¾è·¯

# 1. LLM å±‚ï¼šå¼€å¯æµå¼ç”Ÿæˆ
llm = ChatOpenAI(streaming=True)

# 2. Callback å±‚ï¼šå¤„ç† token
class WebSocketHandler(BaseCallbackHandler):
    def __init__(self, websocket):
        self.ws = websocket

    def on_llm_new_token(self, token, **kwargs):
        # å‘é€åˆ°å‰ç«¯
        self.ws.send(token)

# 3. å‰ç«¯å±‚ï¼šæ¥æ”¶å’Œæ˜¾ç¤º
# JavaScript
ws.onmessage = (event) => {
    appendToChat(event.data);
};

# 4. è¿˜éœ€è¦è€ƒè™‘ï¼š
# - é”™è¯¯å¤„ç†
# - è¿æ¥æ–­å¼€
# - å¤šç”¨æˆ·éš”ç¦»
# - è¶…æ—¶å¤„ç†
```

---

### è¯¯åŒº3ï¼šæ‰€æœ‰äº‹ä»¶éƒ½ä¼šè§¦å‘ Callback âŒ

**ä¸ºä»€ä¹ˆé”™ï¼Ÿ**
- åªæœ‰ Handler å®ç°çš„æ–¹æ³•æ‰ä¼šè¢«è°ƒç”¨
- æœªå®ç°çš„æ–¹æ³•é»˜è®¤ç©ºæ“ä½œ
- éœ€è¦æ ¹æ®éœ€æ±‚é€‰æ‹©æ€§å®ç°

**ä¸ºä»€ä¹ˆäººä»¬å®¹æ˜“è¿™æ ·é”™ï¼Ÿ**
ä»¥ä¸ºç»§æ‰¿ BaseCallbackHandler å°±ä¼šæ”¶åˆ°æ‰€æœ‰äº‹ä»¶ã€‚

**æ­£ç¡®ç†è§£ï¼š**

```python
# åªå®ç°éœ€è¦çš„æ–¹æ³•
class MinimalHandler(BaseCallbackHandler):
    """åªå…³å¿ƒ LLM å¼€å§‹å’Œç»“æŸ"""

    def on_llm_start(self, serialized, prompts, **kwargs):
        print("å¼€å§‹")  # è¿™ä¸ªä¼šè¢«è°ƒç”¨

    def on_llm_end(self, response, **kwargs):
        print("ç»“æŸ")  # è¿™ä¸ªä¼šè¢«è°ƒç”¨

    # on_llm_new_token æ²¡å®ç°
    # â†’ æµå¼ token äº‹ä»¶ä¸ä¼šè¢«å¤„ç†

    # on_chain_start æ²¡å®ç°
    # â†’ Chain å¼€å§‹äº‹ä»¶ä¸ä¼šè¢«å¤„ç†

# å¦‚æœéœ€è¦æ‰€æœ‰äº‹ä»¶
class FullHandler(BaseCallbackHandler):
    def on_llm_start(self, ...): pass
    def on_llm_new_token(self, ...): pass
    def on_llm_end(self, ...): pass
    def on_chain_start(self, ...): pass
    def on_chain_end(self, ...): pass
    # ... å®ç°æ‰€æœ‰éœ€è¦çš„æ–¹æ³•
```

---

## 7. ã€å®æˆ˜ä»£ç ã€‘

```python
"""
ç¤ºä¾‹ï¼šCallback å›è°ƒç³»ç»Ÿå®Œæ•´æ¼”ç¤º
å±•ç¤º LangChain ä¸­ Callback çš„æ ¸å¿ƒç”¨æ³•
"""

from typing import Any, Dict, List, Optional
from dataclasses import dataclass, field
from datetime import datetime
import time

# ===== 1. åŸºç¡€äº‹ä»¶å’Œæ¶ˆæ¯ç»“æ„ =====
print("=== 1. åŸºç¡€ç»“æ„ ===")

@dataclass
class LLMResult:
    """LLM è¿”å›ç»“æœ"""
    text: str
    token_count: int = 0

@dataclass
class Event:
    """äº‹ä»¶å¯¹è±¡"""
    type: str
    data: Any
    timestamp: datetime = field(default_factory=datetime.now)

# ===== 2. BaseCallbackHandler å®ç° =====
print("\n=== 2. BaseCallbackHandler ===")

class BaseCallbackHandler:
    """å›è°ƒå¤„ç†å™¨åŸºç±»"""

    def on_llm_start(self, serialized: Dict, prompts: List[str], **kwargs) -> None:
        """LLM å¼€å§‹"""
        pass

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """æ–° token"""
        pass

    def on_llm_end(self, response: LLMResult, **kwargs) -> None:
        """LLM ç»“æŸ"""
        pass

    def on_llm_error(self, error: Exception, **kwargs) -> None:
        """LLM é”™è¯¯"""
        pass

    def on_chain_start(self, serialized: Dict, inputs: Dict, **kwargs) -> None:
        """Chain å¼€å§‹"""
        pass

    def on_chain_end(self, outputs: Dict, **kwargs) -> None:
        """Chain ç»“æŸ"""
        pass

    def on_tool_start(self, serialized: Dict, input_str: str, **kwargs) -> None:
        """Tool å¼€å§‹"""
        pass

    def on_tool_end(self, output: str, **kwargs) -> None:
        """Tool ç»“æŸ"""
        pass

# ===== 3. è‡ªå®šä¹‰ Handler å®ç° =====
print("\n=== 3. è‡ªå®šä¹‰ Handler ===")

class StreamingHandler(BaseCallbackHandler):
    """æµå¼è¾“å‡ºå¤„ç†å™¨"""

    def __init__(self):
        self.tokens = []

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.tokens.append(token)
        print(token, end="", flush=True)

    def get_full_response(self) -> str:
        return "".join(self.tokens)

class MonitorHandler(BaseCallbackHandler):
    """ç›‘æ§å¤„ç†å™¨"""

    def __init__(self):
        self.events = []
        self.start_time = None

    def on_llm_start(self, serialized: Dict, prompts: List[str], **kwargs) -> None:
        self.start_time = time.time()
        self.events.append(Event("llm_start", {"prompt_count": len(prompts)}))
        print(f"\n[Monitor] LLM å¼€å§‹ï¼Œ{len(prompts)} ä¸ª prompt")

    def on_llm_end(self, response: LLMResult, **kwargs) -> None:
        duration = time.time() - self.start_time if self.start_time else 0
        self.events.append(Event("llm_end", {"duration": duration}))
        print(f"[Monitor] LLM ç»“æŸï¼Œè€—æ—¶ {duration:.2f}s")

    def on_chain_start(self, serialized: Dict, inputs: Dict, **kwargs) -> None:
        self.events.append(Event("chain_start", inputs))
        print(f"[Monitor] Chain å¼€å§‹: {list(inputs.keys())}")

    def on_chain_end(self, outputs: Dict, **kwargs) -> None:
        self.events.append(Event("chain_end", outputs))
        print(f"[Monitor] Chain ç»“æŸ: {list(outputs.keys())}")

class CostHandler(BaseCallbackHandler):
    """æˆæœ¬ç»Ÿè®¡å¤„ç†å™¨"""

    def __init__(self, price_per_1k_tokens: float = 0.002):
        self.total_tokens = 0
        self.total_cost = 0.0
        self.price_per_1k = price_per_1k_tokens

    def on_llm_end(self, response: LLMResult, **kwargs) -> None:
        tokens = response.token_count
        cost = (tokens / 1000) * self.price_per_1k
        self.total_tokens += tokens
        self.total_cost += cost
        print(f"[Cost] æœ¬æ¬¡: {tokens} tokens (${cost:.4f}), "
              f"ç´¯è®¡: {self.total_tokens} tokens (${self.total_cost:.4f})")

# ===== 4. CallbackManager å®ç° =====
print("\n=== 4. CallbackManager ===")

class CallbackManager:
    """å›è°ƒç®¡ç†å™¨"""

    def __init__(self, handlers: List[BaseCallbackHandler] = None):
        self.handlers = handlers or []

    def add_handler(self, handler: BaseCallbackHandler):
        self.handlers.append(handler)

    def on_llm_start(self, serialized: Dict, prompts: List[str], **kwargs):
        for handler in self.handlers:
            try:
                handler.on_llm_start(serialized, prompts, **kwargs)
            except Exception as e:
                print(f"Handler é”™è¯¯: {e}")

    def on_llm_new_token(self, token: str, **kwargs):
        for handler in self.handlers:
            try:
                handler.on_llm_new_token(token, **kwargs)
            except Exception as e:
                pass  # æµå¼è¾“å‡ºä¸æ‰“æ–­

    def on_llm_end(self, response: LLMResult, **kwargs):
        for handler in self.handlers:
            try:
                handler.on_llm_end(response, **kwargs)
            except Exception as e:
                print(f"Handler é”™è¯¯: {e}")

    def on_chain_start(self, serialized: Dict, inputs: Dict, **kwargs):
        for handler in self.handlers:
            try:
                handler.on_chain_start(serialized, inputs, **kwargs)
            except Exception as e:
                print(f"Handler é”™è¯¯: {e}")

    def on_chain_end(self, outputs: Dict, **kwargs):
        for handler in self.handlers:
            try:
                handler.on_chain_end(outputs, **kwargs)
            except Exception as e:
                print(f"Handler é”™è¯¯: {e}")

# ===== 5. æ¨¡æ‹Ÿ LLM å’Œ Chain =====
print("\n=== 5. æ¨¡æ‹Ÿ LLM ===")

class MockLLM:
    """æ¨¡æ‹Ÿ LLMï¼ˆæ”¯æŒæµå¼ï¼‰"""

    def __init__(self, callbacks: List[BaseCallbackHandler] = None, streaming: bool = False):
        self.callback_manager = CallbackManager(callbacks or [])
        self.streaming = streaming

    def invoke(self, prompt: str) -> str:
        # é€šçŸ¥å¼€å§‹
        self.callback_manager.on_llm_start({}, [prompt])

        # æ¨¡æ‹Ÿç”Ÿæˆ
        response_text = f"è¿™æ˜¯å¯¹ã€Œ{prompt}ã€çš„å›ç­”ã€‚"

        if self.streaming:
            # æµå¼ï¼šé€å­—è¾“å‡º
            for char in response_text:
                self.callback_manager.on_llm_new_token(char)
                time.sleep(0.05)  # æ¨¡æ‹Ÿç”Ÿæˆå»¶è¿Ÿ
        else:
            time.sleep(0.5)  # æ¨¡æ‹Ÿéæµå¼å»¶è¿Ÿ

        # é€šçŸ¥ç»“æŸ
        result = LLMResult(text=response_text, token_count=len(response_text) * 2)
        self.callback_manager.on_llm_end(result)

        return response_text

# æµ‹è¯•æµå¼è¾“å‡º
print("\næµ‹è¯•æµå¼è¾“å‡º:")
streaming_handler = StreamingHandler()
llm = MockLLM(callbacks=[streaming_handler], streaming=True)
result = llm.invoke("ä½ å¥½")
print(f"\nå®Œæ•´å“åº”: {streaming_handler.get_full_response()}")

# ===== 6. å¤š Handler ç»„åˆ =====
print("\n=== 6. å¤š Handler ç»„åˆ ===")

# åˆ›å»ºå¤šä¸ª Handler
stream_handler = StreamingHandler()
monitor_handler = MonitorHandler()
cost_handler = CostHandler(price_per_1k_tokens=0.002)

# åˆ›å»º LLMï¼ˆç»„åˆå¤šä¸ª Handlerï¼‰
llm = MockLLM(
    callbacks=[stream_handler, monitor_handler, cost_handler],
    streaming=True
)

print("\nç»„åˆä½¿ç”¨å¤šä¸ª Handler:")
result = llm.invoke("å†™ä¸€é¦–è¯—")

print(f"\n\näº‹ä»¶è®°å½•: {len(monitor_handler.events)} ä¸ªäº‹ä»¶")
for event in monitor_handler.events:
    print(f"  - {event.type}: {event.data}")

# ===== 7. Chain ä¸­ä½¿ç”¨ Callback =====
print("\n=== 7. Chain ä¸­ä½¿ç”¨ Callback ===")

class MockChain:
    """æ¨¡æ‹Ÿ Chain"""

    def __init__(self, llm: MockLLM, callbacks: List[BaseCallbackHandler] = None):
        self.llm = llm
        self.callback_manager = CallbackManager(callbacks or [])

    def invoke(self, inputs: Dict) -> Dict:
        # Chain å¼€å§‹
        self.callback_manager.on_chain_start({}, inputs)

        # æ‰§è¡Œ LLM
        prompt = inputs.get("input", "")
        response = self.llm.invoke(prompt)

        # Chain ç»“æŸ
        outputs = {"output": response}
        self.callback_manager.on_chain_end(outputs)

        return outputs

# æµ‹è¯•
monitor = MonitorHandler()
chain = MockChain(
    llm=MockLLM(callbacks=[], streaming=False),
    callbacks=[monitor]
)

print("\nChain æ‰§è¡Œ:")
result = chain.invoke({"input": "ä»€ä¹ˆæ˜¯ Pythonï¼Ÿ"})
print(f"ç»“æœ: {result['output'][:30]}...")

# ===== 8. é€šè¿‡ config ä¼ é€’ Callback =====
print("\n=== 8. é€šè¿‡ config ä¼ é€’ ===")

class ConfigurableLLM:
    """æ”¯æŒ config ä¼ é€’çš„ LLM"""

    def invoke(self, prompt: str, config: Dict = None) -> str:
        config = config or {}
        callbacks = config.get("callbacks", [])
        manager = CallbackManager(callbacks)

        manager.on_llm_start({}, [prompt])

        response = f"å›ç­”: {prompt}"
        time.sleep(0.2)

        result = LLMResult(text=response, token_count=50)
        manager.on_llm_end(result)

        return response

# æµ‹è¯•
llm = ConfigurableLLM()

# è°ƒç”¨æ—¶ä¼ é€’ callback
handler = MonitorHandler()
result = llm.invoke("æµ‹è¯•", config={"callbacks": [handler]})
print(f"ç»“æœ: {result}")

# ===== 9. é”™è¯¯å¤„ç† =====
print("\n=== 9. é”™è¯¯å¤„ç† ===")

class ErrorHandler(BaseCallbackHandler):
    """é”™è¯¯å¤„ç†å™¨"""

    def __init__(self):
        self.errors = []

    def on_llm_error(self, error: Exception, **kwargs) -> None:
        self.errors.append(error)
        print(f"[Error] LLM é”™è¯¯: {error}")

class MockLLMWithError:
    """å¯èƒ½å‡ºé”™çš„ LLM"""

    def __init__(self, callbacks: List[BaseCallbackHandler] = None):
        self.callback_manager = CallbackManager(callbacks or [])

    def invoke(self, prompt: str) -> str:
        self.callback_manager.on_llm_start({}, [prompt])

        if "é”™è¯¯" in prompt:
            error = Exception("æ¨¡æ‹Ÿçš„ LLM é”™è¯¯")
            # é€šçŸ¥é”™è¯¯
            for handler in self.callback_manager.handlers:
                if hasattr(handler, 'on_llm_error'):
                    handler.on_llm_error(error)
            raise error

        return f"å›ç­”: {prompt}"

# æµ‹è¯•é”™è¯¯å¤„ç†
error_handler = ErrorHandler()
llm = MockLLMWithError(callbacks=[error_handler])

try:
    llm.invoke("è§¦å‘é”™è¯¯")
except:
    pass

print(f"è®°å½•çš„é”™è¯¯: {error_handler.errors}")

# ===== 10. å®é™…åº”ç”¨ï¼šToken ç»Ÿè®¡ =====
print("\n=== 10. Token ç»Ÿè®¡åº”ç”¨ ===")

class TokenCounter(BaseCallbackHandler):
    """Token è®¡æ•°å™¨"""

    def __init__(self):
        self.prompt_tokens = 0
        self.completion_tokens = 0
        self.total_tokens = 0
        self.call_count = 0

    def on_llm_start(self, serialized: Dict, prompts: List[str], **kwargs) -> None:
        # ä¼°ç®— prompt tokens
        tokens = sum(len(p) // 4 for p in prompts)
        self.prompt_tokens += tokens

    def on_llm_end(self, response: LLMResult, **kwargs) -> None:
        self.completion_tokens += response.token_count
        self.total_tokens = self.prompt_tokens + self.completion_tokens
        self.call_count += 1

    def report(self):
        print(f"=== Token ä½¿ç”¨æŠ¥å‘Š ===")
        print(f"è°ƒç”¨æ¬¡æ•°: {self.call_count}")
        print(f"Prompt Tokens: {self.prompt_tokens}")
        print(f"Completion Tokens: {self.completion_tokens}")
        print(f"Total Tokens: {self.total_tokens}")
        print(f"é¢„ä¼°æˆæœ¬: ${self.total_tokens * 0.002 / 1000:.4f}")

# æµ‹è¯•
counter = TokenCounter()
llm = MockLLM(callbacks=[counter], streaming=False)

# å¤šæ¬¡è°ƒç”¨
for i in range(3):
    llm.invoke(f"é—®é¢˜ {i+1}: è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•é—®é¢˜")

# æ‰“å°æŠ¥å‘Š
counter.report()

print("\n=== å®Œæˆï¼===")
```

**è¿è¡Œè¾“å‡ºç¤ºä¾‹ï¼š**
```
=== 1. åŸºç¡€ç»“æ„ ===

=== 2. BaseCallbackHandler ===

=== 3. è‡ªå®šä¹‰ Handler ===

=== 4. CallbackManager ===

=== 5. æ¨¡æ‹Ÿ LLM ===

æµ‹è¯•æµå¼è¾“å‡º:
[Monitor] LLM å¼€å§‹ï¼Œ1 ä¸ª prompt
è¿™æ˜¯å¯¹ã€Œä½ å¥½ã€çš„å›ç­”ã€‚
[Monitor] LLM ç»“æŸï¼Œè€—æ—¶ 0.75s
å®Œæ•´å“åº”: è¿™æ˜¯å¯¹ã€Œä½ å¥½ã€çš„å›ç­”ã€‚

=== 6. å¤š Handler ç»„åˆ ===

ç»„åˆä½¿ç”¨å¤šä¸ª Handler:
[Monitor] LLM å¼€å§‹ï¼Œ1 ä¸ª prompt
è¿™æ˜¯å¯¹ã€Œå†™ä¸€é¦–è¯—ã€çš„å›ç­”ã€‚
[Monitor] LLM ç»“æŸï¼Œè€—æ—¶ 0.85s
[Cost] æœ¬æ¬¡: 26 tokens ($0.0001), ç´¯è®¡: 26 tokens ($0.0001)

äº‹ä»¶è®°å½•: 2 ä¸ªäº‹ä»¶
  - llm_start: {'prompt_count': 1}
  - llm_end: {'duration': 0.85}

=== 10. Token ç»Ÿè®¡åº”ç”¨ ===
=== Token ä½¿ç”¨æŠ¥å‘Š ===
è°ƒç”¨æ¬¡æ•°: 3
Prompt Tokens: 21
Completion Tokens: 138
Total Tokens: 159
é¢„ä¼°æˆæœ¬: $0.0003

=== å®Œæˆï¼===
```

---

## 8. ã€é¢è¯•å¿…é—®ã€‘

### é—®é¢˜1ï¼š"LangChain ä¸­å¦‚ä½•å®ç°æµå¼è¾“å‡ºï¼Ÿ"

**æ™®é€šå›ç­”ï¼ˆâŒ ä¸å‡ºå½©ï¼‰ï¼š**
"ä½¿ç”¨ Callback çš„ on_llm_new_token æ–¹æ³•ã€‚"

**å‡ºå½©å›ç­”ï¼ˆâœ… æ¨èï¼‰ï¼š**

> **LangChain æµå¼è¾“å‡ºæœ‰ä¸‰ç§æ–¹å¼ï¼š**
>
> **1. Callback æ–¹å¼**
> ```python
> class StreamHandler(BaseCallbackHandler):
>     def on_llm_new_token(self, token, **kwargs):
>         print(token, end="")
>
> llm = ChatOpenAI(streaming=True, callbacks=[StreamHandler()])
> ```
>
> **2. astream æ–¹å¼**
> ```python
> async for chunk in llm.astream("ä½ å¥½"):
>     print(chunk.content, end="")
> ```
>
> **3. astream_events æ–¹å¼ï¼ˆæœ€è¯¦ç»†ï¼‰**
> ```python
> async for event in chain.astream_events(input, version="v2"):
>     if event["event"] == "on_llm_stream":
>         print(event["data"]["chunk"].content, end="")
> ```
>
> **å…³é”®é…ç½®ï¼š**
> - LLM éœ€è¦ `streaming=True`
> - Callback éœ€è¦å®ç° `on_llm_new_token`
>
> **å®é™…åº”ç”¨**ï¼šåœ¨ Web åº”ç”¨ä¸­ï¼Œæˆ‘ç”¨ WebSocket é…åˆ Callback å®ç°å®æ—¶æ˜¾ç¤ºã€‚ç”¨æˆ·æé—®åç«‹å³çœ‹åˆ° AI é€å­—å›ç­”ï¼Œä½“éªŒå¤§å¹…æå‡ã€‚

**ä¸ºä»€ä¹ˆè¿™ä¸ªå›ç­”å‡ºå½©ï¼Ÿ**
1. âœ… ä¸‰ç§æ–¹å¼å¯¹æ¯”
2. âœ… æœ‰ä»£ç ç¤ºä¾‹
3. âœ… æåˆ°å…³é”®é…ç½®
4. âœ… æœ‰å®é™…åº”ç”¨åœºæ™¯

---

### é—®é¢˜2ï¼š"å¦‚ä½•ç›‘æ§ LangChain åº”ç”¨çš„æ‰§è¡Œè¿‡ç¨‹ï¼Ÿ"

**å‡ºå½©å›ç­”ï¼ˆâœ… æ¨èï¼‰ï¼š**

> **ç›‘æ§ä¸»è¦é€šè¿‡ Callback ç³»ç»Ÿå®ç°ï¼š**
>
> **1. æ‰§è¡Œè¿½è¸ª**
> ```python
> class TraceHandler(BaseCallbackHandler):
>     def on_chain_start(self, serialized, inputs, **kwargs):
>         log.info(f"Chain å¼€å§‹: {inputs}")
>     def on_llm_start(self, serialized, prompts, **kwargs):
>         log.info(f"LLM è°ƒç”¨: {prompts}")
> ```
>
> **2. æ€§èƒ½ç›‘æ§**
> ```python
> def on_llm_start(...):
>     self.start_time = time.time()
> def on_llm_end(...):
>     duration = time.time() - self.start_time
>     metrics.record("llm_latency", duration)
> ```
>
> **3. æˆæœ¬ç»Ÿè®¡**
> ```python
> def on_llm_end(self, response, **kwargs):
>     tokens = response.llm_output.get("token_usage", {})
>     self.total_cost += tokens.get("total_tokens", 0) * price
> ```
>
> **4. é›†æˆ LangSmith**
> ```python
> # å®˜æ–¹è¿½è¸ªå¹³å°
> os.environ["LANGCHAIN_TRACING_V2"] = "true"
> os.environ["LANGCHAIN_API_KEY"] = "..."
> ```

---

## 9. ã€åŒ–éª¨ç»µæŒã€‘

### å¡ç‰‡1ï¼šCallback æ˜¯ä»€ä¹ˆï¼Ÿ ğŸ¯

**ä¸€å¥è¯ï¼š** Callback æ˜¯åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­è§¦å‘çš„é’©å­å‡½æ•°ï¼Œç”¨äºç›‘æ§å’Œæ‰©å±•ã€‚

**ä¸¾ä¾‹ï¼š**
```python
def on_llm_start(...):
    print("å¼€å§‹")  # LLM å¼€å§‹æ—¶è§¦å‘
```

**åº”ç”¨ï¼š** æ—¥å¿—ã€ç›‘æ§ã€æµå¼è¾“å‡ºã€‚

---

### å¡ç‰‡2ï¼šBaseCallbackHandler åŸºç±» ğŸ“¡

**ä¸€å¥è¯ï¼š** å®šä¹‰æ‰€æœ‰å¯ç›‘å¬äº‹ä»¶çš„å¤„ç†å™¨åŸºç±»ã€‚

**ä¸¾ä¾‹ï¼š**
```python
class MyHandler(BaseCallbackHandler):
    def on_llm_start(self, ...): pass
    def on_llm_end(self, ...): pass
```

**åº”ç”¨ï¼š** ç»§æ‰¿å¹¶å®ç°éœ€è¦çš„æ–¹æ³•ã€‚

---

### å¡ç‰‡3ï¼šæµå¼è¾“å‡º on_llm_new_token ğŸŒŠ

**ä¸€å¥è¯ï¼š** æ¯ç”Ÿæˆä¸€ä¸ª token å°±è§¦å‘ï¼Œå®ç°å®æ—¶æ˜¾ç¤ºã€‚

**ä¸¾ä¾‹ï¼š**
```python
def on_llm_new_token(self, token, **kwargs):
    print(token, end="", flush=True)
```

**åº”ç”¨ï¼š** æå‡ç”¨æˆ·ä½“éªŒçš„æ ¸å¿ƒåŠŸèƒ½ã€‚

---

### å¡ç‰‡4ï¼šCallbackManager ç®¡ç†å™¨ ğŸ›ï¸

**ä¸€å¥è¯ï¼š** ç®¡ç†å¤šä¸ª Handlerï¼Œè´Ÿè´£äº‹ä»¶çš„å¹¿æ’­åˆ†å‘ã€‚

**ä¸¾ä¾‹ï¼š**
```python
manager = CallbackManager([handler1, handler2])
# äº‹ä»¶ä¼šé€šçŸ¥æ‰€æœ‰ handler
```

**åº”ç”¨ï¼š** åŒæ—¶ä½¿ç”¨å¤šä¸ª Handlerã€‚

---

### å¡ç‰‡5ï¼šä¸¤ç§ä¼ é€’ Callback çš„æ–¹å¼ ğŸ“¤

**ä¸€å¥è¯ï¼š** æ„é€ æ—¶ä¼ é€’ï¼ˆå…¨å±€ï¼‰æˆ–è°ƒç”¨æ—¶ä¼ é€’ï¼ˆå•æ¬¡ï¼‰ã€‚

**ä¸¾ä¾‹ï¼š**
```python
# æ„é€ æ—¶
llm = ChatOpenAI(callbacks=[handler])

# è°ƒç”¨æ—¶
result = chain.invoke(input, config={"callbacks": [handler]})
```

**åº”ç”¨ï¼š** çµæ´»æ§åˆ¶ Callback èŒƒå›´ã€‚

---

### å¡ç‰‡6ï¼šastream å¼‚æ­¥æµå¼ ğŸ”„

**ä¸€å¥è¯ï¼š** ä½¿ç”¨ async for ç›´æ¥è·å–æµå¼è¾“å‡ºã€‚

**ä¸¾ä¾‹ï¼š**
```python
async for chunk in llm.astream("ä½ å¥½"):
    print(chunk.content)
```

**åº”ç”¨ï¼š** æœ€ç®€å•çš„æµå¼æ–¹å¼ã€‚

---

### å¡ç‰‡7ï¼šRunnableConfig é…ç½®ä¼ é€’ âš™ï¸

**ä¸€å¥è¯ï¼š** é€šè¿‡ config ä¼ é€’ callbacksã€tagsã€metadataã€‚

**ä¸¾ä¾‹ï¼š**
```python
config = {"callbacks": [handler], "tags": ["test"]}
result = chain.invoke(input, config=config)
```

**åº”ç”¨ï¼š** ç»Ÿä¸€çš„é…ç½®ä¼ é€’æœºåˆ¶ã€‚

---

### å¡ç‰‡8ï¼šäº‹ä»¶ç±»å‹é€ŸæŸ¥ ğŸ“‹

**ä¸€å¥è¯ï¼š** LLM/Chain/Tool/Agent å„æœ‰ start/end/error äº‹ä»¶ã€‚

**ä¸¾ä¾‹ï¼š**
```
on_llm_start, on_llm_end, on_llm_error
on_chain_start, on_chain_end
on_tool_start, on_tool_end
on_agent_action, on_agent_finish
```

**åº”ç”¨ï¼š** æ ¹æ®éœ€æ±‚é€‰æ‹©ç›‘å¬çš„äº‹ä»¶ã€‚

---

### å¡ç‰‡9ï¼šCallback ä¸é˜»å¡ä¸»æµç¨‹ âš¡

**ä¸€å¥è¯ï¼š** Callback æ˜¯æ—è·¯å¤„ç†ï¼Œä¸å½±å“ä¸»æµç¨‹çš„è¿”å›å€¼ã€‚

**ä¸¾ä¾‹ï¼š**
```python
# å³ä½¿ Callback å¾ˆæ…¢æˆ–å‡ºé”™
# ä¸»æµç¨‹ä¾ç„¶æ­£å¸¸è¿”å›ç»“æœ
```

**åº”ç”¨ï¼š** å®‰å…¨åœ°æ·»åŠ ç›‘æ§é€»è¾‘ã€‚

---

### å¡ç‰‡10ï¼šCallback åœ¨ LangChain æºç ä¸­çš„ä½ç½® â­

**ä¸€å¥è¯ï¼š** æ‰€æœ‰ Runnable ç»„ä»¶éƒ½æ”¯æŒ Callbackï¼Œé€šè¿‡ config ä¼ é€’ã€‚

**ä¸¾ä¾‹ï¼š**
```python
# langchain_core/runnables/base.py
class Runnable:
    def invoke(self, input, config=None):
        # config ä¸­åŒ…å« callbacks
```

**åº”ç”¨ï¼š** ç†è§£ Callback ä¸ Runnable çš„é›†æˆã€‚

---

## 10. ã€ä¸€å¥è¯æ€»ç»“ã€‘

**Callback æ˜¯ LangChain çš„äº‹ä»¶ç›‘å¬æœºåˆ¶ï¼Œé€šè¿‡åœ¨ LLM/Chain/Tool æ‰§è¡Œè¿‡ç¨‹ä¸­è§¦å‘é’©å­å‡½æ•°ï¼Œå®ç°æµå¼è¾“å‡ºã€æ—¥å¿—è®°å½•ã€æ€§èƒ½ç›‘æ§å’Œæˆæœ¬ç»Ÿè®¡ç­‰åŠŸèƒ½ï¼Œæ˜¯æ„å»ºå¯è§‚æµ‹ LLM åº”ç”¨çš„å…³é”®ç»„ä»¶ã€‚**

---

## ğŸ“š å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£ Callback çš„äº‹ä»¶é©±åŠ¨æœºåˆ¶
- [ ] ä¼šå®ç°è‡ªå®šä¹‰ CallbackHandler
- [ ] æŒæ¡æµå¼è¾“å‡ºçš„å®ç°æ–¹å¼
- [ ] äº†è§£ CallbackManager çš„ä½œç”¨
- [ ] çŸ¥é“ä¸¤ç§ä¼ é€’ Callback çš„æ–¹å¼
- [ ] èƒ½å¤Ÿç”¨ Callback å®ç°ç›‘æ§å’Œæ—¥å¿—

## ğŸ”— ä¸‹ä¸€æ­¥å­¦ä¹ 

- **LangSmith**ï¼šå®˜æ–¹çš„è¿½è¸ªå’Œç›‘æ§å¹³å°
- **Runnable åè®®**ï¼šæ·±å…¥ç†è§£ Callback ä¸ Runnable çš„é›†æˆ
- **ç”Ÿäº§éƒ¨ç½²**ï¼šå¦‚ä½•åœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ Callback

---

**ç‰ˆæœ¬ï¼š** v1.0
**æœ€åæ›´æ–°ï¼š** 2025-01-14
