# Retriever æ£€ç´¢å™¨

> åŸå­åŒ–çŸ¥è¯†ç‚¹ | LangChain ä½¿ç”¨ | LangChain æºç å­¦ä¹ æ ¸å¿ƒçŸ¥è¯†

---

## 1. ã€30å­—æ ¸å¿ƒã€‘

**Retriever æ˜¯ RAG æ¨¡å¼çš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„æ–‡æ¡£ï¼ŒVectorStoreRetriever æ˜¯æœ€å¸¸ç”¨çš„å®ç°ã€‚**

---

## 2. ã€ç¬¬ä¸€æ€§åŸç†ã€‘

### ä»€ä¹ˆæ˜¯ç¬¬ä¸€æ€§åŸç†ï¼Ÿ

**ç¬¬ä¸€æ€§åŸç†**ï¼šå›åˆ°äº‹ç‰©æœ€åŸºæœ¬çš„çœŸç†ï¼Œä»æºå¤´æ€è€ƒé—®é¢˜

### Retriever æ£€ç´¢å™¨çš„ç¬¬ä¸€æ€§åŸç† ğŸ¯

#### 1. æœ€åŸºç¡€çš„å®šä¹‰

**Retriever = æ ¹æ®æŸ¥è¯¢æ‰¾åˆ°ç›¸å…³æ–‡æ¡£çš„ç»„ä»¶**

ä»…æ­¤è€Œå·²ï¼æ²¡æœ‰æ›´åŸºç¡€çš„äº†ã€‚

```python
# Retriever çš„æœ¬è´¨
def retrieve(query: str) -> List[Document]:
    # è¾“å…¥ï¼šç”¨æˆ·é—®é¢˜
    # è¾“å‡ºï¼šç›¸å…³æ–‡æ¡£åˆ—è¡¨
    relevant_docs = search_knowledge_base(query)
    return relevant_docs
```

#### 2. ä¸ºä»€ä¹ˆéœ€è¦ Retrieverï¼Ÿ

**æ ¸å¿ƒé—®é¢˜ï¼šLLM çš„çŸ¥è¯†æœ‰å±€é™**

```python
# LLM çš„å±€é™æ€§
# âŒ çŸ¥è¯†æˆªæ­¢æ—¥æœŸï¼ˆ2023å¹´ä¹‹å‰ï¼‰
# âŒ ä¸çŸ¥é“ä½ çš„ç§æœ‰æ•°æ®ï¼ˆå…¬å¸æ–‡æ¡£ã€ä¸ªäººç¬”è®°ï¼‰
# âŒ ä¸çŸ¥é“å®æ—¶ä¿¡æ¯ï¼ˆä»Šå¤©çš„æ–°é—»ï¼‰

# Retriever çš„è§£å†³æ–¹æ¡ˆ
# âœ… ä»ä½ çš„çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³ä¿¡æ¯
# âœ… æŠŠä¿¡æ¯ä½œä¸ºä¸Šä¸‹æ–‡ä¼ ç»™ LLM
# âœ… LLM åŸºäºè¿™äº›ä¿¡æ¯å›ç­”

user_question = "æˆ‘ä»¬å…¬å¸çš„é€€æ¬¾æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ"
relevant_docs = retriever.invoke(user_question)
# [Document(content="é€€æ¬¾æ”¿ç­–ï¼š30å¤©å†…å¯é€€...")]

# LLM çœ‹åˆ°æ–‡æ¡£åå¯ä»¥å‡†ç¡®å›ç­”
```

#### 3. Retriever çš„ä¸‰å±‚ä»·å€¼

##### ä»·å€¼1ï¼šæ‰©å±•çŸ¥è¯†èŒƒå›´

```python
# è®© LLM èƒ½å›ç­”å®ƒæœ¬æ¥ä¸çŸ¥é“çš„é—®é¢˜
knowledge_base = [
    "å…¬å¸æˆç«‹äº2020å¹´",
    "é€€æ¬¾æ”¿ç­–ï¼š30å¤©å†…å…¨é¢é€€æ¬¾",
    "æŠ€æœ¯æ¶æ„ä½¿ç”¨å¾®æœåŠ¡",
    ...  # å…¬å¸å†…éƒ¨æ–‡æ¡£
]

# ç”¨æˆ·é—®ï¼šå…¬å¸ä»€ä¹ˆæ—¶å€™æˆç«‹çš„ï¼Ÿ
# LLM æœ¬èº«ä¸çŸ¥é“ï¼Œä½† Retriever æ‰¾åˆ°æ–‡æ¡£åå°±èƒ½å›ç­”
```

##### ä»·å€¼2ï¼šæä¾›å‡†ç¡®ä¿¡æ¯

```python
# é¿å… LLM "å¹»è§‰"ï¼ˆç¼–é€ ä¿¡æ¯ï¼‰

# âŒ æ²¡æœ‰ Retriever
# ç”¨æˆ·ï¼šæˆ‘ä»¬çš„APIé™æµæ˜¯å¤šå°‘ï¼Ÿ
# LLMï¼šï¼ˆå¯èƒ½ç¼–é€ ï¼‰å¤§æ¦‚æ˜¯æ¯åˆ†é’Ÿ100æ¬¡è¯·æ±‚å§...

# âœ… æœ‰ Retriever
# Retriever æ‰¾åˆ°ï¼šAPIé™æµæ–‡æ¡£è¯´æ˜...
# LLMï¼šæ ¹æ®æ–‡æ¡£ï¼ŒAPIé™æµæ˜¯æ¯åˆ†é’Ÿ1000æ¬¡è¯·æ±‚ã€‚
```

##### ä»·å€¼3ï¼šå®ç°é—®ç­”ç³»ç»Ÿ

```python
# RAG = Retrieval Augmented Generation
# æ£€ç´¢å¢å¼ºç”Ÿæˆ

rag_pipeline = (
    æ£€ç´¢ç›¸å…³æ–‡æ¡£
    â†’ å°†æ–‡æ¡£ä½œä¸ºä¸Šä¸‹æ–‡
    â†’ LLM ç”Ÿæˆå›ç­”
)
```

#### 4. ä»ç¬¬ä¸€æ€§åŸç†æ¨å¯¼ RAG

**æ¨ç†é“¾ï¼š**

```
1. LLM çŸ¥è¯†æœ‰é™
   â†“
2. éœ€è¦è®¿é—®å¤–éƒ¨çŸ¥è¯†
   â†“
3. å¦‚ä½•æ‰¾åˆ°ç›¸å…³çš„çŸ¥è¯†ï¼Ÿ
   â†“
4. éœ€è¦æ£€ç´¢æœºåˆ¶
   â†“
5. æ–‡æœ¬å¦‚ä½•æ¯”è¾ƒç›¸ä¼¼åº¦ï¼Ÿ
   â†“
6. ä½¿ç”¨ Embedding å‘é‡åŒ–
   â†“
7. å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢
   â†“
8. è¿™å°±æ˜¯ VectorStoreRetriever
```

#### 5. ä¸€å¥è¯æ€»ç»“ç¬¬ä¸€æ€§åŸç†

**Retriever æ˜¯è¿æ¥ LLM å’Œå¤–éƒ¨çŸ¥è¯†çš„æ¡¥æ¢ï¼Œé€šè¿‡æ£€ç´¢ç›¸å…³æ–‡æ¡£ä¸º LLM æä¾›ä¸Šä¸‹æ–‡ï¼Œå®ç°åŸºäºçŸ¥è¯†åº“çš„ç²¾å‡†é—®ç­”ã€‚**

---

## 3. ã€æ ¸å¿ƒæ¦‚å¿µï¼ˆå…¨é¢è¦†ç›–ï¼‰ã€‘

### æ ¸å¿ƒæ¦‚å¿µ1ï¼šBaseRetriever æ¥å£ ğŸ”

**BaseRetriever æ˜¯æ‰€æœ‰æ£€ç´¢å™¨çš„æŠ½è±¡åŸºç±»ï¼Œå®šä¹‰äº†ç»Ÿä¸€çš„æ£€ç´¢æ¥å£**

```python
from langchain_core.retrievers import BaseRetriever
from langchain_core.documents import Document
from typing import List

# BaseRetriever çš„æ ¸å¿ƒæ¥å£
class BaseRetriever:
    """æ£€ç´¢å™¨åŸºç±»"""

    def invoke(self, input: str) -> List[Document]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆæ¨èä½¿ç”¨ï¼‰"""
        return self._get_relevant_documents(input)

    def _get_relevant_documents(self, query: str) -> List[Document]:
        """æŠ½è±¡æ–¹æ³•ï¼šå­ç±»å¿…é¡»å®ç°"""
        raise NotImplementedError

    async def ainvoke(self, input: str) -> List[Document]:
        """å¼‚æ­¥æ£€ç´¢"""
        return await self._aget_relevant_documents(input)
```

**è‡ªå®šä¹‰ Retriever ç¤ºä¾‹ï¼š**

```python
from langchain_core.retrievers import BaseRetriever
from langchain_core.documents import Document
from typing import List

class KeywordRetriever(BaseRetriever):
    """åŸºäºå…³é”®è¯çš„ç®€å•æ£€ç´¢å™¨"""

    documents: List[Document]

    def _get_relevant_documents(self, query: str) -> List[Document]:
        """æ ¹æ®å…³é”®è¯åŒ¹é…æ£€ç´¢"""
        results = []
        query_words = query.lower().split()

        for doc in self.documents:
            content_lower = doc.page_content.lower()
            if any(word in content_lower for word in query_words):
                results.append(doc)

        return results[:4]  # è¿”å›å‰4ä¸ª

# ä½¿ç”¨
docs = [
    Document(page_content="Python æ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€"),
    Document(page_content="LangChain æ˜¯ AI æ¡†æ¶"),
    Document(page_content="æœºå™¨å­¦ä¹ éœ€è¦æ•°æ®"),
]

retriever = KeywordRetriever(documents=docs)
results = retriever.invoke("Python ç¼–ç¨‹")
# [Document(page_content="Python æ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€")]
```

**åœ¨ LangChain æºç ä¸­çš„åº”ç”¨ï¼š**

```python
# langchain_core/retrievers.py
class BaseRetriever(RunnableSerializable, ABC):
    """æ£€ç´¢å™¨åŸºç±»ï¼ŒåŒæ—¶æ˜¯ Runnable"""

    class Config:
        arbitrary_types_allowed = True

    @abstractmethod
    def _get_relevant_documents(
        self, query: str, *, run_manager: CallbackManagerForRetrieverRun
    ) -> List[Document]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""

    # Runnable æ¥å£å®ç°
    def invoke(
        self, input: str, config: Optional[RunnableConfig] = None
    ) -> List[Document]:
        return self._get_relevant_documents(input)
```

---

### æ ¸å¿ƒæ¦‚å¿µ2ï¼šVectorStoreRetriever å‘é‡æ£€ç´¢ ğŸ“Š

**VectorStoreRetriever åŸºäºå‘é‡ç›¸ä¼¼åº¦æ£€ç´¢ï¼Œæ˜¯æœ€å¸¸ç”¨çš„æ£€ç´¢å™¨**

```python
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document

# 1. å‡†å¤‡æ–‡æ¡£
documents = [
    Document(page_content="Python æ˜¯ä¸€ç§è§£é‡Šå‹ç¼–ç¨‹è¯­è¨€", metadata={"source": "python.md"}),
    Document(page_content="LangChain æ˜¯æ„å»º LLM åº”ç”¨çš„æ¡†æ¶", metadata={"source": "langchain.md"}),
    Document(page_content="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯", metadata={"source": "ml.md"}),
]

# 2. åˆ›å»º Embedding æ¨¡å‹
embeddings = OpenAIEmbeddings()

# 3. åˆ›å»º VectorStore
vectorstore = FAISS.from_documents(documents, embeddings)

# 4. åˆ›å»º Retriever
retriever = vectorstore.as_retriever(
    search_type="similarity",  # ç›¸ä¼¼åº¦æœç´¢
    search_kwargs={"k": 4}     # è¿”å› 4 ä¸ªç»“æœ
)

# 5. æ£€ç´¢
results = retriever.invoke("ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ")
for doc in results:
    print(f"å†…å®¹: {doc.page_content}")
    print(f"æ¥æº: {doc.metadata.get('source')}")
```

**search_type é€‰é¡¹ï¼š**

| ç±»å‹ | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|-----|------|---------|
| `similarity` | çº¯ç›¸ä¼¼åº¦æ’åº | é€šç”¨åœºæ™¯ |
| `mmr` | æœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼ˆå¤šæ ·æ€§ï¼‰ | é¿å…ç»“æœå¤ªç›¸ä¼¼ |
| `similarity_score_threshold` | å¸¦é˜ˆå€¼è¿‡æ»¤ | è´¨é‡è¦æ±‚é«˜ |

```python
# MMR æ£€ç´¢ï¼šå¹³è¡¡ç›¸å…³æ€§å’Œå¤šæ ·æ€§
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 4,
        "fetch_k": 20,       # å…ˆå– 20 ä¸ª
        "lambda_mult": 0.5   # å¤šæ ·æ€§æƒé‡
    }
)

# å¸¦é˜ˆå€¼çš„æ£€ç´¢
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={
        "score_threshold": 0.8,  # åªè¿”å›ç›¸ä¼¼åº¦ > 0.8 çš„
        "k": 4
    }
)
```

---

### æ ¸å¿ƒæ¦‚å¿µ3ï¼šRAG Pattern æ£€ç´¢å¢å¼ºç”Ÿæˆ ğŸ”—

**RAG (Retrieval Augmented Generation) æ˜¯æ£€ç´¢ + ç”Ÿæˆçš„å®Œæ•´æµç¨‹**

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI

# RAG Chain æ¨¡æ¿
template = """åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´"æˆ‘ä¸çŸ¥é“"ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š"""

prompt = ChatPromptTemplate.from_template(template)
llm = ChatOpenAI()

# æ„å»º RAG Chain
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# ä½¿ç”¨
answer = rag_chain.invoke("LangChain æ˜¯ä»€ä¹ˆï¼Ÿ")
print(answer)
```

**RAG æµç¨‹å›¾ï¼š**

```
ç”¨æˆ·é—®é¢˜: "LangChain æ˜¯ä»€ä¹ˆï¼Ÿ"
         â†“
    [Retriever æ£€ç´¢]
         â†“
    ç›¸å…³æ–‡æ¡£: ["LangChain æ˜¯æ„å»º LLM åº”ç”¨çš„æ¡†æ¶..."]
         â†“
    [æ„å»º Prompt]
    "ä¸Šä¸‹æ–‡ï¼šLangChain æ˜¯...
     é—®é¢˜ï¼šLangChain æ˜¯ä»€ä¹ˆï¼Ÿ"
         â†“
    [LLM ç”Ÿæˆ]
         â†“
    å›ç­”: "LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»º LLM åº”ç”¨çš„æ¡†æ¶..."
```

---

### æ ¸å¿ƒæ¦‚å¿µ4ï¼šDocument æ–‡æ¡£å¯¹è±¡ ğŸ“„

**Document æ˜¯ LangChain ä¸­è¡¨ç¤ºæ–‡æ¡£çš„æ ‡å‡†æ•°æ®ç»“æ„**

```python
from langchain_core.documents import Document

# åˆ›å»ºæ–‡æ¡£
doc = Document(
    page_content="è¿™æ˜¯æ–‡æ¡£çš„å†…å®¹",
    metadata={
        "source": "docs/readme.md",
        "page": 1,
        "author": "å¼ ä¸‰",
        "date": "2024-01-01"
    }
)

# è®¿é—®å±æ€§
print(doc.page_content)  # å†…å®¹
print(doc.metadata)      # å…ƒæ•°æ®

# æ‰¹é‡åˆ›å»º
docs = [
    Document(page_content="æ–‡æ¡£1", metadata={"id": 1}),
    Document(page_content="æ–‡æ¡£2", metadata={"id": 2}),
]
```

**å…ƒæ•°æ®çš„ç”¨é€”ï¼š**

```python
# 1. è¿‡æ»¤æ£€ç´¢ç»“æœ
results = retriever.invoke("æŸ¥è¯¢", filter={"source": "official"})

# 2. è¿½è¸ªæ¥æº
for doc in results:
    print(f"æ¥æº: {doc.metadata.get('source')}")

# 3. æ„å»ºå¼•ç”¨
answer = f"{response}\n\nå‚è€ƒæ¥æºï¼š{doc.metadata.get('source')}"
```

---

### æ‰©å±•æ¦‚å¿µ5ï¼šEnsembleRetriever å¤šè·¯å¬å› ğŸ”€

**EnsembleRetriever ç»„åˆå¤šä¸ªæ£€ç´¢å™¨ï¼Œèåˆä¸åŒæ£€ç´¢ç­–ç•¥**

```python
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever

# åˆ›å»ºä¸åŒç±»å‹çš„æ£€ç´¢å™¨
# 1. å‘é‡æ£€ç´¢ï¼ˆè¯­ä¹‰ç›¸ä¼¼ï¼‰
vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

# 2. BM25 æ£€ç´¢ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰
bm25_retriever = BM25Retriever.from_documents(documents)
bm25_retriever.k = 4

# 3. ç»„åˆæ£€ç´¢å™¨
ensemble_retriever = EnsembleRetriever(
    retrievers=[vector_retriever, bm25_retriever],
    weights=[0.7, 0.3]  # å‘é‡æ£€ç´¢æƒé‡æ›´é«˜
)

# ä½¿ç”¨
results = ensemble_retriever.invoke("Python ç¼–ç¨‹è¯­è¨€")
# èåˆä¸¤ç§æ£€ç´¢çš„ç»“æœ
```

**å¤šè·¯å¬å›çš„ä¼˜åŠ¿ï¼š**

| æ£€ç´¢æ–¹å¼ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|---------|------|------|
| å‘é‡æ£€ç´¢ | è¯­ä¹‰ç†è§£å¥½ | å¯èƒ½å¿½ç•¥å…³é”®è¯ |
| BM25 | å…³é”®è¯ç²¾ç¡® | ç¼ºä¹è¯­ä¹‰ç†è§£ |
| Ensemble | ä¸¤è€…ç»“åˆ | è®¡ç®—æˆæœ¬é«˜ |

---

### æ‰©å±•æ¦‚å¿µ6ï¼šæ–‡æ¡£åŠ è½½ä¸åˆ†å‰² ğŸ“š

**å®Œæ•´çš„ RAG æµç¨‹åŒ…æ‹¬æ–‡æ¡£åŠ è½½å’Œåˆ†å‰²**

```python
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 1. åŠ è½½æ–‡æ¡£
loader = TextLoader("docs/readme.md")
documents = loader.load()

# æˆ–è€…åŠ è½½ PDF
# loader = PyPDFLoader("docs/manual.pdf")
# documents = loader.load()

# 2. åˆ†å‰²æ–‡æ¡£ï¼ˆå› ä¸ºæ–‡æ¡£å¯èƒ½å¤ªé•¿ï¼‰
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,    # æ¯å— 1000 å­—ç¬¦
    chunk_overlap=200,  # é‡å  200 å­—ç¬¦
    separators=["\n\n", "\n", "ã€‚", "ï¼Œ", " ", ""]
)

splits = text_splitter.split_documents(documents)
print(f"åŸå§‹ {len(documents)} ä¸ªæ–‡æ¡£ï¼Œåˆ†å‰²ä¸º {len(splits)} ä¸ªå—")

# 3. åˆ›å»ºå‘é‡å­˜å‚¨
vectorstore = FAISS.from_documents(splits, embeddings)

# 4. åˆ›å»ºæ£€ç´¢å™¨
retriever = vectorstore.as_retriever()
```

---

## 4. ã€æœ€å°å¯ç”¨ã€‘

æŒæ¡ä»¥ä¸‹å†…å®¹ï¼Œå°±èƒ½åœ¨ LangChain ä¸­ä½¿ç”¨ Retrieverï¼š

### 4.1 åˆ›å»ºå‘é‡æ£€ç´¢å™¨

```python
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document

# å‡†å¤‡æ–‡æ¡£
docs = [
    Document(page_content="Python æ˜¯ç¼–ç¨‹è¯­è¨€"),
    Document(page_content="LangChain æ˜¯ AI æ¡†æ¶"),
]

# åˆ›å»ºæ£€ç´¢å™¨
vectorstore = FAISS.from_documents(docs, OpenAIEmbeddings())
retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

# æ£€ç´¢
results = retriever.invoke("ä»€ä¹ˆæ˜¯ Pythonï¼Ÿ")
```

### 4.2 æ„å»º RAG Chain

```python
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate

template = """æ ¹æ®ä¸Šä¸‹æ–‡å›ç­”ï¼š
{context}

é—®é¢˜ï¼š{question}"""

prompt = ChatPromptTemplate.from_template(template)

rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

answer = rag_chain.invoke("LangChain æ˜¯ä»€ä¹ˆï¼Ÿ")
```

### 4.3 æ–‡æ¡£åŠ è½½ä¸åˆ†å‰²

```python
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# åŠ è½½
docs = TextLoader("readme.md").load()

# åˆ†å‰²
splitter = RecursiveCharacterTextSplitter(chunk_size=1000)
splits = splitter.split_documents(docs)
```

### 4.4 è‡ªå®šä¹‰æ£€ç´¢å™¨

```python
from langchain_core.retrievers import BaseRetriever

class MyRetriever(BaseRetriever):
    def _get_relevant_documents(self, query: str):
        # è‡ªå®šä¹‰æ£€ç´¢é€»è¾‘
        return search_my_database(query)
```

**è¿™äº›çŸ¥è¯†è¶³ä»¥ï¼š**
- æ„å»ºåŸºæœ¬çš„ RAG åº”ç”¨
- ä»æ–‡ä»¶åˆ›å»ºçŸ¥è¯†åº“
- å®ç°é—®ç­”ç³»ç»Ÿ
- è‡ªå®šä¹‰æ£€ç´¢é€»è¾‘

---

## 5. ã€1ä¸ªç±»æ¯”ã€‘ï¼ˆåŒè½¨åˆ¶ï¼‰

### ç±»æ¯”1ï¼šRetriever æ˜¯å›¾ä¹¦ç®¡ç†å‘˜

#### ğŸ¨ å‰ç«¯è§†è§’ï¼šAPI æ•°æ®è·å– / SWR

Retriever å°±åƒå‰ç«¯çš„æ•°æ®è·å–å±‚ï¼Œæ ¹æ®æŸ¥è¯¢æ¡ä»¶è·å–æ•°æ®ã€‚

```javascript
// SWR / React Query
const { data } = useSWR(
  `/api/search?q=${query}`,
  fetcher
);

// æˆ–è€…è‡ªå®šä¹‰ fetcher
async function searchDocs(query) {
  const response = await fetch(`/api/docs?q=${encodeURIComponent(query)}`);
  return response.json();
}
```

```python
# LangChain Retriever
results = retriever.invoke(query)
# è¿”å›ç›¸å…³æ–‡æ¡£åˆ—è¡¨
```

**å…³é”®ç›¸ä¼¼ç‚¹ï¼š**
- éƒ½æ˜¯æ ¹æ®æŸ¥è¯¢è·å–æ•°æ®
- éƒ½æœ‰ç¼“å­˜/ç´¢å¼•ä¼˜åŒ–
- éƒ½è¿”å›ç»“æ„åŒ–ç»“æœ

#### ğŸ§’ å°æœ‹å‹è§†è§’ï¼šå›¾ä¹¦ç®¡ç†å‘˜

Retriever å°±åƒå›¾ä¹¦é¦†çš„ç®¡ç†å‘˜ï¼š

```
ä½ å»å›¾ä¹¦é¦†ï¼š
"æˆ‘æƒ³æ‰¾å…³äºæé¾™çš„ä¹¦"

å›¾ä¹¦ç®¡ç†å‘˜ï¼ˆRetrieverï¼‰ï¼š
1. å¬æ‡‚ä½ çš„é—®é¢˜
2. åœ¨ä¹¦æ¶ä¸Šæœç´¢
3. æ‰¾åˆ°ç›¸å…³çš„ä¹¦
4. æŠŠä¹¦ç»™ä½ 

"ç»™ä½ ï¼Œè¿™ 3 æœ¬éƒ½æ˜¯è®²æé¾™çš„ï¼"
[æé¾™ç™¾ç§‘] [æé¾™çš„ç§˜å¯†] [æé¾™å¤§å‘ç°]
```

---

### ç±»æ¯”2ï¼šVectorStore æ˜¯æ™ºèƒ½ä¹¦æ¶

#### ğŸ¨ å‰ç«¯è§†è§’ï¼šæœç´¢ç´¢å¼• / Elasticsearch

VectorStore å°±åƒä¸€ä¸ªæ™ºèƒ½æœç´¢ç´¢å¼•ï¼Œæ”¯æŒè¯­ä¹‰æœç´¢ã€‚

```javascript
// Elasticsearch å…¨æ–‡æœç´¢
const results = await client.search({
  index: 'documents',
  body: {
    query: {
      match: {
        content: 'Python ç¼–ç¨‹'
      }
    }
  }
});

// å‘é‡æœç´¢
const results = await client.search({
  body: {
    knn: {
      field: 'embedding',
      query_vector: [0.1, 0.2, ...],
      k: 10
    }
  }
});
```

#### ğŸ§’ å°æœ‹å‹è§†è§’ï¼šæ™ºèƒ½ç©å…·æ”¶çº³ç®±

VectorStore å°±åƒä¸€ä¸ªæ™ºèƒ½æ”¶çº³ç®±ï¼š

```
æ™®é€šæ”¶çº³ç®±ï¼š
æŒ‰é¢œè‰²åˆ†ç±» â†’ çº¢è‰²åŒºã€è“è‰²åŒºã€ç»¿è‰²åŒº
åªèƒ½æ‰¾"çº¢è‰²ç©å…·"

æ™ºèƒ½æ”¶çº³ç®±ï¼ˆVectorStoreï¼‰ï¼š
æŒ‰"æ„Ÿè§‰"åˆ†ç±» â†’
  "å¼€å¿ƒçš„ç©å…·"æ”¾ä¸€èµ·
  "åˆºæ¿€çš„ç©å…·"æ”¾ä¸€èµ·
  "å®‰é™çš„ç©å…·"æ”¾ä¸€èµ·

ä½ è¯´"æˆ‘æƒ³ç©å¼€å¿ƒçš„"
æ™ºèƒ½æ”¶çº³ç®±å°±èƒ½æ‰¾åˆ°æ°”çƒã€ç§¯æœ¨ã€ç©å¶...
ï¼ˆå®ƒä»¬é¢œè‰²ä¸åŒï¼Œä½†éƒ½æ˜¯"å¼€å¿ƒçš„"ï¼‰
```

---

### ç±»æ¯”3ï¼šRAG æ˜¯å…ˆæŸ¥èµ„æ–™å†å›ç­”

#### ğŸ¨ å‰ç«¯è§†è§’ï¼šSSR æ•°æ®é¢„å–

RAG å°±åƒæœåŠ¡ç«¯æ¸²æŸ“æ—¶çš„æ•°æ®é¢„å–ã€‚

```javascript
// Next.js getServerSideProps
export async function getServerSideProps(context) {
  // 1. è·å–æ•°æ®ï¼ˆç›¸å½“äº Retrieverï¼‰
  const docs = await fetchRelevantDocs(context.query.q);

  // 2. ä¼ ç»™é¡µé¢ç»„ä»¶ï¼ˆç›¸å½“äºä¼ ç»™ LLMï¼‰
  return { props: { docs } };
}

// é¡µé¢ç»„ä»¶ä½¿ç”¨æ•°æ®æ¸²æŸ“ï¼ˆç›¸å½“äº LLM ç”Ÿæˆå›ç­”ï¼‰
function Page({ docs }) {
  return <Answer docs={docs} />;
}
```

#### ğŸ§’ å°æœ‹å‹è§†è§’ï¼šè€ƒè¯•æ—¶å¯ä»¥æŸ¥èµ„æ–™

RAG å°±åƒå¼€å·è€ƒè¯•ï¼š

```
é—­å·è€ƒè¯•ï¼ˆæ²¡æœ‰ RAGï¼‰ï¼š
é—®ï¼šæé¾™ä»€ä¹ˆæ—¶å€™ç­ç»çš„ï¼Ÿ
ä½ ï¼šå‘ƒ...å¥½åƒæ˜¯å¾ˆä¹…ä»¥å‰...6500å¹´ï¼Ÿï¼ˆå¯èƒ½ç­”é”™ï¼‰

å¼€å·è€ƒè¯•ï¼ˆæœ‰ RAGï¼‰ï¼š
é—®ï¼šæé¾™ä»€ä¹ˆæ—¶å€™ç­ç»çš„ï¼Ÿ
ä½ ï¼šç­‰ç­‰ï¼Œè®©æˆ‘æŸ¥æŸ¥èµ„æ–™...
   [ç¿»ä¹¦ï¼šæé¾™åœ¨6500ä¸‡å¹´å‰ç­ç»]
ä½ ï¼šæé¾™åœ¨6500ä¸‡å¹´å‰ç­ç»çš„ï¼ï¼ˆå‡†ç¡®ç­”æ¡ˆï¼‰
```

---

### ç±»æ¯”æ€»ç»“è¡¨

| LangChain æ¦‚å¿µ | å‰ç«¯ç±»æ¯” | å°æœ‹å‹ç±»æ¯” |
|---------------|---------|-----------|
| Retriever | API fetcher / SWR | å›¾ä¹¦ç®¡ç†å‘˜ |
| VectorStore | æœç´¢ç´¢å¼• / Elasticsearch | æ™ºèƒ½æ”¶çº³ç®± |
| Embedding | æ•°æ®å‘é‡åŒ– / ç‰¹å¾æå– | ç»™ä¸œè¥¿è´´æ ‡ç­¾ |
| RAG | SSR æ•°æ®é¢„å– | å¼€å·è€ƒè¯• |
| Document | æ•°æ®å¯¹è±¡ | ä¸€æœ¬ä¹¦ |
| ç›¸ä¼¼åº¦æœç´¢ | æ¨¡ç³Šæœç´¢ | æ‰¾ç±»ä¼¼çš„ä¸œè¥¿ |
| chunk_size | åˆ†é¡µå¤§å° | æŠŠä¹¦æ’•æˆå°çº¸æ¡ |

---

## 6. ã€åç›´è§‰ç‚¹ã€‘

### è¯¯åŒº1ï¼šRetriever åªèƒ½ç”¨å‘é‡æ£€ç´¢ âŒ

**ä¸ºä»€ä¹ˆé”™ï¼Ÿ**
- è¿˜æœ‰ BM25ï¼ˆå…³é”®è¯æ£€ç´¢ï¼‰
- è¿˜æœ‰ SQLï¼ˆæ•°æ®åº“æŸ¥è¯¢ï¼‰
- è¿˜æœ‰ APIï¼ˆå¤–éƒ¨æœåŠ¡ï¼‰
- å¯ä»¥ç»„åˆå¤šç§æ–¹å¼

**ä¸ºä»€ä¹ˆäººä»¬å®¹æ˜“è¿™æ ·é”™ï¼Ÿ**
å› ä¸ºå‘é‡æ£€ç´¢æœ€å¸¸è§ï¼Œæ•™ç¨‹éƒ½ä»è¿™é‡Œå¼€å§‹ã€‚

**æ­£ç¡®ç†è§£ï¼š**

```python
# 1. å‘é‡æ£€ç´¢
vector_retriever = vectorstore.as_retriever()

# 2. BM25 å…³é”®è¯æ£€ç´¢
from langchain_community.retrievers import BM25Retriever
bm25_retriever = BM25Retriever.from_documents(docs)

# 3. SQL æ£€ç´¢
from langchain_community.retrievers import SQLDatabaseRetriever
sql_retriever = SQLDatabaseRetriever(db=database)

# 4. è‡ªå®šä¹‰ API æ£€ç´¢
class APIRetriever(BaseRetriever):
    def _get_relevant_documents(self, query):
        response = requests.get(f"/api/search?q={query}")
        return [Document(page_content=d["content"]) for d in response.json()]

# 5. ç»„åˆæ£€ç´¢
from langchain.retrievers import EnsembleRetriever
ensemble = EnsembleRetriever(
    retrievers=[vector_retriever, bm25_retriever],
    weights=[0.7, 0.3]
)
```

---

### è¯¯åŒº2ï¼šRAG å°±æ˜¯æŠŠæ–‡æ¡£å¡ç»™ LLM âŒ

**ä¸ºä»€ä¹ˆé”™ï¼Ÿ**
- éœ€è¦æ£€ç´¢è´¨é‡ä¼˜åŒ–
- éœ€è¦è€ƒè™‘ä¸Šä¸‹æ–‡çª—å£é™åˆ¶
- éœ€è¦è®¾è®¡å¥½ prompt
- å¯èƒ½éœ€è¦é‡æ’åº

**ä¸ºä»€ä¹ˆäººä»¬å®¹æ˜“è¿™æ ·é”™ï¼Ÿ**
RAG çœ‹èµ·æ¥ç®€å•ï¼Œä½†ç»†èŠ‚å†³å®šæ•ˆæœã€‚

**æ­£ç¡®ç†è§£ï¼š**

```python
# âŒ ç®€å•ä½†æ•ˆæœå·®
rag_chain = retriever | prompt | llm

# âœ… è€ƒè™‘æ›´å¤šå› ç´ 

# 1. æ£€ç´¢è´¨é‡ï¼šä½¿ç”¨ MMR å¢åŠ å¤šæ ·æ€§
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 4, "fetch_k": 20}
)

# 2. é‡æ’åºï¼šç”¨å°æ¨¡å‹å¯¹ç»“æœé‡æ–°æ’åº
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker

compressor = CrossEncoderReranker(model_name="cross-encoder/ms-marco-MiniLM-L-6-v2")
rerank_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=retriever
)

# 3. Prompt è®¾è®¡ï¼šæ˜ç¡®æŒ‡ç¤º
prompt = """ä½ æ˜¯ä¸€ä¸ªé—®ç­”åŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚
å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´"æ ¹æ®æä¾›çš„èµ„æ–™æ— æ³•å›ç­”"ã€‚
ä¸è¦ç¼–é€ ä¿¡æ¯ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š"""

# 4. ä¸Šä¸‹æ–‡é•¿åº¦æ§åˆ¶
def limit_context(docs, max_tokens=3000):
    total = 0
    result = []
    for doc in docs:
        tokens = len(doc.page_content) // 4  # ä¼°ç®—
        if total + tokens > max_tokens:
            break
        result.append(doc)
        total += tokens
    return result
```

---

### è¯¯åŒº3ï¼šæ£€ç´¢è¶Šå¤šæ–‡æ¡£è¶Šå¥½ âŒ

**ä¸ºä»€ä¹ˆé”™ï¼Ÿ**
- å¤ªå¤šæ–‡æ¡£ä¼šç¨€é‡Šç›¸å…³æ€§
- å¢åŠ å™ªéŸ³å’Œå¹²æ‰°
- å¯èƒ½è¶…å‡ºä¸Šä¸‹æ–‡çª—å£
- å¢åŠ æˆæœ¬å’Œå»¶è¿Ÿ

**ä¸ºä»€ä¹ˆäººä»¬å®¹æ˜“è¿™æ ·é”™ï¼Ÿ**
ä»¥ä¸º"ä¿¡æ¯è¶Šå¤šè¶Šå¥½"ã€‚

**æ­£ç¡®ç†è§£ï¼š**

```python
# âŒ æ£€ç´¢å¤ªå¤š
retriever = vectorstore.as_retriever(search_kwargs={"k": 20})
# 20 ä¸ªæ–‡æ¡£ï¼Œå¾ˆå¤šå¯èƒ½ä¸ç›¸å…³

# âœ… é€‚é‡æ£€ç´¢ + è´¨é‡æ§åˆ¶
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={
        "k": 5,              # æœ€å¤š 5 ä¸ª
        "score_threshold": 0.7  # ç›¸ä¼¼åº¦ > 0.7
    }
)

# å®è·µå»ºè®®
# - ä¸€èˆ¬åœºæ™¯ï¼š3-5 ä¸ªæ–‡æ¡£
# - å¤æ‚é—®é¢˜ï¼š5-10 ä¸ªæ–‡æ¡£
# - ä½¿ç”¨ MMR å¢åŠ å¤šæ ·æ€§è€Œéæ•°é‡
```

---

## 7. ã€å®æˆ˜ä»£ç ã€‘

```python
"""
ç¤ºä¾‹ï¼šRetriever æ£€ç´¢å™¨å®Œæ•´æ¼”ç¤º
å±•ç¤º LangChain ä¸­ Retriever çš„æ ¸å¿ƒç”¨æ³•
"""

from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field
import math

# ===== 1. Document æ•°æ®ç»“æ„ =====
print("=== 1. Document æ•°æ®ç»“æ„ ===")

@dataclass
class Document:
    """æ–‡æ¡£å¯¹è±¡"""
    page_content: str
    metadata: Dict[str, Any] = field(default_factory=dict)

    def __str__(self):
        return f"Document(content='{self.page_content[:50]}...', metadata={self.metadata})"

# åˆ›å»ºæ–‡æ¡£
docs = [
    Document(
        page_content="Python æ˜¯ä¸€ç§è§£é‡Šå‹ã€é¢å‘å¯¹è±¡çš„é«˜çº§ç¼–ç¨‹è¯­è¨€",
        metadata={"source": "python.md", "category": "ç¼–ç¨‹è¯­è¨€"}
    ),
    Document(
        page_content="LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»º LLM åº”ç”¨çš„æ¡†æ¶",
        metadata={"source": "langchain.md", "category": "AIæ¡†æ¶"}
    ),
    Document(
        page_content="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯",
        metadata={"source": "ml.md", "category": "AI"}
    ),
    Document(
        page_content="æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œå¤„ç†å¤æ‚ä»»åŠ¡",
        metadata={"source": "dl.md", "category": "AI"}
    ),
    Document(
        page_content="å‘é‡æ•°æ®åº“ç”¨äºå­˜å‚¨å’Œæ£€ç´¢å‘é‡æ•°æ®",
        metadata={"source": "vectordb.md", "category": "æ•°æ®åº“"}
    ),
]

for doc in docs:
    print(f"  {doc}")

# ===== 2. ç®€å•çš„ Embedding å®ç° =====
print("\n=== 2. ç®€å• Embedding ===")

class SimpleEmbedding:
    """ç®€å•çš„ Embeddingï¼ˆåŸºäºè¯é¢‘ï¼‰"""

    def __init__(self, vocabulary: List[str] = None):
        self.vocabulary = vocabulary or []

    def fit(self, texts: List[str]):
        """æ„å»ºè¯æ±‡è¡¨"""
        all_words = set()
        for text in texts:
            words = text.lower().split()
            all_words.update(words)
        self.vocabulary = list(all_words)
        print(f"è¯æ±‡è¡¨å¤§å°: {len(self.vocabulary)}")

    def embed(self, text: str) -> List[float]:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡"""
        text_lower = text.lower()
        vector = []
        for word in self.vocabulary:
            # ç®€å•çš„è¯é¢‘
            count = text_lower.count(word)
            vector.append(count)
        # å½’ä¸€åŒ–
        norm = math.sqrt(sum(x**2 for x in vector)) or 1
        return [x / norm for x in vector]

# åˆ›å»º Embedding
embedding = SimpleEmbedding()
embedding.fit([doc.page_content for doc in docs])

# æµ‹è¯•
vec = embedding.embed("Python ç¼–ç¨‹è¯­è¨€")
print(f"å‘é‡ç»´åº¦: {len(vec)}")
print(f"å‘é‡ï¼ˆå‰10ç»´ï¼‰: {vec[:10]}")

# ===== 3. VectorStore å®ç° =====
print("\n=== 3. VectorStore ===")

class SimpleVectorStore:
    """ç®€å•çš„å‘é‡å­˜å‚¨"""

    def __init__(self, embedding: SimpleEmbedding):
        self.embedding = embedding
        self.documents: List[Document] = []
        self.vectors: List[List[float]] = []

    def add_documents(self, docs: List[Document]):
        """æ·»åŠ æ–‡æ¡£"""
        for doc in docs:
            self.documents.append(doc)
            vec = self.embedding.embed(doc.page_content)
            self.vectors.append(vec)
        print(f"æ·»åŠ  {len(docs)} ä¸ªæ–‡æ¡£")

    def similarity_search(self, query: str, k: int = 4) -> List[Document]:
        """ç›¸ä¼¼åº¦æœç´¢"""
        query_vec = self.embedding.embed(query)

        # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        scores = []
        for i, doc_vec in enumerate(self.vectors):
            similarity = sum(a * b for a, b in zip(query_vec, doc_vec))
            scores.append((i, similarity))

        # æ’åºå– top-k
        scores.sort(key=lambda x: x[1], reverse=True)
        results = [self.documents[i] for i, _ in scores[:k]]
        return results

    def as_retriever(self, search_kwargs: Dict = None) -> "VectorStoreRetriever":
        """åˆ›å»ºæ£€ç´¢å™¨"""
        search_kwargs = search_kwargs or {"k": 4}
        return VectorStoreRetriever(vectorstore=self, search_kwargs=search_kwargs)

# åˆ›å»ºå‘é‡å­˜å‚¨
vectorstore = SimpleVectorStore(embedding)
vectorstore.add_documents(docs)

# æµ‹è¯•æœç´¢
print("\næœç´¢ 'Python ç¼–ç¨‹':")
results = vectorstore.similarity_search("Python ç¼–ç¨‹", k=2)
for doc in results:
    print(f"  {doc.page_content[:50]}...")

# ===== 4. BaseRetriever å®ç° =====
print("\n=== 4. Retriever å®ç° ===")

class BaseRetriever:
    """æ£€ç´¢å™¨åŸºç±»"""

    def invoke(self, query: str) -> List[Document]:
        return self._get_relevant_documents(query)

    def _get_relevant_documents(self, query: str) -> List[Document]:
        raise NotImplementedError

class VectorStoreRetriever(BaseRetriever):
    """å‘é‡å­˜å‚¨æ£€ç´¢å™¨"""

    def __init__(self, vectorstore: SimpleVectorStore, search_kwargs: Dict = None):
        self.vectorstore = vectorstore
        self.search_kwargs = search_kwargs or {"k": 4}

    def _get_relevant_documents(self, query: str) -> List[Document]:
        k = self.search_kwargs.get("k", 4)
        return self.vectorstore.similarity_search(query, k=k)

# åˆ›å»ºæ£€ç´¢å™¨
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# æµ‹è¯•æ£€ç´¢
print("\næ£€ç´¢ 'äººå·¥æ™ºèƒ½':")
results = retriever.invoke("äººå·¥æ™ºèƒ½")
for doc in results:
    print(f"  - {doc.page_content[:50]}...")

# ===== 5. è‡ªå®šä¹‰æ£€ç´¢å™¨ =====
print("\n=== 5. è‡ªå®šä¹‰æ£€ç´¢å™¨ ===")

class KeywordRetriever(BaseRetriever):
    """å…³é”®è¯æ£€ç´¢å™¨"""

    def __init__(self, documents: List[Document]):
        self.documents = documents

    def _get_relevant_documents(self, query: str) -> List[Document]:
        query_words = set(query.lower().split())
        results = []

        for doc in self.documents:
            content_lower = doc.page_content.lower()
            # è®¡ç®—åŒ¹é…çš„å…³é”®è¯æ•°é‡
            matches = sum(1 for word in query_words if word in content_lower)
            if matches > 0:
                results.append((doc, matches))

        # æŒ‰åŒ¹é…æ•°æ’åº
        results.sort(key=lambda x: x[1], reverse=True)
        return [doc for doc, _ in results[:4]]

# æµ‹è¯•
keyword_retriever = KeywordRetriever(docs)
print("\nå…³é”®è¯æ£€ç´¢ 'æ·±åº¦å­¦ä¹  ç¥ç»ç½‘ç»œ':")
results = keyword_retriever.invoke("æ·±åº¦å­¦ä¹  ç¥ç»ç½‘ç»œ")
for doc in results:
    print(f"  - {doc.page_content[:50]}...")

# ===== 6. Ensemble ç»„åˆæ£€ç´¢ =====
print("\n=== 6. Ensemble ç»„åˆæ£€ç´¢ ===")

class EnsembleRetriever(BaseRetriever):
    """ç»„åˆæ£€ç´¢å™¨"""

    def __init__(self, retrievers: List[BaseRetriever], weights: List[float] = None):
        self.retrievers = retrievers
        self.weights = weights or [1.0 / len(retrievers)] * len(retrievers)

    def _get_relevant_documents(self, query: str) -> List[Document]:
        # æ”¶é›†æ‰€æœ‰ç»“æœ
        all_docs = {}

        for retriever, weight in zip(self.retrievers, self.weights):
            results = retriever.invoke(query)
            for i, doc in enumerate(results):
                key = doc.page_content
                score = weight * (len(results) - i)  # ä½ç½®è¶Šé å‰åˆ†æ•°è¶Šé«˜
                if key in all_docs:
                    all_docs[key] = (doc, all_docs[key][1] + score)
                else:
                    all_docs[key] = (doc, score)

        # æŒ‰æ€»åˆ†æ’åº
        sorted_docs = sorted(all_docs.values(), key=lambda x: x[1], reverse=True)
        return [doc for doc, _ in sorted_docs[:4]]

# åˆ›å»ºç»„åˆæ£€ç´¢å™¨
ensemble = EnsembleRetriever(
    retrievers=[retriever, keyword_retriever],
    weights=[0.7, 0.3]
)

print("\nç»„åˆæ£€ç´¢ 'Python ç¼–ç¨‹è¯­è¨€':")
results = ensemble.invoke("Python ç¼–ç¨‹è¯­è¨€")
for doc in results:
    print(f"  - {doc.page_content[:50]}...")

# ===== 7. RAG Chain å®ç° =====
print("\n=== 7. RAG Chain ===")

class MockLLM:
    """æ¨¡æ‹Ÿ LLM"""

    def invoke(self, prompt: str) -> str:
        # ç®€å•çš„è§„åˆ™å“åº”
        if "Python" in prompt:
            return "æ ¹æ®ä¸Šä¸‹æ–‡ï¼ŒPython æ˜¯ä¸€ç§è§£é‡Šå‹ã€é¢å‘å¯¹è±¡çš„é«˜çº§ç¼–ç¨‹è¯­è¨€ã€‚"
        if "LangChain" in prompt:
            return "æ ¹æ®ä¸Šä¸‹æ–‡ï¼ŒLangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»º LLM åº”ç”¨çš„æ¡†æ¶ã€‚"
        return "æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ï¼Œæˆ‘æ‰¾åˆ°äº†ç›¸å…³ä¿¡æ¯ã€‚"

def format_docs(docs: List[Document]) -> str:
    """æ ¼å¼åŒ–æ–‡æ¡£ä¸ºå­—ç¬¦ä¸²"""
    return "\n\n".join(doc.page_content for doc in docs)

class RAGChain:
    """RAG Chain å®ç°"""

    def __init__(self, retriever: BaseRetriever, llm: MockLLM):
        self.retriever = retriever
        self.llm = llm

    def invoke(self, question: str) -> Dict[str, Any]:
        # 1. æ£€ç´¢
        docs = self.retriever.invoke(question)
        context = format_docs(docs)

        # 2. æ„å»º prompt
        prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š"""

        # 3. ç”Ÿæˆå›ç­”
        answer = self.llm.invoke(prompt)

        return {
            "question": question,
            "context": context,
            "answer": answer,
            "source_documents": docs
        }

# åˆ›å»º RAG Chain
llm = MockLLM()
rag_chain = RAGChain(retriever, llm)

# æµ‹è¯•
print("\né—®ï¼šä»€ä¹ˆæ˜¯ Pythonï¼Ÿ")
result = rag_chain.invoke("ä»€ä¹ˆæ˜¯ Pythonï¼Ÿ")
print(f"ç­”ï¼š{result['answer']}")
print(f"æ¥æºæ–‡æ¡£æ•°ï¼š{len(result['source_documents'])}")

# ===== 8. æ–‡æ¡£åˆ†å‰² =====
print("\n=== 8. æ–‡æ¡£åˆ†å‰² ===")

class TextSplitter:
    """æ–‡æœ¬åˆ†å‰²å™¨"""

    def __init__(self, chunk_size: int = 100, chunk_overlap: int = 20):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def split_text(self, text: str) -> List[str]:
        """åˆ†å‰²æ–‡æœ¬"""
        chunks = []
        start = 0
        while start < len(text):
            end = start + self.chunk_size
            chunk = text[start:end]
            chunks.append(chunk)
            start = end - self.chunk_overlap
        return chunks

    def split_documents(self, docs: List[Document]) -> List[Document]:
        """åˆ†å‰²æ–‡æ¡£"""
        result = []
        for doc in docs:
            chunks = self.split_text(doc.page_content)
            for i, chunk in enumerate(chunks):
                new_doc = Document(
                    page_content=chunk,
                    metadata={**doc.metadata, "chunk_index": i}
                )
                result.append(new_doc)
        return result

# æµ‹è¯•åˆ†å‰²
long_doc = Document(
    page_content="è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„æ–‡æœ¬ã€‚" * 20,
    metadata={"source": "long.md"}
)

splitter = TextSplitter(chunk_size=50, chunk_overlap=10)
splits = splitter.split_documents([long_doc])
print(f"åŸå§‹æ–‡æ¡£: {len(long_doc.page_content)} å­—ç¬¦")
print(f"åˆ†å‰²å: {len(splits)} ä¸ªå—")
for i, split in enumerate(splits[:3]):
    print(f"  å— {i}: {len(split.page_content)} å­—ç¬¦")

# ===== 9. å¸¦å…ƒæ•°æ®è¿‡æ»¤ =====
print("\n=== 9. å…ƒæ•°æ®è¿‡æ»¤ ===")

class FilteredRetriever(BaseRetriever):
    """æ”¯æŒå…ƒæ•°æ®è¿‡æ»¤çš„æ£€ç´¢å™¨"""

    def __init__(self, base_retriever: BaseRetriever, filter_fn=None):
        self.base_retriever = base_retriever
        self.filter_fn = filter_fn

    def _get_relevant_documents(self, query: str) -> List[Document]:
        docs = self.base_retriever.invoke(query)
        if self.filter_fn:
            docs = [doc for doc in docs if self.filter_fn(doc)]
        return docs

# åªæ£€ç´¢ AI ç›¸å…³çš„æ–‡æ¡£
ai_retriever = FilteredRetriever(
    retriever,
    filter_fn=lambda doc: doc.metadata.get("category") == "AI"
)

print("\nåªæ£€ç´¢ AI ç±»åˆ«çš„æ–‡æ¡£:")
results = ai_retriever.invoke("å­¦ä¹ ")
for doc in results:
    print(f"  - [{doc.metadata.get('category')}] {doc.page_content[:30]}...")

print("\n=== å®Œæˆï¼===")
```

**è¿è¡Œè¾“å‡ºç¤ºä¾‹ï¼š**
```
=== 1. Document æ•°æ®ç»“æ„ ===
  Document(content='Python æ˜¯ä¸€ç§è§£é‡Šå‹ã€é¢å‘å¯¹è±¡çš„é«˜çº§ç¼–ç¨‹è¯­è¨€...', metadata={'source': 'python.md'})
  ...

=== 2. ç®€å• Embedding ===
è¯æ±‡è¡¨å¤§å°: 28
å‘é‡ç»´åº¦: 28

=== 3. VectorStore ===
æ·»åŠ  5 ä¸ªæ–‡æ¡£

æœç´¢ 'Python ç¼–ç¨‹':
  Python æ˜¯ä¸€ç§è§£é‡Šå‹ã€é¢å‘å¯¹è±¡çš„é«˜çº§ç¼–ç¨‹è¯­è¨€...

=== 4. Retriever å®ç° ===

æ£€ç´¢ 'äººå·¥æ™ºèƒ½':
  - æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯...
  - æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œå¤„ç†å¤æ‚ä»»åŠ¡...

=== 7. RAG Chain ===

é—®ï¼šä»€ä¹ˆæ˜¯ Pythonï¼Ÿ
ç­”ï¼šæ ¹æ®ä¸Šä¸‹æ–‡ï¼ŒPython æ˜¯ä¸€ç§è§£é‡Šå‹ã€é¢å‘å¯¹è±¡çš„é«˜çº§ç¼–ç¨‹è¯­è¨€ã€‚
æ¥æºæ–‡æ¡£æ•°ï¼š3

=== å®Œæˆï¼===
```

---

## 8. ã€é¢è¯•å¿…é—®ã€‘

### é—®é¢˜1ï¼š"ä»€ä¹ˆæ˜¯ RAGï¼Ÿä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ"

**æ™®é€šå›ç­”ï¼ˆâŒ ä¸å‡ºå½©ï¼‰ï¼š**
"RAG æ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œå…ˆæ£€ç´¢æ–‡æ¡£å†è®© LLM å›ç­”ã€‚"

**å‡ºå½©å›ç­”ï¼ˆâœ… æ¨èï¼‰ï¼š**

> **RAG (Retrieval Augmented Generation) è§£å†³ LLM çš„çŸ¥è¯†å±€é™ï¼š**
>
> **ä¸ºä»€ä¹ˆéœ€è¦ RAGï¼š**
> - LLM çŸ¥è¯†æœ‰æˆªæ­¢æ—¥æœŸ
> - LLM ä¸çŸ¥é“ç§æœ‰æ•°æ®
> - LLM å¯èƒ½äº§ç”Ÿ"å¹»è§‰"
>
> **RAG å·¥ä½œæµç¨‹ï¼š**
> ```
> 1. ç”¨æˆ·é—®é¢˜ â†’ 2. æ£€ç´¢ç›¸å…³æ–‡æ¡£ â†’ 3. æ–‡æ¡£ä½œä¸ºä¸Šä¸‹æ–‡ â†’ 4. LLM ç”Ÿæˆå›ç­”
> ```
>
> **æ ¸å¿ƒç»„ä»¶ï¼š**
> - **Retriever**ï¼šæ£€ç´¢ç›¸å…³æ–‡æ¡£
> - **VectorStore**ï¼šå­˜å‚¨å’Œç´¢å¼•æ–‡æ¡£
> - **Embedding**ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
>
> **å…³é”®ä¼˜åŒ–ç‚¹ï¼š**
> - æ£€ç´¢è´¨é‡ï¼šä½¿ç”¨ MMR å¢åŠ å¤šæ ·æ€§
> - æ–‡æ¡£åˆ†å‰²ï¼šåˆç†çš„ chunk_size
> - Prompt è®¾è®¡ï¼šæ˜ç¡®æŒ‡ç¤º LLM åŸºäºä¸Šä¸‹æ–‡å›ç­”
>
> **å®é™…ç»éªŒ**ï¼šåœ¨ä¼ä¸šçŸ¥è¯†åº“é¡¹ç›®ä¸­ï¼Œæˆ‘ä½¿ç”¨ RAG è®© LLM èƒ½å›ç­”å…¬å¸å†…éƒ¨æ–‡æ¡£çš„é—®é¢˜ã€‚é€šè¿‡ Ensemble Retrieverï¼ˆå‘é‡+BM25ï¼‰æå‡äº† 30% çš„æ£€ç´¢å‡†ç¡®ç‡ã€‚

**ä¸ºä»€ä¹ˆè¿™ä¸ªå›ç­”å‡ºå½©ï¼Ÿ**
1. âœ… è§£é‡Šäº† RAG çš„å¿…è¦æ€§
2. âœ… æ¸…æ™°çš„æµç¨‹è¯´æ˜
3. âœ… æåˆ°äº†ä¼˜åŒ–ç‚¹
4. âœ… æœ‰å®é™…é¡¹ç›®ç»éªŒ

---

### é—®é¢˜2ï¼š"å¦‚ä½•æå‡ RAG çš„æ•ˆæœï¼Ÿ"

**å‡ºå½©å›ç­”ï¼ˆâœ… æ¨èï¼‰ï¼š**

> **RAG ä¼˜åŒ–ä»ä¸‰ä¸ªç»´åº¦ï¼š**
>
> **1. æ£€ç´¢è´¨é‡**
> ```python
> # å¤šè·¯å¬å›
> ensemble = EnsembleRetriever([vector, bm25], weights=[0.7, 0.3])
>
> # é‡æ’åº
> reranker = CrossEncoderReranker()
>
> # MMR å¤šæ ·æ€§
> retriever = vectorstore.as_retriever(search_type="mmr")
> ```
>
> **2. æ–‡æ¡£å¤„ç†**
> ```python
> # åˆç†åˆ†å—
> splitter = RecursiveCharacterTextSplitter(
>     chunk_size=500,
>     chunk_overlap=50
> )
>
> # ä¿ç•™å…ƒæ•°æ®
> metadata={"source": "...", "section": "..."}
> ```
>
> **3. Prompt ä¼˜åŒ–**
> ```python
> prompt = """ä¸¥æ ¼åŸºäºä¸Šä¸‹æ–‡å›ç­”ï¼Œä¸è¦ç¼–é€ ã€‚
> å¦‚æœä¸Šä¸‹æ–‡ä¸è¶³ï¼Œè¯·è¯´æ˜ã€‚
> å¼•ç”¨æ¥æºã€‚"""
> ```

---

## 9. ã€åŒ–éª¨ç»µæŒã€‘

### å¡ç‰‡1ï¼šRetriever æ˜¯ä»€ä¹ˆï¼Ÿ ğŸ¯

**ä¸€å¥è¯ï¼š** Retriever æ ¹æ®æŸ¥è¯¢ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ã€‚

**ä¸¾ä¾‹ï¼š**
```python
docs = retriever.invoke("ä»€ä¹ˆæ˜¯ Pythonï¼Ÿ")
# è¿”å›ç›¸å…³æ–‡æ¡£åˆ—è¡¨
```

**åº”ç”¨ï¼š** RAG åº”ç”¨çš„æ ¸å¿ƒç»„ä»¶ã€‚

---

### å¡ç‰‡2ï¼šVectorStore å‘é‡å­˜å‚¨ ğŸ“Š

**ä¸€å¥è¯ï¼š** å­˜å‚¨æ–‡æ¡£å‘é‡ï¼Œæ”¯æŒç›¸ä¼¼åº¦æœç´¢ã€‚

**ä¸¾ä¾‹ï¼š**
```python
vectorstore = FAISS.from_documents(docs, embeddings)
retriever = vectorstore.as_retriever()
```

**åº”ç”¨ï¼š** è¯­ä¹‰æœç´¢çš„åŸºç¡€è®¾æ–½ã€‚

---

### å¡ç‰‡3ï¼šEmbedding æ–‡æœ¬å‘é‡åŒ– ğŸ”¢

**ä¸€å¥è¯ï¼š** å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼å‘é‡ï¼Œç›¸ä¼¼æ–‡æœ¬çš„å‘é‡è·ç¦»æ›´è¿‘ã€‚

**ä¸¾ä¾‹ï¼š**
```python
embeddings = OpenAIEmbeddings()
vector = embeddings.embed_query("Python ç¼–ç¨‹")
```

**åº”ç”¨ï¼š** å®ç°è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—ã€‚

---

### å¡ç‰‡4ï¼šRAG Pattern æ£€ç´¢å¢å¼ºç”Ÿæˆ ğŸ”—

**ä¸€å¥è¯ï¼š** å…ˆæ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œå†è®© LLM åŸºäºæ–‡æ¡£ç”Ÿæˆå›ç­”ã€‚

**ä¸¾ä¾‹ï¼š**
```python
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt | llm
)
```

**åº”ç”¨ï¼š** çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿã€‚

---

### å¡ç‰‡5ï¼šDocument æ–‡æ¡£å¯¹è±¡ ğŸ“„

**ä¸€å¥è¯ï¼š** LangChain ä¸­è¡¨ç¤ºæ–‡æ¡£çš„æ ‡å‡†ç»“æ„ï¼ŒåŒ…å«å†…å®¹å’Œå…ƒæ•°æ®ã€‚

**ä¸¾ä¾‹ï¼š**
```python
doc = Document(
    page_content="å†…å®¹",
    metadata={"source": "file.md"}
)
```

**åº”ç”¨ï¼š** æ‰€æœ‰æ–‡æ¡£å¤„ç†çš„åŸºç¡€ã€‚

---

### å¡ç‰‡6ï¼šsearch_type æœç´¢ç±»å‹ ğŸ”

**ä¸€å¥è¯ï¼š** similarityï¼ˆç›¸ä¼¼åº¦ï¼‰ã€mmrï¼ˆå¤šæ ·æ€§ï¼‰ã€thresholdï¼ˆé˜ˆå€¼è¿‡æ»¤ï¼‰ã€‚

**ä¸¾ä¾‹ï¼š**
```python
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 4}
)
```

**åº”ç”¨ï¼š** æ ¹æ®åœºæ™¯é€‰æ‹©åˆé€‚çš„æœç´¢ç­–ç•¥ã€‚

---

### å¡ç‰‡7ï¼šEnsembleRetriever å¤šè·¯å¬å› ğŸ”€

**ä¸€å¥è¯ï¼š** ç»„åˆå¤šä¸ªæ£€ç´¢å™¨ï¼Œèåˆä¸åŒæ£€ç´¢ç­–ç•¥çš„ä¼˜åŠ¿ã€‚

**ä¸¾ä¾‹ï¼š**
```python
ensemble = EnsembleRetriever(
    retrievers=[vector_retriever, bm25_retriever],
    weights=[0.7, 0.3]
)
```

**åº”ç”¨ï¼š** æå‡æ£€ç´¢å‡†ç¡®ç‡ã€‚

---

### å¡ç‰‡8ï¼šTextSplitter æ–‡æ¡£åˆ†å‰² âœ‚ï¸

**ä¸€å¥è¯ï¼š** å°†é•¿æ–‡æ¡£åˆ†å‰²æˆé€‚åˆæ£€ç´¢çš„å°å—ã€‚

**ä¸¾ä¾‹ï¼š**
```python
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
```

**åº”ç”¨ï¼š** å¤„ç†é•¿æ–‡æ¡£çš„å¿…è¦æ­¥éª¤ã€‚

---

### å¡ç‰‡9ï¼šè‡ªå®šä¹‰ Retriever ğŸ”§

**ä¸€å¥è¯ï¼š** ç»§æ‰¿ BaseRetrieverï¼Œå®ç° _get_relevant_documents æ–¹æ³•ã€‚

**ä¸¾ä¾‹ï¼š**
```python
class MyRetriever(BaseRetriever):
    def _get_relevant_documents(self, query):
        return search_my_database(query)
```

**åº”ç”¨ï¼š** æ¥å…¥è‡ªå®šä¹‰æ•°æ®æºã€‚

---

### å¡ç‰‡10ï¼šRetriever åœ¨ LangChain æºç ä¸­çš„ä½ç½® â­

**ä¸€å¥è¯ï¼š** Retriever å®ç° Runnable æ¥å£ï¼Œå¯ä»¥ç›´æ¥ç”¨åœ¨ LCEL Chain ä¸­ã€‚

**ä¸¾ä¾‹ï¼š**
```python
# Retriever ä½œä¸º Runnable
chain = {"context": retriever, "q": RunnablePassthrough()} | prompt
```

**åº”ç”¨ï¼š** ç†è§£ Retriever ä¸ LCEL çš„æ— ç¼é›†æˆã€‚

---

## 10. ã€ä¸€å¥è¯æ€»ç»“ã€‘

**Retriever æ˜¯ RAG æ¨¡å¼çš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£ä»çŸ¥è¯†åº“æ£€ç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„æ–‡æ¡£ï¼ŒVectorStoreRetriever åŸºäºå‘é‡ç›¸ä¼¼åº¦æ˜¯æœ€å¸¸ç”¨çš„å®ç°ï¼Œé…åˆ Embedding å’Œåˆç†çš„æ£€ç´¢ç­–ç•¥å¯ä»¥å¤§å¹…æå‡ LLM é—®ç­”çš„å‡†ç¡®æ€§ã€‚**

---

## ğŸ“š å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£ Retriever åœ¨ RAG ä¸­çš„ä½œç”¨
- [ ] ä¼šåˆ›å»º VectorStoreRetriever
- [ ] èƒ½å¤Ÿæ„å»ºå®Œæ•´çš„ RAG Chain
- [ ] äº†è§£ä¸åŒ search_type çš„åŒºåˆ«
- [ ] çŸ¥é“å¦‚ä½•åˆ†å‰²é•¿æ–‡æ¡£
- [ ] ä¼šä½¿ç”¨ EnsembleRetriever å¤šè·¯å¬å›

## ğŸ”— ä¸‹ä¸€æ­¥å­¦ä¹ 

- **Callback å›è°ƒç³»ç»Ÿ**ï¼šç›‘æ§æ£€ç´¢è¿‡ç¨‹
- **Agent ä¸ Retriever**ï¼šè®© Agent ä½¿ç”¨æ£€ç´¢å·¥å…·
- **é«˜çº§ RAG**ï¼šé‡æ’åºã€æŸ¥è¯¢æ‰©å±•ã€å‡è®¾æ–‡æ¡£

---

**ç‰ˆæœ¬ï¼š** v1.0
**æœ€åæ›´æ–°ï¼š** 2025-01-14
