# Token ä¸ä¸Šä¸‹æ–‡çª—å£

> åŸå­åŒ–çŸ¥è¯†ç‚¹ | LLMé¢†åŸŸçŸ¥è¯† | LangChain æºç å­¦ä¹ å‰ç½®çŸ¥è¯†

---

## 1. ã€30å­—æ ¸å¿ƒã€‘

**Token æ˜¯ LLM å¤„ç†æ–‡æœ¬çš„åŸºæœ¬å•ä½ï¼Œä¸Šä¸‹æ–‡çª—å£æ˜¯ LLM ä¸€æ¬¡èƒ½å¤„ç†çš„ Token æ€»é‡é™åˆ¶ã€‚**

---

## 2. ã€ç¬¬ä¸€æ€§åŸç†ã€‘

### ä»€ä¹ˆæ˜¯ç¬¬ä¸€æ€§åŸç†ï¼Ÿ

**ç¬¬ä¸€æ€§åŸç†**ï¼šå›åˆ°äº‹ç‰©æœ€åŸºæœ¬çš„çœŸç†ï¼Œä»æºå¤´æ€è€ƒé—®é¢˜

### Token ä¸ä¸Šä¸‹æ–‡çª—å£çš„ç¬¬ä¸€æ€§åŸç† ğŸ¯

#### 1. æœ€åŸºç¡€çš„å®šä¹‰

**Token = æ–‡æœ¬çš„æœ€å°å¤„ç†å•å…ƒï¼ˆä¸ç­‰äºå­—ç¬¦ï¼Œä¹Ÿä¸ç­‰äºå•è¯ï¼‰**

ä»…æ­¤è€Œå·²ï¼æ²¡æœ‰æ›´åŸºç¡€çš„äº†ã€‚

```python
# Token æ‹†åˆ†ç¤ºä¾‹
"Hello World" â†’ ["Hello", " World"]           # 2 tokens
"ä½ å¥½ä¸–ç•Œ"    â†’ ["ä½ ", "å¥½", "ä¸–", "ç•Œ"]       # 4 tokensï¼ˆä¸­æ–‡æ¯å­—çº¦1tokenï¼‰
"LangChain"   â†’ ["Lang", "Chain"]             # 2 tokensï¼ˆå­è¯æ‹†åˆ†ï¼‰
```

**ä¸Šä¸‹æ–‡çª—å£ï¼ˆContext Windowï¼‰= LLM èƒ½åŒæ—¶"çœ‹åˆ°"çš„ Token æ€»æ•°**

```python
# ä¸Šä¸‹æ–‡çª—å£ = è¾“å…¥ Token + è¾“å‡º Token
context_window = input_tokens + output_tokens
# GPT-4: 128K tokens
# Claude 3: 200K tokens
```

#### 2. ä¸ºä»€ä¹ˆéœ€è¦ Tokenï¼Ÿ

**æ ¸å¿ƒé—®é¢˜ï¼šè®¡ç®—æœºæ— æ³•ç›´æ¥å¤„ç†ä»»æ„é•¿åº¦çš„æ–‡æœ¬**

```python
# ç›´æ¥å¤„ç†æ–‡æœ¬çš„é—®é¢˜
text = "Hello, how are you?"

# é—®é¢˜1ï¼šæ–‡æœ¬é•¿åº¦å¯å˜ï¼Œç¥ç»ç½‘ç»œéœ€è¦å›ºå®šè¾“å…¥
# é—®é¢˜2ï¼šå­—ç¬¦çº§å¤„ç†æ•ˆç‡å¤ªä½ï¼ˆ26ä¸ªå­—æ¯ç»„åˆæ— ç©·ï¼‰
# é—®é¢˜3ï¼šå•è¯çº§å¤„ç†è¯æ±‡è¡¨å¤ªå¤§ï¼ˆè‹±è¯­æœ‰ 100ä¸‡+ å•è¯ï¼‰

# è§£å†³æ–¹æ¡ˆï¼šTokenï¼ˆå­è¯ï¼‰
# - å¸¸è§è¯ä¿æŒå®Œæ•´ï¼š"the", "is"
# - ç”Ÿåƒ»è¯æ‹†æˆå­è¯ï¼š"unhappiness" â†’ ["un", "happiness"]
# - è¯æ±‡è¡¨å¤§å°å¯æ§ï¼š~50,000-100,000 tokens
```

#### 3. Token çš„ä¸‰å±‚ä»·å€¼

##### ä»·å€¼1ï¼šæ•ˆç‡ä¸è´¨é‡çš„å¹³è¡¡

```python
# å­—ç¬¦çº§ï¼šæ•ˆç‡ä½ï¼Œéœ€è¦å­¦ä¹ å­—ç¬¦ç»„åˆ
"cat" â†’ ['c', 'a', 't']  # 3ä¸ªå•å…ƒ

# å•è¯çº§ï¼šè¯æ±‡è¡¨çˆ†ç‚¸ï¼Œæ— æ³•å¤„ç†æœªçŸ¥è¯
"ChatGPT" â†’ ['ChatGPT']  # OOVï¼ˆOut of Vocabularyï¼‰é—®é¢˜

# Tokençº§ï¼šå¹³è¡¡æ•ˆç‡å’Œè¦†ç›–
"ChatGPT" â†’ ['Chat', 'G', 'PT']  # å¯ä»¥å¤„ç†ä»»ä½•è¯
```

##### ä»·å€¼2ï¼šæˆæœ¬è®¡ç®—çš„åŸºç¡€

```python
# API æŒ‰ Token è®¡è´¹
# GPT-4: $0.03/1K input tokens, $0.06/1K output tokens

text = "è¯·ç”¨ Python å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•"
tokens = count_tokens(text)  # ~15 tokens
cost = tokens * 0.00003  # è¾“å…¥æˆæœ¬

# ç†è§£ Token æ‰èƒ½æ§åˆ¶æˆæœ¬
```

##### ä»·å€¼3ï¼šä¸Šä¸‹æ–‡ç®¡ç†çš„ä¾æ®

```python
# ä¸Šä¸‹æ–‡çª—å£æœ‰é™ï¼Œå¿…é¡»åˆç†åˆ†é…
context_window = 128000  # GPT-4 Turbo

# åˆ†é…ç­–ç•¥
system_prompt = 2000      # ç³»ç»Ÿæç¤º
history = 10000           # å¯¹è¯å†å²
retrieved_docs = 50000    # RAG æ£€ç´¢å†…å®¹
user_input = 1000         # ç”¨æˆ·è¾“å…¥
reserved_output = 65000   # ç•™ç»™è¾“å‡º

# è¶…å‡ºçª—å£ = ä¸¢å¤±ä¿¡æ¯æˆ–æŠ¥é”™
```

#### 4. ä»ç¬¬ä¸€æ€§åŸç†æ¨å¯¼ LangChain åº”ç”¨

**æ¨ç†é“¾ï¼š**

```
1. LLM ä»¥ Token ä¸ºå•ä½å¤„ç†æ–‡æœ¬
   â†“
2. æ¯ä¸ª LLM æœ‰å›ºå®šçš„ä¸Šä¸‹æ–‡çª—å£å¤§å°
   â†“
3. è¾“å…¥+è¾“å‡ºä¸èƒ½è¶…è¿‡ä¸Šä¸‹æ–‡çª—å£
   â†“
4. é•¿å¯¹è¯éœ€è¦"æˆªæ–­"æˆ–"æ€»ç»“"å†å²
   â†“
5. RAG æ£€ç´¢éœ€è¦æ§åˆ¶æ£€ç´¢å†…å®¹å¤§å°
   â†“
6. LangChain éœ€è¦ Token è®¡æ•°å’Œç®¡ç†å·¥å…·
   â†“
7. ConversationBufferWindowMemoryã€ConversationSummaryMemory ç­‰
```

#### 5. ä¸€å¥è¯æ€»ç»“ç¬¬ä¸€æ€§åŸç†

**Token æ˜¯ LLM ç†è§£ä¸–ç•Œçš„æœ€å°å•å…ƒï¼Œä¸Šä¸‹æ–‡çª—å£æ˜¯ LLM çš„"å·¥ä½œè®°å¿†"å®¹é‡ï¼Œç†è§£è¿™ä¸¤ä¸ªæ¦‚å¿µæ˜¯è®¾è®¡é«˜æ•ˆ LangChain åº”ç”¨çš„åŸºç¡€ã€‚**

---

## 3. ã€æ ¸å¿ƒæ¦‚å¿µï¼ˆå…¨é¢è¦†ç›–ï¼‰ã€‘

### æ ¸å¿ƒæ¦‚å¿µ1ï¼šToken ä¸ Tokenizer ğŸ”¤

**Tokenizer æ˜¯å°†æ–‡æœ¬æ‹†åˆ†æˆ Token çš„å·¥å…·ï¼Œä¸åŒæ¨¡å‹ä½¿ç”¨ä¸åŒçš„ Tokenizer**

```python
import tiktoken

# OpenAI çš„ Tokenizer
encoding = tiktoken.encoding_for_model("gpt-4")

# æ–‡æœ¬ â†’ Token IDs
text = "Hello, World!"
tokens = encoding.encode(text)
print(f"Tokens: {tokens}")  # [9906, 11, 4435, 0]
print(f"Tokenæ•°: {len(tokens)}")  # 4

# Token IDs â†’ Token æ–‡æœ¬
for token_id in tokens:
    print(f"  {token_id} â†’ '{encoding.decode([token_id])}'")
# 9906 â†’ 'Hello'
# 11 â†’ ','
# 4435 â†’ ' World'
# 0 â†’ '!'

# ä¸­æ–‡ Token åŒ–
chinese_text = "ä½ å¥½ä¸–ç•Œ"
chinese_tokens = encoding.encode(chinese_text)
print(f"ä¸­æ–‡ Tokenæ•°: {len(chinese_tokens)}")  # çº¦ 4-8 ä¸ª
```

**ä¸åŒè¯­è¨€çš„ Token æ•ˆç‡ï¼š**

| è¯­è¨€ | ç¤ºä¾‹æ–‡æœ¬ | Token æ•° | æ•ˆç‡ |
|------|---------|----------|------|
| è‹±è¯­ | "Hello World" | 2 | é«˜ |
| ä¸­æ–‡ | "ä½ å¥½ä¸–ç•Œ" | 4-6 | ä¸­ |
| æ—¥è¯­ | "ã“ã‚“ã«ã¡ã¯" | 5-8 | ä½ |
| ä»£ç  | `def hello():` | 4 | é«˜ |

**åœ¨ LangChain æºç ä¸­çš„åº”ç”¨ï¼š**

```python
# langchain_core/language_models/base.py
class BaseLanguageModel(ABC):
    """è¯­è¨€æ¨¡å‹åŸºç±»"""

    def get_num_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬çš„ Token æ•°é‡"""
        # ä¸åŒæ¨¡å‹æœ‰ä¸åŒå®ç°
        pass

    def get_num_tokens_from_messages(self, messages: List[BaseMessage]) -> int:
        """è®¡ç®—æ¶ˆæ¯åˆ—è¡¨çš„ Token æ•°é‡"""
        pass

# ä½¿ç”¨ç¤ºä¾‹
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4")
num_tokens = llm.get_num_tokens("Hello World")
```

---

### æ ¸å¿ƒæ¦‚å¿µ2ï¼šä¸Šä¸‹æ–‡çª—å£ï¼ˆContext Windowï¼‰ ğŸ“

**ä¸Šä¸‹æ–‡çª—å£æ˜¯ LLM ä¸€æ¬¡è¯·æ±‚èƒ½å¤„ç†çš„æœ€å¤§ Token æ•°é‡**

```python
# å¸¸è§æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£
context_windows = {
    "gpt-3.5-turbo": 16385,
    "gpt-4": 8192,
    "gpt-4-turbo": 128000,
    "gpt-4o": 128000,
    "claude-3-opus": 200000,
    "claude-3-sonnet": 200000,
    "claude-3-haiku": 200000,
}

# ä¸Šä¸‹æ–‡ = è¾“å…¥ + è¾“å‡º
# è¾“å…¥ï¼šSystem Prompt + å†å²æ¶ˆæ¯ + ç”¨æˆ·è¾“å…¥ + RAG å†…å®¹
# è¾“å‡ºï¼šæ¨¡å‹ç”Ÿæˆçš„å›å¤

def check_context_fit(model: str, input_tokens: int, max_output: int) -> bool:
    """æ£€æŸ¥æ˜¯å¦è¶…å‡ºä¸Šä¸‹æ–‡çª—å£"""
    window = context_windows.get(model, 4096)
    return input_tokens + max_output <= window
```

**ä¸Šä¸‹æ–‡çª—å£åˆ†é…ç¤ºä¾‹ï¼š**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  128K Token çª—å£                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  System Prompt  â”‚  å†å²æ¶ˆæ¯  â”‚  RAG å†…å®¹  â”‚  è¾“å‡ºç©ºé—´  â”‚
â”‚     2K          â”‚    20K    â”‚    50K    â”‚    56K    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**åœ¨ LangChain æºç ä¸­çš„åº”ç”¨ï¼š**

```python
# langchain_openai/chat_models/base.py
class ChatOpenAI(BaseChatModel):
    model_name: str = "gpt-3.5-turbo"
    max_tokens: Optional[int] = None  # é™åˆ¶è¾“å‡º Token

    def _get_context_length(self) -> int:
        """è·å–æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£å¤§å°"""
        model_context_lengths = {
            "gpt-4": 8192,
            "gpt-4-turbo": 128000,
            "gpt-4o": 128000,
            "gpt-3.5-turbo": 16385,
        }
        return model_context_lengths.get(self.model_name, 4096)
```

---

### æ ¸å¿ƒæ¦‚å¿µ3ï¼šToken è®¡æ•°ä¸æˆæœ¬ ğŸ’°

**API æŒ‰ Token æ”¶è´¹ï¼Œç†è§£ Token æ‰èƒ½æ§åˆ¶æˆæœ¬**

```python
import tiktoken

def count_tokens(text: str, model: str = "gpt-4") -> int:
    """è®¡ç®—æ–‡æœ¬çš„ Token æ•°é‡"""
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

def estimate_cost(
    input_tokens: int,
    output_tokens: int,
    model: str = "gpt-4"
) -> float:
    """ä¼°ç®— API è°ƒç”¨æˆæœ¬"""
    # 2024å¹´ä»·æ ¼ï¼ˆç¾å…ƒ/1K tokensï¼‰
    pricing = {
        "gpt-4": {"input": 0.03, "output": 0.06},
        "gpt-4-turbo": {"input": 0.01, "output": 0.03},
        "gpt-4o": {"input": 0.005, "output": 0.015},
        "gpt-3.5-turbo": {"input": 0.0005, "output": 0.0015},
    }

    price = pricing.get(model, pricing["gpt-3.5-turbo"])
    input_cost = (input_tokens / 1000) * price["input"]
    output_cost = (output_tokens / 1000) * price["output"]

    return input_cost + output_cost

# ä½¿ç”¨ç¤ºä¾‹
prompt = "è¯·è¯¦ç»†è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼ŒåŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ "
input_tokens = count_tokens(prompt)
estimated_output = 500  # ä¼°è®¡è¾“å‡º 500 tokens

cost = estimate_cost(input_tokens, estimated_output, "gpt-4")
print(f"è¾“å…¥: {input_tokens} tokens")
print(f"é¢„è®¡æˆæœ¬: ${cost:.4f}")
```

**ä¸åŒåœºæ™¯çš„ Token æ¶ˆè€—ï¼š**

| åœºæ™¯ | è¾“å…¥ Token | è¾“å‡º Token | é¢„ä¼°æˆæœ¬ï¼ˆGPT-4ï¼‰ |
|------|-----------|------------|-----------------|
| ç®€å•é—®ç­” | 50 | 100 | $0.0075 |
| ä»£ç ç”Ÿæˆ | 200 | 500 | $0.036 |
| é•¿æ–‡æ¡£æ€»ç»“ | 10000 | 500 | $0.33 |
| RAG é—®ç­” | 5000 | 300 | $0.168 |

---

### æ ¸å¿ƒæ¦‚å¿µ4ï¼šæ¶ˆæ¯æˆªæ–­ä¸å†å²ç®¡ç† âœ‚ï¸

**å½“å¯¹è¯è¶…å‡ºä¸Šä¸‹æ–‡çª—å£æ—¶ï¼Œéœ€è¦ç­–ç•¥æ€§åœ°æˆªæ–­æˆ–æ€»ç»“**

```python
from typing import List
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage

def truncate_messages_by_token(
    messages: List[BaseMessage],
    max_tokens: int,
    tokenizer_fn
) -> List[BaseMessage]:
    """æŒ‰ Token æ•°é‡æˆªæ–­æ¶ˆæ¯ï¼ˆä¿ç•™æœ€è¿‘çš„ï¼‰"""
    total_tokens = 0
    truncated = []

    # ä»æœ€æ–°åˆ°æœ€æ—§éå†
    for msg in reversed(messages):
        msg_tokens = tokenizer_fn(msg.content)
        if total_tokens + msg_tokens > max_tokens:
            break
        truncated.insert(0, msg)
        total_tokens += msg_tokens

    return truncated

# ä½¿ç”¨ç¤ºä¾‹
messages = [
    HumanMessage(content="ä½ å¥½"),
    AIMessage(content="ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„ï¼Ÿ"),
    HumanMessage(content="è§£é‡Šä¸€ä¸‹Pythonè£…é¥°å™¨"),
    AIMessage(content="è£…é¥°å™¨æ˜¯...ï¼ˆé•¿å›å¤ï¼‰"),
    # ... æ›´å¤šæ¶ˆæ¯
]

truncated = truncate_messages_by_token(
    messages,
    max_tokens=2000,
    tokenizer_fn=lambda x: len(x) // 3  # ç®€åŒ–çš„ token è®¡æ•°
)
```

**LangChain ä¸­çš„ Memory ç­–ç•¥ï¼š**

```python
# 1. çª—å£è®°å¿†ï¼šä¿ç•™æœ€è¿‘ k æ¡æ¶ˆæ¯
from langchain.memory import ConversationBufferWindowMemory

memory = ConversationBufferWindowMemory(k=5)  # ä¿ç•™æœ€è¿‘5è½®

# 2. Token é™åˆ¶è®°å¿†ï¼šæŒ‰ Token æ•°æˆªæ–­
from langchain.memory import ConversationTokenBufferMemory

memory = ConversationTokenBufferMemory(
    llm=llm,
    max_token_limit=2000  # æœ€å¤š 2000 tokens
)

# 3. æ€»ç»“è®°å¿†ï¼šç”¨ LLM æ€»ç»“å†å²
from langchain.memory import ConversationSummaryMemory

memory = ConversationSummaryMemory(llm=llm)
# è‡ªåŠ¨å°†é•¿å¯¹è¯æ€»ç»“æˆç®€çŸ­æè¿°
```

---

### æ‰©å±•æ¦‚å¿µ5ï¼šmax_tokens å‚æ•° ğŸ›ï¸

**max_tokens æ§åˆ¶æ¨¡å‹ç”Ÿæˆçš„æœ€å¤§è¾“å‡ºé•¿åº¦**

```python
from langchain_openai import ChatOpenAI

# ä¸è®¾ç½® max_tokensï¼šæ¨¡å‹è‡ªåŠ¨å†³å®šè¾“å‡ºé•¿åº¦
llm_auto = ChatOpenAI(model="gpt-4")

# è®¾ç½® max_tokensï¼šé™åˆ¶è¾“å‡ºé•¿åº¦
llm_limited = ChatOpenAI(model="gpt-4", max_tokens=100)

# çŸ­å›ç­”åœºæ™¯
llm_short = ChatOpenAI(model="gpt-4", max_tokens=50)
response = llm_short.invoke("ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯ Python")
# è¾“å‡ºä¼šè¢«æˆªæ–­åœ¨çº¦ 50 tokens

# é•¿æ–‡ç« åœºæ™¯
llm_long = ChatOpenAI(model="gpt-4", max_tokens=4000)
response = llm_long.invoke("å†™ä¸€ç¯‡å…³äºæœºå™¨å­¦ä¹ çš„è¯¦ç»†æ–‡ç« ")
# å¯ä»¥ç”Ÿæˆæ›´é•¿çš„å†…å®¹
```

**max_tokens çš„ä½œç”¨ï¼š**

| å‚æ•°å€¼ | æ•ˆæœ | é€‚ç”¨åœºæ™¯ |
|--------|------|---------|
| ä¸è®¾ç½® | æ¨¡å‹è‡ªåŠ¨å†³å®š | é€šç”¨åœºæ™¯ |
| 50-100 | ç®€çŸ­å›ç­” | åˆ†ç±»ã€æ‘˜è¦ |
| 500-1000 | ä¸­ç­‰é•¿åº¦ | é—®ç­”ã€ç¿»è¯‘ |
| 2000-4000 | é•¿ç¯‡å†…å®¹ | æ–‡ç« ç”Ÿæˆã€ä»£ç  |

---

## 4. ã€æœ€å°å¯ç”¨ã€‘

æŒæ¡ä»¥ä¸‹å†…å®¹ï¼Œå°±èƒ½å¼€å§‹å¤„ç† Token ç›¸å…³çš„ LangChain å¼€å‘ï¼š

### 4.1 è®¡ç®— Token æ•°é‡

```python
import tiktoken

def count_tokens(text: str, model: str = "gpt-4") -> int:
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

# ä½¿ç”¨
tokens = count_tokens("Hello, how are you?")
print(f"Token æ•°: {tokens}")
```

### 4.2 æ£€æŸ¥ä¸Šä¸‹æ–‡çª—å£

```python
def check_fits_context(
    prompt: str,
    model: str = "gpt-4",
    max_output: int = 1000
) -> bool:
    """æ£€æŸ¥æ˜¯å¦è¶…å‡ºä¸Šä¸‹æ–‡çª—å£"""
    context_limits = {
        "gpt-4": 8192,
        "gpt-4-turbo": 128000,
        "gpt-3.5-turbo": 16385,
    }
    limit = context_limits.get(model, 4096)
    prompt_tokens = count_tokens(prompt, model)
    return prompt_tokens + max_output <= limit
```

### 4.3 è®¾ç½® max_tokens

```python
from langchain_openai import ChatOpenAI

# æ ¹æ®éœ€æ±‚è®¾ç½®
llm = ChatOpenAI(
    model="gpt-4",
    max_tokens=500  # é™åˆ¶è¾“å‡ºé•¿åº¦
)
```

### 4.4 ä½¿ç”¨ Token é™åˆ¶çš„ Memory

```python
from langchain.memory import ConversationTokenBufferMemory
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4")
memory = ConversationTokenBufferMemory(
    llm=llm,
    max_token_limit=2000
)
```

**è¿™äº›çŸ¥è¯†è¶³ä»¥ï¼š**
- ä¼°ç®— API è°ƒç”¨æˆæœ¬
- é¿å…ä¸Šä¸‹æ–‡çª—å£è¶…é™é”™è¯¯
- ç®¡ç†é•¿å¯¹è¯çš„å†å²æ¶ˆæ¯
- ä¼˜åŒ– RAG æ£€ç´¢å†…å®¹çš„å¤§å°

---

## 5. ã€1ä¸ªç±»æ¯”ã€‘ï¼ˆåŒè½¨åˆ¶ï¼‰

### ç±»æ¯”1ï¼šToken ä¸æ–‡æœ¬

#### ğŸ¨ å‰ç«¯è§†è§’ï¼šUTF-8 ç¼–ç  / å­—ç¬¦åˆ†è¯

Token å°±åƒæ˜¯ LLM ä¸“ç”¨çš„"å­—ç¬¦ç¼–ç "æ–¹å¼ã€‚

```javascript
// UTF-8ï¼šå°†å­—ç¬¦ç¼–ç ä¸ºå­—èŠ‚
const text = "Hello";
const utf8 = new TextEncoder().encode(text);
// [72, 101, 108, 108, 111]  // 5 å­—èŠ‚

// Tokenï¼šå°†æ–‡æœ¬ç¼–ç ä¸ºè¯­ä¹‰å•å…ƒ
const tokens = tokenizer.encode("Hello");
// [15496]  // 1 token
// "Hello"æ˜¯ä¸€ä¸ªå®Œæ•´çš„å¸¸è§è¯ï¼Œæ‰€ä»¥æ˜¯1ä¸ªtoken
```

```python
# Tokenizer ç¤ºä¾‹
import tiktoken

encoding = tiktoken.encoding_for_model("gpt-4")
tokens = encoding.encode("Hello")
print(tokens)  # [9906]
```

**å…³é”®åŒºåˆ«ï¼š** UTF-8 æŒ‰å­—ç¬¦ç¼–ç ï¼ŒToken æŒ‰"è¯­ä¹‰å•å…ƒ"ç¼–ç 

#### ğŸ§’ å°æœ‹å‹è§†è§’ï¼šä¹é«˜ç§¯æœ¨å—

Token å°±åƒä¹é«˜ç§¯æœ¨å—ï¼š

```
æ™®é€šæ–‡å­— = æ²™å­ï¼ˆå¤ªå°ï¼Œä¸å¥½ç”¨ï¼‰
å•è¯ = å¤§çŸ³å¤´ï¼ˆå¤ªå¤§ï¼Œä¸çµæ´»ï¼‰
Token = ä¹é«˜ç§¯æœ¨ï¼ˆå¤§å°åˆšå¥½ï¼ï¼‰

"Hello World" å˜æˆä¹é«˜ï¼š
[Helloç§¯æœ¨] [Worldç§¯æœ¨]  â†’ 2å—ç§¯æœ¨

"ä½ å¥½ä¸–ç•Œ" å˜æˆä¹é«˜ï¼š
[ä½ ] [å¥½] [ä¸–] [ç•Œ]  â†’ 4å—ç§¯æœ¨
ï¼ˆä¸­æ–‡æ¯ä¸ªå­—æ˜¯ä¸€å—ç§¯æœ¨ï¼‰
```

**ç”Ÿæ´»ä¾‹å­ï¼š**
```
æƒ³è±¡ä½ åœ¨ç”¨ä¹é«˜æ­æˆ¿å­ï¼š
- å¤ªå°çš„ç§¯æœ¨ï¼ˆæ²™å­ï¼‰ï¼šæ•°é‡å¤ªå¤šï¼Œæ­ä¸å®Œ
- å¤ªå¤§çš„ç§¯æœ¨ï¼ˆæ•´å—å¢™ï¼‰ï¼šä¸å¤Ÿçµæ´»ï¼Œæ²¡æ³•æ­çª—æˆ·
- åˆšå¥½çš„ç§¯æœ¨ï¼ˆä¹é«˜ï¼‰ï¼šæ•°é‡åˆé€‚ï¼Œæƒ³æ­ä»€ä¹ˆéƒ½è¡Œ

LLM ç”¨ Token å°±åƒç”¨ä¹é«˜ï¼š
- ä¸ç”¨ä¸€ä¸ªå­—ä¸€ä¸ªå­—å¤„ç†ï¼ˆå¤ªæ…¢ï¼‰
- ä¸ç”¨ä¸€ä¸ªè¯ä¸€ä¸ªè¯å¤„ç†ï¼ˆè¯å¤ªå¤šï¼‰
- ç”¨ Token å¤„ç†ï¼ˆåˆšåˆšå¥½ï¼ï¼‰
```

---

### ç±»æ¯”2ï¼šä¸Šä¸‹æ–‡çª—å£

#### ğŸ¨ å‰ç«¯è§†è§’ï¼šAPI è¯·æ±‚ä½“å¤§å°é™åˆ¶

ä¸Šä¸‹æ–‡çª—å£å°±åƒ HTTP è¯·æ±‚ä½“çš„å¤§å°é™åˆ¶ã€‚

```javascript
// HTTP è¯·æ±‚ä½“é™åˆ¶
const MAX_BODY_SIZE = 1024 * 1024; // 1MB

// å¦‚æœè¯·æ±‚ä½“å¤ªå¤§
fetch('/api/data', {
  method: 'POST',
  body: JSON.stringify(hugeData)  // è¶…è¿‡ 1MB â†’ 413 é”™è¯¯
});

// éœ€è¦åˆ†é¡µæˆ–å‹ç¼©
const chunks = splitIntoChunks(hugeData, MAX_BODY_SIZE);
for (const chunk of chunks) {
  await fetch('/api/data', { body: chunk });
}
```

```python
# LLM ä¸Šä¸‹æ–‡çª—å£é™åˆ¶
MAX_CONTEXT = 128000  # GPT-4 Turbo çš„é™åˆ¶

# å¦‚æœè¾“å…¥å¤ªé•¿
if count_tokens(prompt) > MAX_CONTEXT:
    # éœ€è¦æˆªæ–­æˆ–åˆ†å—
    prompt = truncate_to_tokens(prompt, MAX_CONTEXT - 1000)
```

#### ğŸ§’ å°æœ‹å‹è§†è§’ï¼šä¹¦åŒ…å®¹é‡

ä¸Šä¸‹æ–‡çª—å£å°±åƒä¹¦åŒ…çš„å®¹é‡ï¼š

```
ä½ çš„ä¹¦åŒ…ï¼ˆGPT-4ï¼‰ï¼šèƒ½è£… 128000 æœ¬å°ä¹¦ï¼ˆTokenï¼‰

å¦‚æœä½ æƒ³å¸¦ï¼š
- è¯­æ–‡ä¹¦ï¼ˆSystem Promptï¼‰ï¼š2000æœ¬
- ä½œä¸šæœ¬ï¼ˆå†å²æ¶ˆæ¯ï¼‰ï¼š10000æœ¬
- å‚è€ƒèµ„æ–™ï¼ˆRAG å†…å®¹ï¼‰ï¼š50000æœ¬
- é“…ç¬”ç›’ï¼ˆç”¨æˆ·è¾“å…¥ï¼‰ï¼š1000æœ¬
- è¿˜è¦ç•™ç©ºé—´ç»™æ–°ä¸œè¥¿ï¼ˆè¾“å‡ºï¼‰ï¼š65000æœ¬

æ€»å…±ï¼š128000æœ¬ = åˆšå¥½è£…æ»¡ï¼

å¦‚æœè¶…è¿‡128000æœ¬ â†’ ä¹¦åŒ…è£…ä¸ä¸‹ï¼Œä¼šæœ‰ä¹¦æ‰å‡ºæ¥ï¼
```

**ç”Ÿæ´»ä¾‹å­ï¼š**
```
æƒ³è±¡ä½ åœ¨çœ‹ä¸€æœ¬ä¹¦ï¼š
- çŸ­æœŸè®°å¿†ï¼ˆä¸Šä¸‹æ–‡çª—å£ï¼‰ï¼šä½ èƒ½åŒæ—¶è®°ä½çš„å†…å®¹
- å¦‚æœä¹¦æœ‰1000é¡µï¼Œä½ åªèƒ½åŒæ—¶è®°ä½200é¡µ
- ä¹‹å‰çœ‹çš„å†…å®¹ä¼šæ…¢æ…¢å¿˜è®°

LLM ä¹Ÿæ˜¯è¿™æ ·ï¼š
- ä¸Šä¸‹æ–‡çª—å£ = LLM çš„"çŸ­æœŸè®°å¿†"
- è¶…è¿‡çª—å£çš„å†…å®¹ = LLM çœ‹ä¸åˆ°äº†
```

---

### ç±»æ¯”3ï¼šToken æˆæœ¬è®¡ç®—

#### ğŸ¨ å‰ç«¯è§†è§’ï¼šäº‘æœåŠ¡æŒ‰è¯·æ±‚è®¡è´¹

Token è®¡è´¹å°±åƒäº‘å‡½æ•°æŒ‰è°ƒç”¨æ¬¡æ•°è®¡è´¹ã€‚

```javascript
// AWS Lambda è®¡è´¹
// æ¯ 100ä¸‡æ¬¡è°ƒç”¨ $0.20
// æ¯ GB-ç§’ $0.0000166667

function estimateLambdaCost(invocations, memory, duration) {
  const invocationCost = invocations * 0.0000002;
  const computeCost = memory * duration * 0.0000166667;
  return invocationCost + computeCost;
}
```

```python
# LLM Token è®¡è´¹
# GPT-4: $0.03/1K input, $0.06/1K output

def estimate_llm_cost(input_tokens, output_tokens):
    input_cost = (input_tokens / 1000) * 0.03
    output_cost = (output_tokens / 1000) * 0.06
    return input_cost + output_cost

# 1000 è¾“å…¥ + 500 è¾“å‡º = $0.03 + $0.03 = $0.06
```

#### ğŸ§’ å°æœ‹å‹è§†è§’ï¼šä¹°é›¶é£ŸæŒ‰ä¸ªæ•°è®¡è´¹

Token è®¡è´¹å°±åƒä¹°ç³–æœï¼š

```
ç³–æœåº—ï¼ˆOpenAIï¼‰çš„ä»·æ ¼ï¼š
- ä½ ç»™åº—å‘˜çœ‹çš„ç³–æœå›¾ç‰‡ï¼ˆè¾“å…¥ï¼‰ï¼šæ¯1000å¼  3åˆ†é’±
- åº—å‘˜ç»™ä½ çš„ç³–æœï¼ˆè¾“å‡ºï¼‰ï¼šæ¯1000é¢— 6åˆ†é’±

ä½ çš„è´­ç‰©ï¼š
- çœ‹äº†100å¼ å›¾ç‰‡ï¼ˆè¾“å…¥100 tokensï¼‰ï¼š0.3åˆ†
- æ‹¿äº†50é¢—ç³–ï¼ˆè¾“å‡º50 tokensï¼‰ï¼š0.3åˆ†
- æ€»å…±ï¼š0.6åˆ†é’±

å¦‚æœä½ çœ‹äº†10000å¼ å›¾ç‰‡ï¼Œæ‹¿äº†5000é¢—ç³–ï¼š
- è¾“å…¥ï¼š30åˆ† = 3æ¯›é’±
- è¾“å‡ºï¼š30åˆ† = 3æ¯›é’±
- æ€»å…±ï¼š6æ¯›é’±
```

---

### ç±»æ¯”4ï¼šæ¶ˆæ¯æˆªæ–­ç­–ç•¥

#### ğŸ¨ å‰ç«¯è§†è§’ï¼šæ— é™æ»šåŠ¨ vs åˆ†é¡µ

æ¶ˆæ¯æˆªæ–­å°±åƒå¤„ç†é•¿åˆ—è¡¨çš„ç­–ç•¥ã€‚

```javascript
// ç­–ç•¥1ï¼šçª—å£æˆªæ–­ï¼ˆä¿ç•™æœ€è¿‘Næ¡ï¼‰
// ç±»ä¼¼ï¼šæ— é™æ»šåŠ¨åªæ¸²æŸ“å¯è§†åŒºåŸŸ
const messages = allMessages.slice(-10);  // ä¿ç•™æœ€è¿‘10æ¡

// ç­–ç•¥2ï¼šToken é™åˆ¶æˆªæ–­
// ç±»ä¼¼ï¼šå“åº”åˆ†é¡µï¼Œé™åˆ¶æ¯é¡µæ•°æ®é‡
function paginateBySize(data, maxSize) {
  let result = [];
  let currentSize = 0;
  for (const item of data.reverse()) {
    if (currentSize + item.size > maxSize) break;
    result.unshift(item);
    currentSize += item.size;
  }
  return result;
}

// ç­–ç•¥3ï¼šæ€»ç»“å‹ç¼©
// ç±»ä¼¼ï¼šæ•°æ®èšåˆï¼Œå°†æ˜ç»†å‹ç¼©æˆæ‘˜è¦
const summary = data.reduce((acc, item) => ({
  count: acc.count + 1,
  total: acc.total + item.value
}), { count: 0, total: 0 });
```

#### ğŸ§’ å°æœ‹å‹è§†è§’ï¼šæ—¥è®°æœ¬

æ¶ˆæ¯æˆªæ–­å°±åƒç®¡ç†ä½ çš„æ—¥è®°æœ¬ï¼š

```
æ—¥è®°æœ¬åªæœ‰100é¡µï¼ˆä¸Šä¸‹æ–‡çª—å£ï¼‰

ç­–ç•¥1ï¼šåªç•™æœ€è¿‘çš„
- å†™æ»¡100é¡µåï¼Œæ’•æ‰æœ€æ—©çš„10é¡µ
- å†™æ–°çš„10é¡µ
- æ°¸è¿œä¿æŒ100é¡µ

ç­–ç•¥2ï¼šå‹ç¼©æ—§å†…å®¹
- æŠŠå‰50é¡µçš„å†…å®¹æ€»ç»“æˆ5é¡µ
- è…¾å‡º45é¡µç©ºé—´
- è¯¦ç»†å†…å®¹ â†’ ç²¾ç®€æ‘˜è¦

ç­–ç•¥3ï¼šæŒ‰é‡è¦æ€§ä¿ç•™
- é‡è¦çš„æ—¥è®°æ‰“æ˜Ÿå·
- ç©ºé—´ä¸å¤Ÿæ—¶ï¼Œå…ˆåˆ é™¤æ²¡æœ‰æ˜Ÿå·çš„
```

---

### ç±»æ¯”æ€»ç»“è¡¨

| Token æ¦‚å¿µ | å‰ç«¯ç±»æ¯” | å°æœ‹å‹ç±»æ¯” |
|-----------|---------|-----------|
| Token | UTF-8 å­—ç¬¦ç¼–ç  | ä¹é«˜ç§¯æœ¨å— |
| ä¸Šä¸‹æ–‡çª—å£ | è¯·æ±‚ä½“å¤§å°é™åˆ¶ | ä¹¦åŒ…å®¹é‡ |
| Token è®¡è´¹ | äº‘æœåŠ¡æŒ‰é‡è®¡è´¹ | ç³–æœæŒ‰ä¸ªæ•°ä¹° |
| æ¶ˆæ¯æˆªæ–­ | æ— é™æ»šåŠ¨è™šæ‹ŸåŒ– | æ—¥è®°æœ¬é¡µæ•°ç®¡ç† |
| max_tokens | å“åº”é™é€Ÿ | ä½œæ–‡å­—æ•°é™åˆ¶ |
| Tokenizer | å­—ç¬¦é›†ç¼–ç å™¨ | ç¿»è¯‘å®˜ |

---

## 6. ã€åç›´è§‰ç‚¹ã€‘

### è¯¯åŒº1ï¼šä¸€ä¸ªä¸­æ–‡å­— = ä¸€ä¸ª Token âŒ

**ä¸ºä»€ä¹ˆé”™ï¼Ÿ**
- ä¸åŒ Tokenizer å¯¹ä¸­æ–‡çš„å¤„ç†ä¸åŒ
- å¸¸è§çš„ä¸­æ–‡è¯å¯èƒ½æ˜¯ 1 ä¸ª Tokenï¼Œç”Ÿåƒ»å­—å¯èƒ½æ˜¯å¤šä¸ª
- å®é™… Token æ•°éœ€è¦ç”¨ Tokenizer è®¡ç®—

**ä¸ºä»€ä¹ˆäººä»¬å®¹æ˜“è¿™æ ·é”™ï¼Ÿ**
è‹±æ–‡ä¸­ä¸€ä¸ªå¸¸è§è¯çº¦ç­‰äº 1 Tokenï¼Œäººä»¬ç®€å•ç±»æ¨åˆ°ä¸­æ–‡ã€‚ä½†ä¸­æ–‡çš„ Token åŒ–ç­–ç•¥å¤æ‚å¾—å¤šã€‚

**æ­£ç¡®ç†è§£ï¼š**

```python
import tiktoken

encoding = tiktoken.encoding_for_model("gpt-4")

# æµ‹è¯•ä¸åŒä¸­æ–‡æ–‡æœ¬
texts = [
    "ä½ å¥½",        # 2å­—
    "æœºå™¨å­¦ä¹ ",    # 4å­—
    "äººå·¥æ™ºèƒ½",    # 4å­—
    "LangChain",   # è‹±æ–‡
]

for text in texts:
    tokens = encoding.encode(text)
    print(f"'{text}' â†’ {len(tokens)} tokens (å­—ç¬¦æ•°: {len(text)})")

# è¾“å‡ºç¤ºä¾‹ï¼š
# 'ä½ å¥½' â†’ 2 tokens (å­—ç¬¦æ•°: 2)
# 'æœºå™¨å­¦ä¹ ' â†’ 4 tokens (å­—ç¬¦æ•°: 4)
# 'äººå·¥æ™ºèƒ½' â†’ 3 tokens (å­—ç¬¦æ•°: 4)  # å¸¸è§è¯å¯èƒ½æ›´å°‘
# 'LangChain' â†’ 2 tokens (å­—ç¬¦æ•°: 9)
```

**ç»éªŒæ³•åˆ™ï¼š** ä¸­æ–‡çº¦ 1.5-2 å­—ç¬¦/Tokenï¼Œè‹±æ–‡çº¦ 4 å­—ç¬¦/Token

---

### è¯¯åŒº2ï¼šä¸Šä¸‹æ–‡çª—å£è¶Šå¤§è¶Šå¥½ âŒ

**ä¸ºä»€ä¹ˆé”™ï¼Ÿ**
- å¤§çª—å£ = æ›´é«˜çš„æˆæœ¬ï¼ˆæŒ‰ Token è®¡è´¹ï¼‰
- å¤§çª—å£ = æ›´é•¿çš„å»¶è¿Ÿï¼ˆå¤„ç†æ—¶é—´å¢åŠ ï¼‰
- LLM å¯¹è¶…é•¿ä¸Šä¸‹æ–‡çš„"æ³¨æ„åŠ›"ä¸å‡åŒ€ï¼ˆå¯èƒ½å¿½ç•¥ä¸­é—´å†…å®¹ï¼‰

**ä¸ºä»€ä¹ˆäººä»¬å®¹æ˜“è¿™æ ·é”™ï¼Ÿ**
"è¶Šå¤§è¶Šå¥½"æ˜¯ç›´è§‰æ€ç»´ã€‚ä½†å®é™…ä¸Šï¼Œ128K çª—å£çš„æ¨¡å‹åœ¨å¤„ç† 128K å†…å®¹æ—¶ï¼Œå¯¹ä¸­é—´éƒ¨åˆ†çš„"è®°å¿†"å¯èƒ½å¾ˆå¼±ï¼ˆ"Lost in the Middle"é—®é¢˜ï¼‰ã€‚

**æ­£ç¡®ç†è§£ï¼š**

```python
# âŒ é”™è¯¯ï¼šå¡æ»¡æ•´ä¸ªä¸Šä¸‹æ–‡çª—å£
context = load_all_documents()  # 100K tokens
response = llm.invoke(context + question)
# é—®é¢˜ï¼šä¸­é—´çš„å†…å®¹å¯èƒ½è¢«"å¿½ç•¥"

# âœ… æ­£ç¡®ï¼šåªæ”¾æœ€ç›¸å…³çš„å†…å®¹
relevant_docs = retriever.invoke(question)[:5]  # 5ä¸ªç›¸å…³æ–‡æ¡£
context = format_docs(relevant_docs)  # ~5K tokens
response = llm.invoke(context + question)
# æ›´ç²¾å‡†ï¼Œæ›´ä¾¿å®œï¼Œæ›´å¿«

# æœ€ä½³å®è·µï¼šæŒ‰é‡è¦æ€§åˆ†å±‚
# 1. System Promptï¼ˆå§‹ç»ˆä¿ç•™ï¼‰
# 2. æœ€ç›¸å…³çš„æ£€ç´¢å†…å®¹ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
# 3. æœ€è¿‘çš„å¯¹è¯å†å²ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰
# 4. è¾ƒæ—©çš„å†å²æ‘˜è¦ï¼ˆä½ä¼˜å…ˆçº§ï¼‰
```

**ç»éªŒæ³•åˆ™ï¼š** å®å¯ç²¾é€‰ 5K æœ‰æ•ˆå†…å®¹ï¼Œä¸è¦å †ç Œ 50K å™ªéŸ³

---

### è¯¯åŒº3ï¼šè¾“å‡º Token å’Œè¾“å…¥ Token æˆæœ¬ä¸€æ · âŒ

**ä¸ºä»€ä¹ˆé”™ï¼Ÿ**
- è¾“å‡º Token é€šå¸¸æ¯”è¾“å…¥ Token è´µ 2-3 å€
- ç”Ÿæˆè¾“å‡ºéœ€è¦æ›´å¤šè®¡ç®—èµ„æº
- æ§åˆ¶è¾“å‡ºé•¿åº¦å¯ä»¥æ˜¾è‘—é™ä½æˆæœ¬

**ä¸ºä»€ä¹ˆäººä»¬å®¹æ˜“è¿™æ ·é”™ï¼Ÿ**
è®¡è´¹æ¨¡å¼ä¸é€æ˜ï¼Œå¾ˆå¤šäººåªå…³æ³¨è¾“å…¥ï¼Œå¿½ç•¥äº†è¾“å‡ºæˆæœ¬ã€‚

**æ­£ç¡®ç†è§£ï¼š**

```python
# GPT-4 å®šä»·ï¼ˆ2024ï¼‰
# è¾“å…¥ï¼š$0.03 / 1K tokens
# è¾“å‡ºï¼š$0.06 / 1K tokensï¼ˆè´µ2å€ï¼ï¼‰

# åœºæ™¯å¯¹æ¯”
# åœºæ™¯1ï¼šé•¿é—®é¢˜ï¼ŒçŸ­å›ç­”
input_tokens = 5000  # $0.15
output_tokens = 100  # $0.006
total = 0.156  # æˆæœ¬ä¸»è¦åœ¨è¾“å…¥

# åœºæ™¯2ï¼šçŸ­é—®é¢˜ï¼Œé•¿å›ç­”
input_tokens = 100   # $0.003
output_tokens = 5000 # $0.30
total = 0.303  # æˆæœ¬ä¸»è¦åœ¨è¾“å‡ºï¼

# ä¼˜åŒ–ç­–ç•¥
llm = ChatOpenAI(
    model="gpt-4",
    max_tokens=500  # é™åˆ¶è¾“å‡ºï¼Œæ§åˆ¶æˆæœ¬
)
```

**ç»éªŒæ³•åˆ™ï¼š** è¾“å‡ºæˆæœ¬å¸¸è¢«ä½ä¼°ï¼Œç”¨ max_tokens æ§åˆ¶

---

## 7. ã€å®æˆ˜ä»£ç ã€‘

```python
"""
ç¤ºä¾‹ï¼šToken è®¡æ•°ä¸ä¸Šä¸‹æ–‡çª—å£ç®¡ç†
æ¼”ç¤º LangChain ä¸­ Token ç›¸å…³çš„æ ¸å¿ƒæ“ä½œ
"""

from typing import List, Dict
import re

# ===== 1. æ¨¡æ‹Ÿ Tokenizer =====
print("=== 1. Token è®¡æ•° ===")

class SimpleTokenizer:
    """ç®€åŒ–çš„ Tokenizer å®ç°ï¼ˆæ¼”ç¤ºç”¨ï¼‰"""

    def __init__(self):
        # ç®€åŒ–çš„è¯æ±‡è¡¨
        self.vocab = {}
        self.next_id = 0

    def encode(self, text: str) -> List[int]:
        """æ–‡æœ¬ â†’ Token IDs"""
        # ç®€åŒ–ï¼šæŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹åˆ†å‰²
        tokens = re.findall(r'\w+|[^\w\s]', text)
        ids = []
        for token in tokens:
            if token not in self.vocab:
                self.vocab[token] = self.next_id
                self.next_id += 1
            ids.append(self.vocab[token])
        return ids

    def decode(self, ids: List[int]) -> str:
        """Token IDs â†’ æ–‡æœ¬"""
        reverse_vocab = {v: k for k, v in self.vocab.items()}
        tokens = [reverse_vocab.get(i, '<UNK>') for i in ids]
        return ' '.join(tokens)

    def count_tokens(self, text: str) -> int:
        """è®¡ç®— Token æ•°é‡"""
        return len(self.encode(text))

tokenizer = SimpleTokenizer()

# æµ‹è¯• Token åŒ–
texts = [
    "Hello, World!",
    "LangChain is a framework for LLM applications.",
    "ä½ å¥½ä¸–ç•Œ",  # ç®€åŒ–å¤„ç†
]

for text in texts:
    tokens = tokenizer.encode(text)
    print(f"'{text}'")
    print(f"  Tokenæ•°: {len(tokens)}")
    print(f"  Token IDs: {tokens}")
    print()

# ===== 2. ä¸Šä¸‹æ–‡çª—å£æ£€æŸ¥ =====
print("=== 2. ä¸Šä¸‹æ–‡çª—å£æ£€æŸ¥ ===")

class ContextWindowManager:
    """ä¸Šä¸‹æ–‡çª—å£ç®¡ç†å™¨"""

    # æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£å¤§å°
    CONTEXT_LIMITS = {
        "gpt-4": 8192,
        "gpt-4-turbo": 128000,
        "gpt-3.5-turbo": 16385,
        "claude-3": 200000,
    }

    def __init__(self, model: str, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.context_limit = self.CONTEXT_LIMITS.get(model, 4096)

    def check_fits(self, text: str, reserved_output: int = 1000) -> Dict:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦é€‚åˆä¸Šä¸‹æ–‡çª—å£"""
        input_tokens = self.tokenizer.count_tokens(text)
        available = self.context_limit - reserved_output
        fits = input_tokens <= available

        return {
            "fits": fits,
            "input_tokens": input_tokens,
            "context_limit": self.context_limit,
            "reserved_output": reserved_output,
            "available": available,
            "overflow": max(0, input_tokens - available)
        }

    def truncate_to_fit(self, text: str, reserved_output: int = 1000) -> str:
        """æˆªæ–­æ–‡æœ¬ä»¥é€‚åº”ä¸Šä¸‹æ–‡çª—å£"""
        check = self.check_fits(text, reserved_output)
        if check["fits"]:
            return text

        # ç®€åŒ–æˆªæ–­ï¼šæŒ‰æ¯”ä¾‹æˆªå–
        ratio = check["available"] / check["input_tokens"]
        target_len = int(len(text) * ratio * 0.9)  # ç•™10%ä½™é‡
        return text[:target_len] + "..."

# ä½¿ç”¨ç¤ºä¾‹
manager = ContextWindowManager("gpt-4", tokenizer)

# æµ‹è¯•æ–‡æœ¬
short_text = "What is Python?"
long_text = "Python " * 5000  # æ¨¡æ‹Ÿé•¿æ–‡æœ¬

print(f"çŸ­æ–‡æœ¬æ£€æŸ¥: {manager.check_fits(short_text)}")
print(f"é•¿æ–‡æœ¬æ£€æŸ¥: {manager.check_fits(long_text)}")

truncated = manager.truncate_to_fit(long_text)
print(f"æˆªæ–­åé•¿åº¦: {len(truncated)} å­—ç¬¦")

# ===== 3. æ¶ˆæ¯å†å²ç®¡ç† =====
print("\n=== 3. æ¶ˆæ¯å†å²ç®¡ç† ===")

class Message:
    """ç®€åŒ–çš„æ¶ˆæ¯ç±»"""
    def __init__(self, role: str, content: str):
        self.role = role
        self.content = content

class ConversationTokenBuffer:
    """åŸºäº Token é™åˆ¶çš„å¯¹è¯ç¼“å†²åŒº"""

    def __init__(self, tokenizer, max_tokens: int = 2000):
        self.tokenizer = tokenizer
        self.max_tokens = max_tokens
        self.messages: List[Message] = []

    def add_message(self, role: str, content: str):
        """æ·»åŠ æ¶ˆæ¯"""
        self.messages.append(Message(role, content))
        self._truncate_if_needed()

    def _truncate_if_needed(self):
        """å¦‚æœè¶…è¿‡ Token é™åˆ¶ï¼Œä»æœ€æ—©çš„æ¶ˆæ¯å¼€å§‹åˆ é™¤"""
        while self._total_tokens() > self.max_tokens and len(self.messages) > 1:
            self.messages.pop(0)

    def _total_tokens(self) -> int:
        """è®¡ç®—æ€» Token æ•°"""
        return sum(
            self.tokenizer.count_tokens(m.content)
            for m in self.messages
        )

    def get_messages(self) -> List[Dict]:
        """è·å–æ¶ˆæ¯åˆ—è¡¨"""
        return [
            {"role": m.role, "content": m.content}
            for m in self.messages
        ]

    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "message_count": len(self.messages),
            "total_tokens": self._total_tokens(),
            "max_tokens": self.max_tokens
        }

# ä½¿ç”¨ç¤ºä¾‹
buffer = ConversationTokenBuffer(tokenizer, max_tokens=100)

# æ¨¡æ‹Ÿå¯¹è¯
conversations = [
    ("user", "Hello!"),
    ("assistant", "Hi! How can I help you today?"),
    ("user", "Can you explain Python decorators?"),
    ("assistant", "Decorators are functions that modify the behavior of other functions. They use the @decorator syntax."),
    ("user", "Can you give me an example?"),
    ("assistant", "Sure! Here's a simple example of a decorator that prints when a function is called..."),
]

for role, content in conversations:
    buffer.add_message(role, content)
    stats = buffer.get_stats()
    print(f"æ·»åŠ : [{role}] {content[:30]}...")
    print(f"  çŠ¶æ€: {stats}")

print(f"\næœ€ç»ˆä¿ç•™çš„æ¶ˆæ¯:")
for msg in buffer.get_messages():
    print(f"  [{msg['role']}] {msg['content'][:40]}...")

# ===== 4. æˆæœ¬ä¼°ç®—å™¨ =====
print("\n=== 4. æˆæœ¬ä¼°ç®—å™¨ ===")

class CostEstimator:
    """API æˆæœ¬ä¼°ç®—å™¨"""

    # å®šä»·ï¼ˆç¾å…ƒ/1K tokensï¼‰
    PRICING = {
        "gpt-4": {"input": 0.03, "output": 0.06},
        "gpt-4-turbo": {"input": 0.01, "output": 0.03},
        "gpt-4o": {"input": 0.005, "output": 0.015},
        "gpt-3.5-turbo": {"input": 0.0005, "output": 0.0015},
    }

    def __init__(self, model: str = "gpt-4"):
        self.model = model
        self.pricing = self.PRICING.get(model, self.PRICING["gpt-4"])

    def estimate(self, input_tokens: int, output_tokens: int) -> Dict:
        """ä¼°ç®—æˆæœ¬"""
        input_cost = (input_tokens / 1000) * self.pricing["input"]
        output_cost = (output_tokens / 1000) * self.pricing["output"]
        total = input_cost + output_cost

        return {
            "model": self.model,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "input_cost": f"${input_cost:.4f}",
            "output_cost": f"${output_cost:.4f}",
            "total_cost": f"${total:.4f}"
        }

    def compare_models(self, input_tokens: int, output_tokens: int) -> List[Dict]:
        """æ¯”è¾ƒä¸åŒæ¨¡å‹çš„æˆæœ¬"""
        results = []
        for model in self.PRICING.keys():
            estimator = CostEstimator(model)
            results.append(estimator.estimate(input_tokens, output_tokens))
        return sorted(results, key=lambda x: float(x["total_cost"][1:]))

# ä½¿ç”¨ç¤ºä¾‹
estimator = CostEstimator("gpt-4")

# å•æ¬¡ä¼°ç®—
result = estimator.estimate(input_tokens=1000, output_tokens=500)
print(f"å•æ¬¡è°ƒç”¨æˆæœ¬ä¼°ç®—:")
for k, v in result.items():
    print(f"  {k}: {v}")

# æ¨¡å‹æ¯”è¾ƒ
print(f"\nä¸åŒæ¨¡å‹æˆæœ¬æ¯”è¾ƒ (1000 è¾“å…¥ + 500 è¾“å‡º):")
comparison = estimator.compare_models(1000, 500)
for r in comparison:
    print(f"  {r['model']}: {r['total_cost']}")

# ===== 5. RAG ä¸Šä¸‹æ–‡åˆ†é… =====
print("\n=== 5. RAG ä¸Šä¸‹æ–‡åˆ†é… ===")

class RAGContextAllocator:
    """RAG ä¸Šä¸‹æ–‡åˆ†é…å™¨"""

    def __init__(self, tokenizer, context_limit: int = 8192):
        self.tokenizer = tokenizer
        self.context_limit = context_limit

    def allocate(
        self,
        system_prompt: str,
        retrieved_docs: List[str],
        user_query: str,
        reserved_output: int = 1000
    ) -> Dict:
        """åˆ†é…ä¸Šä¸‹æ–‡ç©ºé—´"""
        # è®¡ç®—å›ºå®šéƒ¨åˆ†
        system_tokens = self.tokenizer.count_tokens(system_prompt)
        query_tokens = self.tokenizer.count_tokens(user_query)
        fixed_tokens = system_tokens + query_tokens + reserved_output

        # è®¡ç®—å¯ç”¨äºæ–‡æ¡£çš„ç©ºé—´
        available_for_docs = self.context_limit - fixed_tokens

        # é€‰æ‹©èƒ½æ”¾å…¥çš„æ–‡æ¡£
        selected_docs = []
        used_tokens = 0

        for doc in retrieved_docs:
            doc_tokens = self.tokenizer.count_tokens(doc)
            if used_tokens + doc_tokens <= available_for_docs:
                selected_docs.append(doc)
                used_tokens += doc_tokens
            else:
                break

        return {
            "context_limit": self.context_limit,
            "system_tokens": system_tokens,
            "query_tokens": query_tokens,
            "reserved_output": reserved_output,
            "available_for_docs": available_for_docs,
            "docs_selected": len(selected_docs),
            "docs_total": len(retrieved_docs),
            "docs_tokens": used_tokens,
            "total_used": fixed_tokens + used_tokens,
            "remaining": self.context_limit - (fixed_tokens + used_tokens)
        }

# ä½¿ç”¨ç¤ºä¾‹
allocator = RAGContextAllocator(tokenizer, context_limit=1000)

system_prompt = "You are a helpful assistant. Answer based on the provided context."
retrieved_docs = [
    "Document 1: Python is a programming language...",
    "Document 2: LangChain is a framework for LLM...",
    "Document 3: Machine learning is a subset of AI...",
    "Document 4: Deep learning uses neural networks...",
]
user_query = "What is LangChain?"

allocation = allocator.allocate(system_prompt, retrieved_docs, user_query)

print("RAG ä¸Šä¸‹æ–‡åˆ†é…:")
for k, v in allocation.items():
    print(f"  {k}: {v}")

print("\n=== å®Œæˆï¼===")
```

**è¿è¡Œè¾“å‡ºç¤ºä¾‹ï¼š**
```
=== 1. Token è®¡æ•° ===
'Hello, World!'
  Tokenæ•°: 4
  Token IDs: [0, 1, 2, 3]

'LangChain is a framework for LLM applications.'
  Tokenæ•°: 8
  Token IDs: [4, 5, 6, 7, 8, 9, 10, 11]

'ä½ å¥½ä¸–ç•Œ'
  Tokenæ•°: 1
  Token IDs: [12]

=== 2. ä¸Šä¸‹æ–‡çª—å£æ£€æŸ¥ ===
çŸ­æ–‡æœ¬æ£€æŸ¥: {'fits': True, 'input_tokens': 4, 'context_limit': 8192, ...}
é•¿æ–‡æœ¬æ£€æŸ¥: {'fits': False, 'input_tokens': 9999, 'overflow': 2807, ...}
æˆªæ–­åé•¿åº¦: 18432 å­—ç¬¦

=== 3. æ¶ˆæ¯å†å²ç®¡ç† ===
æ·»åŠ : [user] Hello!...
  çŠ¶æ€: {'message_count': 1, 'total_tokens': 2, 'max_tokens': 100}
...
æœ€ç»ˆä¿ç•™çš„æ¶ˆæ¯:
  [assistant] Sure! Here's a simple example...

=== 4. æˆæœ¬ä¼°ç®—å™¨ ===
å•æ¬¡è°ƒç”¨æˆæœ¬ä¼°ç®—:
  model: gpt-4
  input_tokens: 1000
  output_tokens: 500
  total_cost: $0.0600

ä¸åŒæ¨¡å‹æˆæœ¬æ¯”è¾ƒ:
  gpt-3.5-turbo: $0.0013
  gpt-4o: $0.0125
  gpt-4-turbo: $0.0250
  gpt-4: $0.0600

=== 5. RAG ä¸Šä¸‹æ–‡åˆ†é… ===
RAG ä¸Šä¸‹æ–‡åˆ†é…:
  context_limit: 1000
  docs_selected: 2
  docs_total: 4
  remaining: 812

=== å®Œæˆï¼===
```

---

## 8. ã€é¢è¯•å¿…é—®ã€‘

### é—®é¢˜ï¼š"ä»€ä¹ˆæ˜¯ Tokenï¼Ÿä¸ºä»€ä¹ˆ LLM ä½¿ç”¨ Token è€Œä¸æ˜¯å­—ç¬¦æˆ–å•è¯ï¼Ÿ"

**æ™®é€šå›ç­”ï¼ˆâŒ ä¸å‡ºå½©ï¼‰ï¼š**
"Token å°±æ˜¯æŠŠæ–‡æœ¬åˆ‡æˆå°å—ï¼Œæ¯”å­—ç¬¦å¤§ï¼Œæ¯”å•è¯å°ã€‚"

**å‡ºå½©å›ç­”ï¼ˆâœ… æ¨èï¼‰ï¼š**

> **Token æ˜¯ LLM å¤„ç†æ–‡æœ¬çš„åŸºæœ¬å•ä½ï¼Œå®ƒæ˜¯æ•ˆç‡å’Œè´¨é‡çš„æœ€ä½³å¹³è¡¡ç‚¹ï¼š**
>
> 1. **ä¸ºä»€ä¹ˆä¸ç”¨å­—ç¬¦ï¼Ÿ**
>    - å­—ç¬¦å¤ªå°ï¼ˆåªæœ‰26ä¸ªå­—æ¯ + ç¬¦å·ï¼‰
>    - éœ€è¦å­¦ä¹ å­—ç¬¦ç»„åˆè§„åˆ™ï¼ˆ"c-a-t" = çŒ«ï¼‰
>    - åºåˆ—å¤ªé•¿ï¼Œè®¡ç®—æˆæœ¬é«˜
>
> 2. **ä¸ºä»€ä¹ˆä¸ç”¨å•è¯ï¼Ÿ**
>    - è¯æ±‡è¡¨å¤ªå¤§ï¼ˆè‹±è¯­æœ‰ 100ä¸‡+ å•è¯ï¼‰
>    - æ— æ³•å¤„ç†æ–°è¯ï¼ˆOOV é—®é¢˜ï¼‰
>    - ä¸åŒè¯­è¨€è¯æ±‡è¡¨å·®å¼‚å¤§
>
> 3. **Token çš„ä¼˜åŠ¿**
>    - å­è¯æ‹†åˆ†ï¼š`"unhappiness"` â†’ `["un", "happiness"]`
>    - è¯æ±‡è¡¨å¯æ§ï¼šçº¦ 50,000-100,000 ä¸ª Token
>    - èƒ½å¤„ç†ä»»ä½•æ–‡æœ¬ï¼ŒåŒ…æ‹¬æ–°è¯ã€ä»£ç ã€è¡¨æƒ…ç¬¦å·
>
> **åœ¨å®é™…åº”ç”¨ä¸­çš„å½±å“ï¼š**
> - API æŒ‰ Token è®¡è´¹ï¼Œç†è§£ Token æ‰èƒ½æ§åˆ¶æˆæœ¬
> - ä¸Šä¸‹æ–‡çª—å£æŒ‰ Token è®¡ç®—ï¼Œå½±å“ RAG æ£€ç´¢é‡
> - ä¸åŒè¯­è¨€ Token æ•ˆç‡ä¸åŒï¼ˆä¸­æ–‡çº¦ 1.5å­—/Tokenï¼Œè‹±æ–‡çº¦ 4å­—ç¬¦/Tokenï¼‰
>
> **åœ¨ LangChain ä¸­**ï¼š`llm.get_num_tokens()` æ–¹æ³•ç”¨äºè®¡ç®— Tokenï¼Œ`ConversationTokenBufferMemory` ç”¨äºåŸºäº Token ç®¡ç†å¯¹è¯å†å²ã€‚

**ä¸ºä»€ä¹ˆè¿™ä¸ªå›ç­”å‡ºå½©ï¼Ÿ**
1. âœ… è§£é‡Šäº†"ä¸ºä»€ä¹ˆ"è€Œä¸åªæ˜¯"æ˜¯ä»€ä¹ˆ"
2. âœ… å¯¹æ¯”äº†ä¸‰ç§æ–¹æ¡ˆçš„ä¼˜åŠ£
3. âœ… è”ç³»äº†å®é™…åº”ç”¨ï¼ˆæˆæœ¬ã€RAGï¼‰
4. âœ… æåˆ°äº†å…·ä½“çš„ LangChain ç»„ä»¶

---

### é—®é¢˜ï¼š"å¦‚ä½•å¤„ç†è¶…è¿‡ä¸Šä¸‹æ–‡çª—å£çš„é•¿æ–‡æ¡£ï¼Ÿ"

**æ™®é€šå›ç­”ï¼ˆâŒ ä¸å‡ºå½©ï¼‰ï¼š**
"æŠŠæ–‡æ¡£åˆ‡çŸ­ï¼Œæˆ–è€…ç”¨æ›´å¤§ä¸Šä¸‹æ–‡çš„æ¨¡å‹ã€‚"

**å‡ºå½©å›ç­”ï¼ˆâœ… æ¨èï¼‰ï¼š**

> **å¤„ç†é•¿æ–‡æ¡£æœ‰ä¸‰ç§ä¸»è¦ç­–ç•¥ï¼š**
>
> **1. æˆªæ–­ç­–ç•¥**
> ```python
> # ä¿ç•™å¼€å¤´å’Œç»“å°¾ï¼ˆé‡è¦ä¿¡æ¯é€šå¸¸åœ¨è¿™é‡Œï¼‰
> truncated = doc[:max_tokens//2] + doc[-max_tokens//2:]
> ```
>
> **2. åˆ†å—å¤„ç†ï¼ˆMap-Reduceï¼‰**
> ```python
> # æ–‡æ¡£åˆ†å— â†’ æ¯å—ç‹¬ç«‹å¤„ç† â†’ åˆå¹¶ç»“æœ
> chunks = split_document(doc, chunk_size=4000)
> summaries = [llm.invoke(chunk) for chunk in chunks]
> final = llm.invoke(combine(summaries))
> ```
>
> **3. RAG æ£€ç´¢ï¼ˆæ¨èï¼‰**
> ```python
> # åªæ£€ç´¢ç›¸å…³éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯å¤„ç†æ•´ä¸ªæ–‡æ¡£
> relevant_chunks = retriever.invoke(question)
> answer = llm.invoke(relevant_chunks + question)
> ```
>
> **é€‰æ‹©ç­–ç•¥çš„ä¾æ®ï¼š**
> | åœºæ™¯ | æ¨èç­–ç•¥ |
> |------|---------|
> | é—®ç­” | RAGï¼ˆåªéœ€ç›¸å…³éƒ¨åˆ†ï¼‰|
> | å…¨æ–‡æ€»ç»“ | Map-Reduce |
> | å¿«é€Ÿé¢„è§ˆ | æˆªæ–­ï¼ˆé¦–å°¾ï¼‰|
>
> **LangChain æ”¯æŒï¼š**
> - `RecursiveCharacterTextSplitter`ï¼šæ™ºèƒ½åˆ†å—
> - `MapReduceDocumentsChain`ï¼šMap-Reduce æ¨¡å¼
> - `VectorStoreRetriever`ï¼šRAG æ£€ç´¢
>
> **æˆ‘çš„ç»éªŒ**ï¼šå¯¹äºçŸ¥è¯†åº“é—®ç­”ï¼ŒRAG æ˜¯æœ€é«˜æ•ˆçš„æ–¹æ¡ˆï¼›å¯¹äºéœ€è¦ç†è§£å…¨æ–‡çš„ä»»åŠ¡ï¼ˆå¦‚åˆåŒå®¡æŸ¥ï¼‰ï¼ŒMap-Reduce æ›´åˆé€‚ã€‚

---

## 9. ã€åŒ–éª¨ç»µæŒã€‘

### å¡ç‰‡1ï¼šToken æ˜¯ä»€ä¹ˆï¼Ÿ ğŸ¯

**ä¸€å¥è¯ï¼š** Token æ˜¯ LLM å¤„ç†æ–‡æœ¬çš„åŸºæœ¬å•ä½ï¼Œä»‹äºå­—ç¬¦å’Œå•è¯ä¹‹é—´ã€‚

**ä¸¾ä¾‹ï¼š**
```python
"Hello World" â†’ ["Hello", " World"]  # 2 tokens
"ä½ å¥½ä¸–ç•Œ" â†’ ["ä½ ", "å¥½", "ä¸–", "ç•Œ"]  # ~4 tokens
```

**åº”ç”¨ï¼š** API æŒ‰ Token è®¡è´¹ï¼Œä¸Šä¸‹æ–‡æŒ‰ Token é™åˆ¶ã€‚

---

### å¡ç‰‡2ï¼šTokenizer åˆ†è¯å™¨ ğŸ”¤

**ä¸€å¥è¯ï¼š** Tokenizer å°†æ–‡æœ¬æ‹†åˆ†æˆ Tokenï¼Œä¸åŒæ¨¡å‹ä½¿ç”¨ä¸åŒçš„ Tokenizerã€‚

**ä¸¾ä¾‹ï¼š**
```python
import tiktoken
encoding = tiktoken.encoding_for_model("gpt-4")
tokens = encoding.encode("Hello")  # [9906]
```

**åº”ç”¨ï¼š** ç²¾ç¡®è®¡ç®— Token æ•°éœ€è¦ä½¿ç”¨æ¨¡å‹å¯¹åº”çš„ Tokenizerã€‚

---

### å¡ç‰‡3ï¼šä¸Šä¸‹æ–‡çª—å£ ğŸ“

**ä¸€å¥è¯ï¼š** ä¸Šä¸‹æ–‡çª—å£æ˜¯ LLM ä¸€æ¬¡èƒ½å¤„ç†çš„æœ€å¤§ Token æ•°ã€‚

**ä¸¾ä¾‹ï¼š**
```python
# GPT-4: 8,192 tokens
# GPT-4 Turbo: 128,000 tokens
# Claude 3: 200,000 tokens
```

**åº”ç”¨ï¼š** è¾“å…¥ + è¾“å‡ºä¸èƒ½è¶…è¿‡ä¸Šä¸‹æ–‡çª—å£ã€‚

---

### å¡ç‰‡4ï¼šToken è®¡è´¹ ğŸ’°

**ä¸€å¥è¯ï¼š** API æŒ‰ Token æ•°é‡è®¡è´¹ï¼Œè¾“å‡ºé€šå¸¸æ¯”è¾“å…¥è´µã€‚

**ä¸¾ä¾‹ï¼š**
```python
# GPT-4 å®šä»·
# è¾“å…¥: $0.03 / 1K tokens
# è¾“å‡º: $0.06 / 1K tokens

cost = (1000 * 0.03 + 500 * 0.06) / 1000  # = $0.06
```

**åº”ç”¨ï¼š** æ§åˆ¶ max_tokens å¯ä»¥é™ä½æˆæœ¬ã€‚

---

### å¡ç‰‡5ï¼šmax_tokens å‚æ•° ğŸ›ï¸

**ä¸€å¥è¯ï¼š** max_tokens é™åˆ¶æ¨¡å‹è¾“å‡ºçš„æœ€å¤§é•¿åº¦ã€‚

**ä¸¾ä¾‹ï¼š**
```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4", max_tokens=500)
# è¾“å‡ºæœ€å¤š 500 tokens
```

**åº”ç”¨ï¼š** æ ¹æ®åœºæ™¯è®¾ç½®åˆé€‚çš„è¾“å‡ºé™åˆ¶ã€‚

---

### å¡ç‰‡6ï¼šæ¶ˆæ¯æˆªæ–­ç­–ç•¥ âœ‚ï¸

**ä¸€å¥è¯ï¼š** å½“å¯¹è¯è¶…å‡ºçª—å£æ—¶ï¼Œéœ€è¦ç­–ç•¥æ€§åœ°æˆªæ–­å†å²æ¶ˆæ¯ã€‚

**ä¸¾ä¾‹ï¼š**
```python
# ç­–ç•¥1ï¼šä¿ç•™æœ€è¿‘ K æ¡
messages = all_messages[-10:]

# ç­–ç•¥2ï¼šæŒ‰ Token æˆªæ–­
# ç­–ç•¥3ï¼šæ€»ç»“å‹ç¼©
```

**åº”ç”¨ï¼š** LangChain çš„ Memory ç»„ä»¶å®ç°å„ç§æˆªæ–­ç­–ç•¥ã€‚

---

### å¡ç‰‡7ï¼šLangChain Token Memory ğŸ§ 

**ä¸€å¥è¯ï¼š** ConversationTokenBufferMemory æŒ‰ Token é™åˆ¶ç®¡ç†å¯¹è¯å†å²ã€‚

**ä¸¾ä¾‹ï¼š**
```python
from langchain.memory import ConversationTokenBufferMemory

memory = ConversationTokenBufferMemory(
    llm=llm,
    max_token_limit=2000
)
```

**åº”ç”¨ï¼š** è‡ªåŠ¨æˆªæ–­è¶…å‡ºé™åˆ¶çš„å†å²æ¶ˆæ¯ã€‚

---

### å¡ç‰‡8ï¼šä¸­è‹±æ–‡ Token å·®å¼‚ ğŸŒ

**ä¸€å¥è¯ï¼š** ä¸­æ–‡æ¯å­—çº¦ 1-2 Tokenï¼Œè‹±æ–‡æ¯è¯çº¦ 1 Tokenã€‚

**ä¸¾ä¾‹ï¼š**
```python
# åŒæ ·çš„å«ä¹‰
"Hello World"  # 2 tokens
"ä½ å¥½ä¸–ç•Œ"      # 4-6 tokens

# ä¸­æ–‡æ¶ˆè€—æ›´å¤š Tokenï¼
```

**åº”ç”¨ï¼š** ä¸­æ–‡åº”ç”¨éœ€è¦é¢„ç•™æ›´å¤š Token ç©ºé—´ã€‚

---

### å¡ç‰‡9ï¼šRAG ä¸ä¸Šä¸‹æ–‡åˆ†é… ğŸ“Š

**ä¸€å¥è¯ï¼š** RAG éœ€è¦åˆç†åˆ†é… System Promptã€æ£€ç´¢å†…å®¹ã€ç”¨æˆ·è¾“å…¥å’Œè¾“å‡ºç©ºé—´ã€‚

**ä¸¾ä¾‹ï¼š**
```
128K çª—å£åˆ†é…ï¼š
- System Prompt: 2K
- æ£€ç´¢å†…å®¹: 50K
- ç”¨æˆ·è¾“å…¥: 1K
- è¾“å‡ºç©ºé—´: 75K
```

**åº”ç”¨ï¼š** æ£€ç´¢å†…å®¹ä¸æ˜¯è¶Šå¤šè¶Šå¥½ï¼Œè¦ç•™å¤Ÿè¾“å‡ºç©ºé—´ã€‚

---

### å¡ç‰‡10ï¼šToken åœ¨ LangChain ä¸­çš„ä½œç”¨ â­

**ä¸€å¥è¯ï¼š** Token æ˜¯ LangChain æˆæœ¬æ§åˆ¶å’Œä¸Šä¸‹æ–‡ç®¡ç†çš„åŸºç¡€ã€‚

**ä¸¾ä¾‹ï¼š**
```python
# è®¡ç®— Token
num_tokens = llm.get_num_tokens(text)

# Token é™åˆ¶çš„ Memory
memory = ConversationTokenBufferMemory(max_token_limit=2000)

# é™åˆ¶è¾“å‡º
llm = ChatOpenAI(max_tokens=500)
```

**åº”ç”¨ï¼š** ç†è§£ Token æ‰èƒ½æ„å»ºé«˜æ•ˆçš„ LangChain åº”ç”¨ã€‚

---

## 10. ã€ä¸€å¥è¯æ€»ç»“ã€‘

**Token æ˜¯ LLM å¤„ç†æ–‡æœ¬çš„æœ€å°å•ä½ï¼Œä¸Šä¸‹æ–‡çª—å£æ˜¯ LLM çš„å·¥ä½œè®°å¿†å®¹é‡ï¼Œç†è§£è¿™ä¸¤ä¸ªæ¦‚å¿µæ˜¯æ§åˆ¶ API æˆæœ¬ã€ç®¡ç†å¯¹è¯å†å²ã€ä¼˜åŒ– RAG æ£€ç´¢çš„åŸºç¡€ã€‚**

---

## ğŸ“š å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£ Token ä¸å­—ç¬¦ã€å•è¯çš„åŒºåˆ«
- [ ] èƒ½å¤Ÿä½¿ç”¨ tiktoken è®¡ç®— Token æ•°é‡
- [ ] çŸ¥é“å¸¸è§æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£å¤§å°
- [ ] ä¼šä½¿ç”¨ max_tokens æ§åˆ¶è¾“å‡ºé•¿åº¦
- [ ] ç†è§£ Token è®¡è´¹æ¨¡å‹
- [ ] èƒ½å¤Ÿå®ç°åŸºäº Token çš„æ¶ˆæ¯æˆªæ–­

## ğŸ”— ä¸‹ä¸€æ­¥å­¦ä¹ 

- **æµå¼è¾“å‡º Streaming**ï¼šToken çº§åˆ«çš„å®æ—¶è¾“å‡º
- **Memory è®°å¿†ç³»ç»Ÿ**ï¼šLangChain çš„å¯¹è¯å†å²ç®¡ç†
- **RAG æ£€ç´¢å™¨**ï¼šä¸Šä¸‹æ–‡çª—å£çš„é«˜æ•ˆåˆ©ç”¨

---

**ç‰ˆæœ¬ï¼š** v1.0
**æœ€åæ›´æ–°ï¼š** 2025-12-12
